{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97e5abba-32b7-4dc9-83d4-4bd4ef9f4ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna imported successfully.\n",
      "PyTorch imported successfully.\n",
      "PyTorch CUDA available: True, Version: 12.1\n",
      "Using PyTorch on GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "imblearn (for SMOTE) imported successfully.\n",
      "DoWhy 0.10 and NetworkX 3.1 imported successfully.\n",
      "ONNX, ONNXRuntime, skl2onnx, and onnxmltools imported successfully.\n",
      "Onnxmltools version: 1.11.1\n",
      "\n",
      "All libraries and modules conditional imports attempted.\n",
      "DEBUG: Script execution started, entering __main__ block.\n",
      "DEBUG: API_KEY appears to be set. Proceeding. Key ends with '...ef0a'\n",
      "DEBUG: Symbols to process: ['AAPL', 'GOOGL']\n",
      "DEBUG: Starting main loop for symbol: AAPL\n",
      "\n",
      "========================================\n",
      "ðŸš€ ENHANCED WORKFLOW FOR: AAPL\n",
      "========================================\n",
      "Fetching data for AAPL from Twelve Data (interval=1day, from 2022-06-04 to 2025-06-03)...\n",
      "Successfully fetched/processed 750 data points for AAPL.\n",
      "\n",
      "--- ðŸ”§ Feature Engineering: AAPL ---\n",
      "Adding technical indicators...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38379/4212630612.py:192: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[9.98611407e+09 8.00782855e+09 1.23548189e+10 1.76871775e+10\n",
      " 1.09688179e+10 9.95967086e+09 9.95940605e+09 1.25755687e+10\n",
      " 9.97233035e+09 9.79635737e+09 1.02719769e+10 1.05694694e+10\n",
      " 9.63284651e+09 9.45818898e+09 1.13631385e+10 1.14780874e+10\n",
      " 1.14194116e+10 1.24235717e+10 9.87757346e+09 1.00419950e+10\n",
      " 1.03075722e+10 1.22197531e+10 1.27330542e+10 1.64772630e+10\n",
      " 1.09881365e+10 1.35716587e+10 9.19816034e+09 9.98346202e+09\n",
      " 1.18232614e+10 9.67097850e+09 1.16499329e+10 9.33857157e+09\n",
      " 9.74196305e+09 1.38739808e+10 8.68182651e+09 1.20859660e+10\n",
      " 1.35908341e+10 1.06564850e+10 1.70375068e+10 1.24421969e+10\n",
      " 1.68017624e+10 1.41309629e+10 1.28473936e+10 1.61259518e+10\n",
      " 1.27816217e+10 1.15707123e+10 9.99105288e+09 1.58872146e+10\n",
      " 1.24359106e+10 1.20936924e+10 1.42446076e+10 9.28966227e+09\n",
      " 1.26294602e+10 1.12878711e+10 1.13142214e+10 2.53217851e+10\n",
      " 1.25398382e+10 1.71641023e+10 1.39115016e+10 1.09080380e+10\n",
      " 1.35458486e+10 1.20141956e+10 1.13232148e+10 7.72796261e+09\n",
      " 8.78874185e+09 1.62356225e+10 1.05462467e+10 1.01707065e+10\n",
      " 8.84851958e+09 1.08724857e+10 1.01008282e+10 1.37599618e+10\n",
      " 1.16005664e+10 9.78646964e+09 1.12560026e+10 9.28421318e+09\n",
      " 9.20237836e+09 9.49610381e+09 7.73378147e+09 8.64235945e+09\n",
      " 9.49524164e+09 1.09668336e+10 1.15099425e+10 9.43356275e+09\n",
      " 7.75704819e+09 1.02597539e+10 1.12187659e+10 1.77575559e+10\n",
      " 2.36531981e+10 1.27905743e+10 9.51706899e+09 1.01357423e+10\n",
      " 7.21041317e+09 6.66787773e+09 1.05973489e+10 1.35303380e+10\n",
      " 7.20927441e+09 1.27054446e+10 1.12039877e+10 1.17336367e+10\n",
      " 1.17783146e+10 1.53700545e+10 1.15216877e+10 1.17123191e+10\n",
      " 1.20562687e+10 1.07780607e+10 9.44065802e+09 8.22892145e+09\n",
      " 8.02092546e+09 1.12706128e+10 9.43297948e+09 7.67571884e+09\n",
      " 7.43822714e+09 1.12428743e+10 8.15453475e+09 8.31267856e+09\n",
      " 7.97452246e+09 1.08637797e+10 9.33790006e+09 8.89730492e+09\n",
      " 1.95897615e+10 9.69014555e+09 9.30311330e+09 8.59096550e+09\n",
      " 7.25714883e+09 1.14154201e+10 9.78787029e+09 9.69023774e+09\n",
      " 9.58351277e+09 9.94039710e+09 1.77120500e+10 1.23366373e+10\n",
      " 1.11913526e+10 2.20548372e+10 9.01976806e+09 8.85895485e+09\n",
      " 9.92597445e+09 1.00685038e+10 1.05417606e+10 1.21337144e+10\n",
      " 9.52684296e+09 9.89486590e+09 9.50559906e+09 9.67570713e+09\n",
      " 8.78435837e+09 1.64385657e+10 8.94379767e+09 1.15413230e+10\n",
      " 7.87584582e+09 9.76925981e+09 9.35613794e+09 1.57252226e+10\n",
      " 8.77168000e+09 7.22026451e+09 9.23265175e+09 9.44257841e+09\n",
      " 7.61211187e+09 9.77655958e+09 7.81007094e+09 8.11554314e+09\n",
      " 7.45114775e+09 9.50767827e+09 9.15082586e+09 7.87793938e+09\n",
      " 9.69123290e+09 1.13567933e+10 1.14388905e+10 8.65352272e+09\n",
      " 1.17164206e+10 1.05526049e+10 1.19610916e+10 9.25680273e+09\n",
      " 9.94434069e+09 8.11408236e+09 8.88820816e+09 9.02411056e+09\n",
      " 9.16968433e+09 8.46067744e+09 1.00840311e+10 7.54330174e+09\n",
      " 7.80824996e+09 8.51560981e+09 1.02532891e+10 7.57384537e+09\n",
      " 9.82804399e+09 8.69789424e+09 9.83683246e+09 1.36833622e+10\n",
      " 1.13822654e+10 1.27713375e+10 9.01167686e+09 9.82689253e+09\n",
      " 1.22677457e+10 1.12573015e+10 1.01357438e+10 1.03261176e+10\n",
      " 8.88619451e+09 7.59431678e+09 7.31019429e+09 8.70997681e+09\n",
      " 1.28370713e+10 7.93291445e+09 9.22428407e+09 1.04151571e+10\n",
      " 1.02082680e+10 1.38651689e+10 1.32302063e+10 8.00412063e+09\n",
      " 6.59898412e+09 1.08956000e+10 7.89745727e+09 8.68023877e+09\n",
      " 7.52069210e+09 1.46546896e+10 1.31141791e+10 1.16555532e+10\n",
      " 8.25595341e+09 1.04620365e+10 1.20606821e+10 1.30691700e+10\n",
      " 8.19627617e+09 1.01368440e+10 8.53192482e+09 7.55660592e+09\n",
      " 9.61810469e+09 9.88803149e+09 1.30249841e+10 1.04077419e+10\n",
      " 1.26227852e+10 1.32294752e+10 9.67591167e+09 9.48135364e+09\n",
      " 1.03875007e+10 8.09026070e+09 9.12187171e+09 7.18981841e+09\n",
      " 1.57448407e+10 1.79183493e+10 7.98523720e+09 8.23764065e+09\n",
      " 8.11482711e+09 8.57215960e+09 7.61983755e+09 1.18757914e+10\n",
      " 1.62470454e+10 3.00898188e+10 1.41371691e+10 9.00209590e+09\n",
      " 1.34003330e+10 9.81541667e+09 1.33229651e+10 1.00511572e+10\n",
      " 8.45777740e+09 8.12340494e+09 6.87903644e+09 9.96964115e+09\n",
      " 9.55836280e+09 1.43985768e+10 9.70793814e+09 9.22052856e+09\n",
      " 1.06082338e+10 1.04078334e+10 3.49308741e+10 4.22798781e+10\n",
      " 2.09618648e+10 2.02559353e+10 1.68835139e+10 1.18921488e+10\n",
      " 1.40978938e+10 1.06545106e+10 1.30103818e+10 1.26879321e+10\n",
      " 8.24788855e+09 1.35815128e+10 1.33715335e+10 1.09685619e+10\n",
      " 1.45152221e+10 1.22335835e+10 1.47125096e+10 1.08426737e+10\n",
      " 9.06159537e+09 9.09569998e+09 1.11168985e+10 2.33366435e+10\n",
      " 1.33352684e+10 1.00034981e+10 9.07320347e+09 8.27288639e+09\n",
      " 9.74610492e+09 9.29357354e+09 1.04095772e+10 9.99915840e+09\n",
      " 6.85898652e+09 7.87357447e+09 8.75899028e+09 8.16637752e+09\n",
      " 8.65091399e+09 1.19659592e+10 8.16987215e+09 9.86326096e+09\n",
      " 8.32635044e+09 8.19221117e+09 9.83505139e+09 1.31956451e+10\n",
      " 1.52109995e+10 7.31805895e+10 9.90828696e+09 8.32178216e+09\n",
      " 7.76484651e+09 1.26473574e+10 8.42893858e+09 7.16753250e+09\n",
      " 7.66011433e+09 6.44190253e+09 9.19455241e+09 1.51887363e+10\n",
      " 7.66028053e+09 1.09143429e+10 8.55340346e+09 8.97885235e+09\n",
      " 8.42850296e+09 6.26450950e+09 1.21849487e+10 9.55022893e+09\n",
      " 8.71379728e+09 9.06510920e+09 1.09197860e+10 1.02140477e+10\n",
      " 1.01724702e+10 8.26956806e+09 8.76153324e+09 2.09112428e+10\n",
      " 1.07908152e+10 7.86575919e+09 6.73214852e+09 1.15134314e+10\n",
      " 9.38246991e+09 1.07759670e+10 9.73771216e+09 1.09500676e+10\n",
      " 9.12189330e+09 1.12042575e+10 8.21875680e+09 1.29249059e+10\n",
      " 1.29604723e+10 3.71285007e+10 1.04104135e+10 5.97673925e+09\n",
      " 7.05213913e+09 1.10441301e+10 9.22615134e+09 9.44363611e+09\n",
      " 1.42732762e+10 1.35124727e+10 2.16917809e+10 1.78987905e+10\n",
      " 1.08140170e+10 1.32761743e+10 2.41444830e+10 1.04042367e+10\n",
      " 9.15770407e+09 6.95713340e+09 1.24623107e+10 1.06247858e+10\n",
      " 1.28573762e+10 9.96694950e+09 1.19043900e+10 7.87972360e+09\n",
      " 7.93810776e+09 1.31124498e+10 1.26670135e+10 1.18744892e+10\n",
      " 1.12953777e+10 1.06353707e+10 1.10316262e+10 1.27632480e+10\n",
      " 1.02426788e+10 9.04276876e+09 1.17427143e+10 2.03438414e+10\n",
      " 9.75755554e+09 7.67977022e+09 8.27708662e+09 1.44524764e+10\n",
      " 8.08082752e+09 8.02018124e+09 3.51181998e+10 1.70135682e+10\n",
      " 2.08320751e+10 1.00996813e+10 1.05476014e+10 1.08575434e+10\n",
      " 9.77981451e+09 7.96590954e+09 8.12442820e+09 7.75613832e+09\n",
      " 1.10280943e+10 1.21764435e+10 9.96487144e+09 7.24958685e+09\n",
      " 1.33676201e+10 1.09931782e+10 1.04710452e+10 1.12273044e+10\n",
      " 9.11403521e+09 7.11515508e+09]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  mfi_series = df_feat.ta.mfi(high=df_feat[h], low=df_feat[l], close=df_feat[c], volume=df_feat[v], append=False)\n",
      "/tmp/ipykernel_38379/4212630612.py:192: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[1.00300486e+10 1.26473852e+10 1.62339912e+10 1.12517242e+10\n",
      " 1.41098082e+10 9.35097896e+09 9.19819886e+09 1.34875418e+10\n",
      " 9.16163569e+09 1.02996074e+10 1.20866617e+10 8.22715736e+09\n",
      " 8.37170893e+09 9.62605634e+09 9.33988618e+09 1.03961021e+10\n",
      " 1.08420068e+10 1.21129296e+10 1.16096177e+10 9.07126366e+09\n",
      " 9.00750348e+09 1.31135798e+10 1.18304358e+10 1.24439573e+10\n",
      " 1.39301643e+10 1.16553317e+10 1.14334111e+10 1.31207271e+10\n",
      " 1.91242078e+10 1.36645455e+10 1.38433070e+10 2.43645396e+10\n",
      " 1.57989375e+10 1.32324147e+10 1.44192093e+10 2.17748714e+10\n",
      " 1.83613494e+10 1.74603932e+10 1.21051987e+10 1.05078306e+10\n",
      " 1.07474853e+10 9.78699739e+09 8.85840469e+09 1.32109186e+10\n",
      " 1.59395814e+10 1.50016338e+10 1.21970226e+10 1.37973180e+10\n",
      " 1.37226250e+10 1.94995641e+10 1.14984687e+10 1.01890705e+10\n",
      " 9.54604987e+09 8.73227356e+09 5.21028372e+09 1.00231277e+10\n",
      " 1.19036739e+10 9.63080285e+09 9.32352165e+09 9.86111998e+09\n",
      " 1.18233256e+10 1.36634847e+10 2.16686936e+10 1.05828977e+10\n",
      " 1.02066755e+10 1.03047635e+10 8.37932180e+09 8.97469439e+09\n",
      " 1.09059270e+10 9.94537174e+09 1.42067818e+10 1.12906932e+10\n",
      " 1.01891557e+10 8.30884865e+09 7.87348777e+09 9.27989439e+09\n",
      " 9.20540035e+09 9.46199338e+09 1.06093803e+10 9.78194872e+09\n",
      " 8.50598497e+09 8.64767076e+09 9.41719684e+09 1.05294415e+10\n",
      " 8.99778928e+09 8.79456346e+09 7.58402001e+09 8.12851537e+09\n",
      " 7.46949871e+09 8.09161205e+09 7.60099180e+09 8.55389694e+09\n",
      " 8.17122252e+09 1.02184305e+10 8.32866063e+09 7.23794582e+09\n",
      " 8.42405159e+09 7.70055332e+09 7.68135658e+09 8.05371554e+09\n",
      " 6.84532943e+09 8.74413200e+09 9.64432271e+09 6.91916716e+09\n",
      " 8.01850324e+09 7.45937645e+09 8.17474226e+09 1.09760674e+10\n",
      " 1.34617885e+10 7.80997596e+09 7.85034219e+09 6.41918929e+09\n",
      " 9.96844769e+09 7.58656525e+09 8.73225707e+09 7.74632364e+09\n",
      " 1.16026757e+10 1.10746627e+10 1.87684057e+10 9.22146542e+09\n",
      " 9.11022876e+09 8.95331451e+09 6.06199514e+09 8.99196269e+09\n",
      " 8.61346230e+09 1.12981311e+10 8.76765207e+09 7.91982781e+09\n",
      " 1.15607388e+10 1.38595190e+10 9.22262426e+09 6.88986409e+09\n",
      " 9.73538072e+09 1.17210296e+10 2.12797393e+10 1.75419067e+10\n",
      " 1.21550120e+10 1.07902414e+10 9.23585909e+09 7.76453203e+09\n",
      " 8.32233683e+09 1.15607871e+10 1.06247043e+10 9.77101170e+09\n",
      " 8.56229707e+09 1.50765874e+10 1.98471489e+10 1.60030566e+10\n",
      " 1.47606465e+10 1.06652908e+10 1.91229052e+10 1.03352211e+10\n",
      " 1.10142578e+10 1.11700046e+10 1.14329200e+10 9.57642801e+09\n",
      " 8.54432300e+09 9.23836424e+09 9.35345262e+09 1.01741582e+10\n",
      " 9.64820944e+09 1.04469967e+10 1.11462580e+10 9.64611734e+09\n",
      " 9.80833169e+09 1.18636379e+10 7.61739177e+09 1.40070377e+10\n",
      " 8.07166933e+09 9.65222821e+09 7.26970698e+09 4.57013885e+09\n",
      " 7.69634251e+09 8.17886331e+09 9.24588837e+09 8.19960298e+09\n",
      " 1.17436478e+10 2.53507918e+10 1.09071159e+10 1.02280952e+10\n",
      " 9.06811586e+09 7.20157584e+09 5.58884755e+09 9.26072904e+09\n",
      " 8.22252699e+09 1.53420733e+10 1.07786433e+10 1.30981358e+10\n",
      " 1.12999491e+10 9.10938798e+09 1.20014124e+10 8.61034696e+09\n",
      " 1.06638122e+10 8.60842561e+09 9.01282435e+09 1.05631986e+10\n",
      " 1.02772591e+10 1.88780066e+10 7.71737748e+09 7.83563797e+09\n",
      " 1.04534350e+10 1.00541970e+10 1.19897820e+10 9.09253995e+09\n",
      " 9.73118307e+09 8.26878652e+09 7.41838640e+09 8.88790690e+09\n",
      " 2.47327169e+10 1.31683147e+10 1.42857316e+10 1.62289530e+10\n",
      " 1.16379609e+10 1.21574866e+10 1.03352366e+10 9.01195969e+09\n",
      " 2.09072672e+10 1.83941409e+10 1.22158769e+10 9.26954644e+09\n",
      " 9.76961011e+09 1.12567384e+10 7.87259924e+09 8.32698403e+09\n",
      " 7.13424413e+09 6.31106219e+09 8.35083094e+09 1.27901213e+10\n",
      " 1.25655480e+10 8.59636416e+09 7.21920459e+09 1.11932399e+10\n",
      " 1.13258406e+10 8.58393686e+09 1.43083283e+10 8.21931644e+09\n",
      " 9.31047667e+09 7.84182433e+09 6.62929750e+09 9.59778020e+09\n",
      " 1.01191828e+10 8.03251009e+09 1.88873269e+10 1.49352004e+10\n",
      " 1.72123252e+10 1.81757912e+10 5.14601818e+10 1.75262917e+10\n",
      " 1.47913335e+10 1.01372906e+10 1.31318290e+10 1.48993866e+10\n",
      " 1.10490934e+10 8.98613886e+09 1.36007503e+10 1.11854663e+10\n",
      " 7.90737873e+09 1.37473034e+10 2.46576957e+10 1.43567972e+10\n",
      " 9.15367003e+09 9.85722258e+09 6.91966735e+09 1.21327734e+10\n",
      " 1.12584208e+10 9.64645162e+09 1.07471624e+10 1.47547436e+10\n",
      " 1.13211290e+10 1.28105143e+10 1.23043175e+10 9.55725319e+09\n",
      " 1.43361622e+10 7.42191868e+09 7.66902357e+09 8.80588534e+09\n",
      " 7.24427352e+09 7.88139514e+09 9.12480749e+09 1.20897998e+10\n",
      " 7.15311124e+09 8.26779129e+09 1.08752609e+10 1.46143729e+10\n",
      " 1.45464776e+10 9.95535648e+09 9.40127334e+09 1.08018422e+10\n",
      " 8.02828171e+09 9.60616662e+09 8.96631543e+09 8.11112026e+09\n",
      " 1.41948500e+10 1.51978453e+10 1.08337799e+10 8.96582362e+09\n",
      " 9.91136640e+09 1.36521986e+10 9.78508449e+09 9.92950633e+09\n",
      " 9.11196373e+09 1.46051189e+10 1.15604788e+10 1.66074473e+10\n",
      " 1.57694308e+10 2.17860125e+10 1.22125825e+10 1.66959381e+10\n",
      " 9.11796420e+09 7.56625218e+09 1.07302701e+10 9.83324129e+09\n",
      " 1.35288119e+10 1.27444110e+10 1.10437021e+10 1.65249652e+10\n",
      " 1.68550438e+10 1.36278493e+10 1.29883959e+10 1.04861415e+10\n",
      " 7.66319272e+09 8.75199529e+09 2.10950632e+10 2.41581047e+10\n",
      " 2.94311241e+10 2.14312098e+10 2.30840720e+10 1.03620546e+10\n",
      " 1.16944093e+10 8.98655724e+09 2.06903386e+10 1.38313079e+10\n",
      " 1.01780841e+10 1.34546680e+10 9.51548384e+09 1.15606445e+10\n",
      " 9.57446120e+09 8.78773024e+09 1.20365747e+10 9.40784285e+09\n",
      " 1.53318021e+10 1.03182501e+10 1.41543018e+10]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  mfi_series = df_feat.ta.mfi(high=df_feat[h], low=df_feat[l], close=df_feat[c], volume=df_feat[v], append=False)\n",
      "/tmp/ipykernel_38379/4212630612.py:312: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  df_new.ta.stc(close=df_new[price_col], tclength=23, fast=50, slow=100, factor=0.5, append=True, col_names=('STC_23_50_05', 'STCD_23_50_05', 'STCK_23_50_05'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Cleaning (Inf/NaN Handling & Imputation): AAPL ---\n",
      "NaNs before imputation: 1858\n",
      "NaNs after imputation: 0\n",
      "\n",
      "--- ðŸ“Š Regime Detection (Simplified): AAPL ---\n",
      "Simple Regimes (0:Med,1:Low,2:High):\n",
      "regime_simple\n",
      "0    34.666667\n",
      "1    32.666667\n",
      "2    32.666667\n",
      "Name: proportion, dtype: float64 %\n",
      "\n",
      "--- ðŸŽ¯ Target Definition: AAPL ---\n",
      "Target distribution:\n",
      "target\n",
      "0    55.333333\n",
      "1    44.666667\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- Causal Feature Discovery and Effect Estimation for AAPL (Subset Analysis) ---\n",
      "Iteratively estimating causal effects for 11 selected features: ['RSI_14', 'MACDh_12_26_9', 'ADX_14', 'close_cwt_mean', 'close_cwt_std', 'close_entropy_sample', 'regime_simple', 'volatility_20', 'log_returns', 'close_trans_seq_volatility', 'close_trans_seq_autocorr1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "[I 2025-06-03 16:35:50,170] A new study created in memory with name: no-name-546191a1-224b-4f65-a453-95eaf9771b8c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal features ranked by absolute effect strength (top 5 of 11):\n",
      "  close_trans_seq_volatility: 0.1427\n",
      "  close_trans_seq_autocorr1: 0.0892\n",
      "  close_cwt_std: 0.0830\n",
      "  close_entropy_sample: 0.0773\n",
      "  close_cwt_mean: 0.0707\n",
      "\n",
      "--- ML Preparation & Feature Selection: AAPL ---\n",
      "Train shapes: X_train=(638, 100), y_train=(638,); Test shapes: X_test=(112, 100), y_test=(112,)\n",
      "Selected 35 features: ['close_trans_seq_volatility', 'close_trans_seq_autocorr1', 'close_cwt_std', 'close_entropy_sample', 'close_cwt_mean', 'RSI_14', 'regime_simple', 'MACDh_12_26_9', 'log_returns', 'volatility_20']...\n",
      "Class distribution before SMOTE: \n",
      "target\n",
      "0    0.532915\n",
      "1    0.467085\n",
      "Name: proportion, dtype: float64\n",
      "Class distribution after SMOTE: \n",
      "target\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n",
      "Shape of X_train after SMOTE: (680, 35)\n",
      "\n",
      "--- Hyperparameter Optimization (Optuna for LightGBM): AAPL ---\n",
      "Optimizing LightGBM HPs with Optuna (50 trials)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1bcdcaa419b4076b87d8a8b600db596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.6473601264979502, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6473601264979502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6223679220139575, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6223679220139575\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.06724190361724207, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.06724190361724207\n",
      "[I 2025-06-03 16:35:50,232] Trial 0 finished with value: 0.6805048667101096 and parameters: {'n_estimators': 950, 'learning_rate': 0.09396999915221718, 'num_leaves': 48, 'max_depth': 5, 'min_child_samples': 44, 'feature_fraction': 0.6223679220139575, 'bagging_fraction': 0.6473601264979502, 'bagging_freq': 4, 'reg_alpha': 0.015703651952627976, 'reg_lambda': 0.021634232531815352, 'min_gain_to_split': 0.06724190361724207}. Best is trial 0 with value: 0.6805048667101096.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8595930713409534, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8595930713409534\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8502137177199772, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8502137177199772\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.05055630606496985, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.05055630606496985\n",
      "[I 2025-06-03 16:35:50,391] Trial 1 finished with value: 0.6552185630366003 and parameters: {'n_estimators': 100, 'learning_rate': 0.024630503408160198, 'num_leaves': 36, 'max_depth': -1, 'min_child_samples': 12, 'feature_fraction': 0.8502137177199772, 'bagging_fraction': 0.8595930713409534, 'bagging_freq': 5, 'reg_alpha': 0.35320092522598445, 'reg_lambda': 0.021427036605378105, 'min_gain_to_split': 0.05055630606496985}. Best is trial 1 with value: 0.6552185630366003.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6262613662978723, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6262613662978723\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6533029259296711, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6533029259296711\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.06756747387008785, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.06756747387008785\n",
      "[I 2025-06-03 16:35:50,452] Trial 2 finished with value: 0.6849795974515013 and parameters: {'n_estimators': 850, 'learning_rate': 0.08748543162495749, 'num_leaves': 28, 'max_depth': -1, 'min_child_samples': 34, 'feature_fraction': 0.6533029259296711, 'bagging_fraction': 0.6262613662978723, 'bagging_freq': 0, 'reg_alpha': 0.7100178637311267, 'reg_lambda': 0.015979986179741813, 'min_gain_to_split': 0.06756747387008785}. Best is trial 1 with value: 0.6552185630366003.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9718118008083819, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9718118008083819\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5193418413183364, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5193418413183364\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.026706876546719518, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.026706876546719518\n",
      "[I 2025-06-03 16:35:50,524] Trial 3 finished with value: 0.6731557588806877 and parameters: {'n_estimators': 1000, 'learning_rate': 0.09667735165911052, 'num_leaves': 17, 'max_depth': 6, 'min_child_samples': 8, 'feature_fraction': 0.5193418413183364, 'bagging_fraction': 0.9718118008083819, 'bagging_freq': 1, 'reg_alpha': 0.9937190627801678, 'reg_lambda': 0.006229306636252701, 'min_gain_to_split': 0.026706876546719518}. Best is trial 1 with value: 0.6552185630366003.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6644483342419429, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6644483342419429\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8135259318872148, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8135259318872148\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.07961255219758855, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.07961255219758855\n",
      "[I 2025-06-03 16:35:50,554] Trial 4 finished with value: 0.680767331461883 and parameters: {'n_estimators': 750, 'learning_rate': 0.07800816614373839, 'num_leaves': 10, 'max_depth': 2, 'min_child_samples': 30, 'feature_fraction': 0.8135259318872148, 'bagging_fraction': 0.6644483342419429, 'bagging_freq': 7, 'reg_alpha': 0.005866209984363986, 'reg_lambda': 0.08403850241368178, 'min_gain_to_split': 0.07961255219758855}. Best is trial 1 with value: 0.6552185630366003.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6158684706633245, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6158684706633245\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5333289423568515, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5333289423568515\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04050319088692204, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04050319088692204\n",
      "[I 2025-06-03 16:35:50,589] Trial 5 finished with value: 0.6876682949567787 and parameters: {'n_estimators': 100, 'learning_rate': 0.04658327815195187, 'num_leaves': 48, 'max_depth': -1, 'min_child_samples': 47, 'feature_fraction': 0.5333289423568515, 'bagging_fraction': 0.6158684706633245, 'bagging_freq': 2, 'reg_alpha': 0.3821117075122998, 'reg_lambda': 0.00802969374599023, 'min_gain_to_split': 0.04050319088692204}. Best is trial 1 with value: 0.6552185630366003.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5971036075198506, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5971036075198506\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9971362399858694, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9971362399858694\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04561516390526959, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04561516390526959\n",
      "[I 2025-06-03 16:35:50,687] Trial 6 finished with value: 0.6571020970620063 and parameters: {'n_estimators': 550, 'learning_rate': 0.03154755304687444, 'num_leaves': 36, 'max_depth': 6, 'min_child_samples': 15, 'feature_fraction': 0.9971362399858694, 'bagging_fraction': 0.5971036075198506, 'bagging_freq': 4, 'reg_alpha': 0.022417141966577448, 'reg_lambda': 0.019155085699386988, 'min_gain_to_split': 0.04561516390526959}. Best is trial 1 with value: 0.6552185630366003.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8757926243380363, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8757926243380363\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7382764393971486, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7382764393971486\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.05894662328174673, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.05894662328174673\n",
      "[I 2025-06-03 16:35:50,828] Trial 7 finished with value: 0.6629935821039825 and parameters: {'n_estimators': 950, 'learning_rate': 0.03991526169312775, 'num_leaves': 41, 'max_depth': 10, 'min_child_samples': 16, 'feature_fraction': 0.7382764393971486, 'bagging_fraction': 0.8757926243380363, 'bagging_freq': 4, 'reg_alpha': 0.046188621340457946, 'reg_lambda': 0.022511886143750007, 'min_gain_to_split': 0.05894662328174673}. Best is trial 1 with value: 0.6552185630366003.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5202012777176407, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5202012777176407\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6669366378089369, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6669366378089369\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.052065874418045645, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.052065874418045645\n",
      "[I 2025-06-03 16:35:50,926] Trial 8 finished with value: 0.6772044737660476 and parameters: {'n_estimators': 700, 'learning_rate': 0.014033260291130079, 'num_leaves': 49, 'max_depth': 5, 'min_child_samples': 14, 'feature_fraction': 0.6669366378089369, 'bagging_fraction': 0.5202012777176407, 'bagging_freq': 7, 'reg_alpha': 0.04156923143678722, 'reg_lambda': 0.043060734707456895, 'min_gain_to_split': 0.052065874418045645}. Best is trial 1 with value: 0.6552185630366003.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8633437414123478, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8633437414123478\n",
      "[LightGBM] [Warning] feature_fraction is set=0.701164494729781, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.701164494729781\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.01048875389285563, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.01048875389285563\n",
      "[I 2025-06-03 16:35:50,989] Trial 9 finished with value: 0.6788072969576953 and parameters: {'n_estimators': 700, 'learning_rate': 0.07712475173788999, 'num_leaves': 28, 'max_depth': 6, 'min_child_samples': 33, 'feature_fraction': 0.701164494729781, 'bagging_fraction': 0.8633437414123478, 'bagging_freq': 2, 'reg_alpha': 0.012713124396315247, 'reg_lambda': 0.041390685858226736, 'min_gain_to_split': 0.01048875389285563}. Best is trial 1 with value: 0.6552185630366003.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7821419776989997, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7821419776989997\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8716648316379636, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8716648316379636\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.09734249753583984, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.09734249753583984\n",
      "[I 2025-06-03 16:35:51,068] Trial 10 finished with value: 0.6841950721397227 and parameters: {'n_estimators': 100, 'learning_rate': 0.016952118617310587, 'num_leaves': 36, 'max_depth': 2, 'min_child_samples': 22, 'feature_fraction': 0.8716648316379636, 'bagging_fraction': 0.7821419776989997, 'bagging_freq': 6, 'reg_alpha': 0.19083368096697773, 'reg_lambda': 0.8589144362297775, 'min_gain_to_split': 0.09734249753583984}. Best is trial 1 with value: 0.6552185630366003.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7698106221621432, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7698106221621432\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9783511874232185, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9783511874232185\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.03399228919409271, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.03399228919409271\n",
      "[I 2025-06-03 16:35:51,312] Trial 11 finished with value: 0.6285006989813784 and parameters: {'n_estimators': 400, 'learning_rate': 0.02200564705239667, 'num_leaves': 36, 'max_depth': 8, 'min_child_samples': 5, 'feature_fraction': 0.9783511874232185, 'bagging_fraction': 0.7698106221621432, 'bagging_freq': 5, 'reg_alpha': 0.001071252646627996, 'reg_lambda': 0.0011616943547235255, 'min_gain_to_split': 0.03399228919409271}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7694669860423335, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7694669860423335\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9674077958120045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9674077958120045\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.025609474429901833, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.025609474429901833\n",
      "[I 2025-06-03 16:35:51,532] Trial 12 finished with value: 0.6448500454984969 and parameters: {'n_estimators': 350, 'learning_rate': 0.021946865695254596, 'num_leaves': 35, 'max_depth': 9, 'min_child_samples': 9, 'feature_fraction': 0.9674077958120045, 'bagging_fraction': 0.7694669860423335, 'bagging_freq': 5, 'reg_alpha': 0.0010666195147580621, 'reg_lambda': 0.001268408958229076, 'min_gain_to_split': 0.025609474429901833}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7512501910353795, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7512501910353795\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9846543351536939, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9846543351536939\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.021452345010584017, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.021452345010584017\n",
      "[I 2025-06-03 16:35:51,716] Trial 13 finished with value: 0.6693901643884935 and parameters: {'n_estimators': 350, 'learning_rate': 0.020754619031944217, 'num_leaves': 22, 'max_depth': 10, 'min_child_samples': 5, 'feature_fraction': 0.9846543351536939, 'bagging_fraction': 0.7512501910353795, 'bagging_freq': 5, 'reg_alpha': 0.001281652750064134, 'reg_lambda': 0.001103702962559256, 'min_gain_to_split': 0.021452345010584017}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8004050368089871, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8004050368089871\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9439608629448417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9439608629448417\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.005201189756208577, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.005201189756208577\n",
      "[I 2025-06-03 16:35:51,864] Trial 14 finished with value: 0.6729450288867875 and parameters: {'n_estimators': 350, 'learning_rate': 0.011272496711926084, 'num_leaves': 43, 'max_depth': 8, 'min_child_samples': 21, 'feature_fraction': 0.9439608629448417, 'bagging_fraction': 0.8004050368089871, 'bagging_freq': 6, 'reg_alpha': 0.0010538040864516848, 'reg_lambda': 0.0011352910669547864, 'min_gain_to_split': 0.005201189756208577}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7186997575106069, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7186997575106069\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9133747572387201, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9133747572387201\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.028151844507776264, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.028151844507776264\n",
      "[I 2025-06-03 16:35:52,135] Trial 15 finished with value: 0.6467060171160566 and parameters: {'n_estimators': 350, 'learning_rate': 0.026022411214387267, 'num_leaves': 32, 'max_depth': 8, 'min_child_samples': 6, 'feature_fraction': 0.9133747572387201, 'bagging_fraction': 0.7186997575106069, 'bagging_freq': 3, 'reg_alpha': 0.0026137032792175182, 'reg_lambda': 0.002057722470245044, 'min_gain_to_split': 0.028151844507776264}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9474251752080957, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9474251752080957\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7974783040960703, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7974783040960703\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.03572423650154072, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.03572423650154072\n",
      "[I 2025-06-03 16:35:52,285] Trial 16 finished with value: 0.6653488810075854 and parameters: {'n_estimators': 500, 'learning_rate': 0.01675849078052029, 'num_leaves': 24, 'max_depth': 8, 'min_child_samples': 22, 'feature_fraction': 0.7974783040960703, 'bagging_fraction': 0.9474251752080957, 'bagging_freq': 5, 'reg_alpha': 0.003538796929280119, 'reg_lambda': 0.003457521465295961, 'min_gain_to_split': 0.03572423650154072}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7014370817231528, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7014370817231528\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9294753259115962, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9294753259115962\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.019624931912861514, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.019624931912861514\n",
      "[I 2025-06-03 16:35:52,429] Trial 17 finished with value: 0.6620291028833235 and parameters: {'n_estimators': 250, 'learning_rate': 0.05049145480036764, 'num_leaves': 41, 'max_depth': 9, 'min_child_samples': 10, 'feature_fraction': 0.9294753259115962, 'bagging_fraction': 0.7014370817231528, 'bagging_freq': 6, 'reg_alpha': 0.006585804796598983, 'reg_lambda': 0.1503449942861961, 'min_gain_to_split': 0.019624931912861514}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8277278480105511, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8277278480105511\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8813667253712263, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8813667253712263\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0009103929001068461, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0009103929001068461\n",
      "[I 2025-06-03 16:35:52,511] Trial 18 finished with value: 0.6837807720312191 and parameters: {'n_estimators': 550, 'learning_rate': 0.010130790162642145, 'num_leaves': 32, 'max_depth': 3, 'min_child_samples': 40, 'feature_fraction': 0.8813667253712263, 'bagging_fraction': 0.8277278480105511, 'bagging_freq': 3, 'reg_alpha': 0.001996833447079378, 'reg_lambda': 0.0029243114343474974, 'min_gain_to_split': 0.0009103929001068461}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9233633852750169, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9233633852750169\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9520434563555012, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9520434563555012\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.015153777058718843, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.015153777058718843\n",
      "[I 2025-06-03 16:35:52,684] Trial 19 finished with value: 0.6455605302448418 and parameters: {'n_estimators': 450, 'learning_rate': 0.03450827933469749, 'num_leaves': 43, 'max_depth': 9, 'min_child_samples': 19, 'feature_fraction': 0.9520434563555012, 'bagging_fraction': 0.9233633852750169, 'bagging_freq': 5, 'reg_alpha': 0.10439730573239, 'reg_lambda': 0.006938012587915571, 'min_gain_to_split': 0.015153777058718843}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7610138971289444, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7610138971289444\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8037963187335636, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8037963187335636\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.03018386657044292, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.03018386657044292\n",
      "[I 2025-06-03 16:35:52,824] Trial 20 finished with value: 0.6567505120421456 and parameters: {'n_estimators': 200, 'learning_rate': 0.02102376553043088, 'num_leaves': 23, 'max_depth': 7, 'min_child_samples': 26, 'feature_fraction': 0.8037963187335636, 'bagging_fraction': 0.7610138971289444, 'bagging_freq': 6, 'reg_alpha': 0.005319650704447482, 'reg_lambda': 0.0019114630036379155, 'min_gain_to_split': 0.03018386657044292}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9254198160575646, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9254198160575646\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9600765344560878, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9600765344560878\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.014066995240531947, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.014066995240531947\n",
      "[I 2025-06-03 16:35:53,042] Trial 21 finished with value: 0.6631125348366989 and parameters: {'n_estimators': 500, 'learning_rate': 0.032984481932376714, 'num_leaves': 42, 'max_depth': 9, 'min_child_samples': 10, 'feature_fraction': 0.9600765344560878, 'bagging_fraction': 0.9254198160575646, 'bagging_freq': 5, 'reg_alpha': 0.06747114273756559, 'reg_lambda': 0.005511201196812335, 'min_gain_to_split': 0.014066995240531947}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9087916719048869, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9087916719048869\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9109254402954241, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9109254402954241\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.016035797353274664, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.016035797353274664\n",
      "[I 2025-06-03 16:35:53,183] Trial 22 finished with value: 0.6359352197954005 and parameters: {'n_estimators': 450, 'learning_rate': 0.031122357113779534, 'num_leaves': 38, 'max_depth': 10, 'min_child_samples': 18, 'feature_fraction': 0.9109254402954241, 'bagging_fraction': 0.9087916719048869, 'bagging_freq': 5, 'reg_alpha': 0.11532250048368878, 'reg_lambda': 0.009525071263219905, 'min_gain_to_split': 0.016035797353274664}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9914868154320267, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9914868154320267\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9122628203311232, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9122628203311232\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.039015342556297246, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.039015342556297246\n",
      "[I 2025-06-03 16:35:53,469] Trial 23 finished with value: 0.6394202729199914 and parameters: {'n_estimators': 400, 'learning_rate': 0.025467543627013194, 'num_leaves': 38, 'max_depth': 10, 'min_child_samples': 5, 'feature_fraction': 0.9122628203311232, 'bagging_fraction': 0.9914868154320267, 'bagging_freq': 4, 'reg_alpha': 0.11176243490908287, 'reg_lambda': 0.0011347103274905207, 'min_gain_to_split': 0.039015342556297246}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9892743493059847, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9892743493059847\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8967774608840943, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8967774608840943\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.03710547988367775, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.03710547988367775\n",
      "[I 2025-06-03 16:35:53,758] Trial 24 finished with value: 0.6340899584883192 and parameters: {'n_estimators': 600, 'learning_rate': 0.02697078875578633, 'num_leaves': 39, 'max_depth': 10, 'min_child_samples': 5, 'feature_fraction': 0.8967774608840943, 'bagging_fraction': 0.9892743493059847, 'bagging_freq': 3, 'reg_alpha': 0.1255166981459959, 'reg_lambda': 0.00331536515846342, 'min_gain_to_split': 0.03710547988367775}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8981933474390853, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8981933474390853\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8401666394579269, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8401666394579269\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.032558301408418894, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.032558301408418894\n",
      "[I 2025-06-03 16:35:53,858] Trial 25 finished with value: 0.6671311208437584 and parameters: {'n_estimators': 600, 'learning_rate': 0.056834258458450714, 'num_leaves': 46, 'max_depth': 7, 'min_child_samples': 26, 'feature_fraction': 0.8401666394579269, 'bagging_fraction': 0.8981933474390853, 'bagging_freq': 3, 'reg_alpha': 0.25438498592790293, 'reg_lambda': 0.003664091607820215, 'min_gain_to_split': 0.032558301408418894}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9876570310015166, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9876570310015166\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8917942784774511, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8917942784774511\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.042909102907228276, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.042909102907228276\n",
      "[I 2025-06-03 16:35:54,032] Trial 26 finished with value: 0.6593314492748062 and parameters: {'n_estimators': 650, 'learning_rate': 0.02929322980246965, 'num_leaves': 39, 'max_depth': 10, 'min_child_samples': 18, 'feature_fraction': 0.8917942784774511, 'bagging_fraction': 0.9876570310015166, 'bagging_freq': 2, 'reg_alpha': 0.1309018338603457, 'reg_lambda': 0.010547826635599028, 'min_gain_to_split': 0.042909102907228276}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8218693603813939, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8218693603813939\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7660375141839929, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7660375141839929\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.05728831357229655, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.05728831357229655\n",
      "[I 2025-06-03 16:35:54,189] Trial 27 finished with value: 0.6513895397356364 and parameters: {'n_estimators': 250, 'learning_rate': 0.03922077293219158, 'num_leaves': 31, 'max_depth': 7, 'min_child_samples': 13, 'feature_fraction': 0.7660375141839929, 'bagging_fraction': 0.8218693603813939, 'bagging_freq': 3, 'reg_alpha': 0.06315769278418414, 'reg_lambda': 0.0020283479169311185, 'min_gain_to_split': 0.05728831357229655}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9539202003737586, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9539202003737586\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8584183509486556, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8584183509486556\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.01969450514054366, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.01969450514054366\n",
      "[I 2025-06-03 16:35:54,426] Trial 28 finished with value: 0.6644203016017692 and parameters: {'n_estimators': 800, 'learning_rate': 0.017033245352499345, 'num_leaves': 33, 'max_depth': 8, 'min_child_samples': 11, 'feature_fraction': 0.8584183509486556, 'bagging_fraction': 0.9539202003737586, 'bagging_freq': 1, 'reg_alpha': 0.02721250613168775, 'reg_lambda': 0.004586723930458048, 'min_gain_to_split': 0.01969450514054366}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9077464968557384, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9077464968557384\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5901314030588533, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5901314030588533\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.06938514396383288, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.06938514396383288\n",
      "[I 2025-06-03 16:35:54,501] Trial 29 finished with value: 0.686303790927049 and parameters: {'n_estimators': 450, 'learning_rate': 0.039688728627623515, 'num_leaves': 46, 'max_depth': 4, 'min_child_samples': 42, 'feature_fraction': 0.5901314030588533, 'bagging_fraction': 0.9077464968557384, 'bagging_freq': 4, 'reg_alpha': 0.014153037002781107, 'reg_lambda': 0.011250763174502312, 'min_gain_to_split': 0.06938514396383288}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8400540150440534, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8400540150440534\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9295847171443113, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9295847171443113\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.007392166673986549, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.007392166673986549\n",
      "[I 2025-06-03 16:35:54,790] Trial 30 finished with value: 0.6606393886895727 and parameters: {'n_estimators': 600, 'learning_rate': 0.014192392347224329, 'num_leaves': 28, 'max_depth': 10, 'min_child_samples': 7, 'feature_fraction': 0.9295847171443113, 'bagging_fraction': 0.8400540150440534, 'bagging_freq': 4, 'reg_alpha': 0.17695975408301054, 'reg_lambda': 0.0021835737581505582, 'min_gain_to_split': 0.007392166673986549}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9977615836767824, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9977615836767824\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8925857546768011, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8925857546768011\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.040481491964568614, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.040481491964568614\n",
      "[I 2025-06-03 16:35:55,061] Trial 31 finished with value: 0.6286844871368193 and parameters: {'n_estimators': 400, 'learning_rate': 0.028208246169449617, 'num_leaves': 39, 'max_depth': 10, 'min_child_samples': 5, 'feature_fraction': 0.8925857546768011, 'bagging_fraction': 0.9977615836767824, 'bagging_freq': 4, 'reg_alpha': 0.09247623910216148, 'reg_lambda': 0.001381698862858631, 'min_gain_to_split': 0.040481491964568614}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.956747279359327, subsample=1.0 will be ignored. Current value: bagging_fraction=0.956747279359327\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9125498646447657, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9125498646447657\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04528596793392606, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04528596793392606\n",
      "[I 2025-06-03 16:35:55,361] Trial 32 finished with value: 0.6540956522241085 and parameters: {'n_estimators': 450, 'learning_rate': 0.02785688463020398, 'num_leaves': 39, 'max_depth': 9, 'min_child_samples': 5, 'feature_fraction': 0.9125498646447657, 'bagging_fraction': 0.956747279359327, 'bagging_freq': 3, 'reg_alpha': 0.08793684143482117, 'reg_lambda': 0.002961127068368011, 'min_gain_to_split': 0.04528596793392606}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9985702720597257, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9985702720597257\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8393689117824594, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8393689117824594\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.03550603241395796, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.03550603241395796\n",
      "[I 2025-06-03 16:35:55,561] Trial 33 finished with value: 0.6629359270671965 and parameters: {'n_estimators': 250, 'learning_rate': 0.023659846886755456, 'num_leaves': 38, 'max_depth': 10, 'min_child_samples': 13, 'feature_fraction': 0.8393689117824594, 'bagging_fraction': 0.9985702720597257, 'bagging_freq': 4, 'reg_alpha': 0.3153678708036274, 'reg_lambda': 0.0016922844671775088, 'min_gain_to_split': 0.03550603241395796}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8837892867368883, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8837892867368883\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9816483387767156, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9816483387767156\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.05022143756288463, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.05022143756288463\n",
      "[I 2025-06-03 16:35:55,815] Trial 34 finished with value: 0.6573275563945233 and parameters: {'n_estimators': 400, 'learning_rate': 0.019230762907074324, 'num_leaves': 45, 'max_depth': 9, 'min_child_samples': 8, 'feature_fraction': 0.9816483387767156, 'bagging_fraction': 0.8837892867368883, 'bagging_freq': 5, 'reg_alpha': 0.5920012087731403, 'reg_lambda': 0.004620788895514428, 'min_gain_to_split': 0.05022143756288463}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9366487020871259, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9366487020871259\n",
      "[LightGBM] [Warning] feature_fraction is set=0.880373776938879, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.880373776938879\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02416723472010985, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02416723472010985\n",
      "[I 2025-06-03 16:35:55,960] Trial 35 finished with value: 0.6484194536727541 and parameters: {'n_estimators': 600, 'learning_rate': 0.03536078952147814, 'num_leaves': 35, 'max_depth': 8, 'min_child_samples': 17, 'feature_fraction': 0.880373776938879, 'bagging_fraction': 0.9366487020871259, 'bagging_freq': 6, 'reg_alpha': 0.1664445632822472, 'reg_lambda': 0.011216864305170509, 'min_gain_to_split': 0.02416723472010985}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9699260928862403, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9699260928862403\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8283725972878453, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8283725972878453\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.060955733488533455, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.060955733488533455\n",
      "[I 2025-06-03 16:35:56,219] Trial 36 finished with value: 0.6668962907467537 and parameters: {'n_estimators': 500, 'learning_rate': 0.029047830043689944, 'num_leaves': 39, 'max_depth': 0, 'min_child_samples': 11, 'feature_fraction': 0.8283725972878453, 'bagging_fraction': 0.9699260928862403, 'bagging_freq': 1, 'reg_alpha': 0.06598610016578718, 'reg_lambda': 0.0015278675248590087, 'min_gain_to_split': 0.060955733488533455}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7154593573760901, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7154593573760901\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7586911182516987, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7586911182516987\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.037100126046056045, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.037100126046056045\n",
      "[I 2025-06-03 16:35:56,384] Trial 37 finished with value: 0.662153558826031 and parameters: {'n_estimators': 300, 'learning_rate': 0.04882256028232769, 'num_leaves': 34, 'max_depth': 10, 'min_child_samples': 7, 'feature_fraction': 0.7586911182516987, 'bagging_fraction': 0.7154593573760901, 'bagging_freq': 7, 'reg_alpha': 0.4621065534382243, 'reg_lambda': 0.007860847808152046, 'min_gain_to_split': 0.037100126046056045}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5564541378440941, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5564541378440941\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8908229837315332, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8908229837315332\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.07428507654375081, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.07428507654375081\n",
      "[I 2025-06-03 16:35:56,462] Trial 38 finished with value: 0.6818835051255612 and parameters: {'n_estimators': 550, 'learning_rate': 0.023215722893073082, 'num_leaves': 29, 'max_depth': 7, 'min_child_samples': 36, 'feature_fraction': 0.8908229837315332, 'bagging_fraction': 0.5564541378440941, 'bagging_freq': 2, 'reg_alpha': 0.038382918520487794, 'reg_lambda': 0.0027790344984138654, 'min_gain_to_split': 0.07428507654375081}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6543072926143603, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6543072926143603\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9902301373865393, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9902301373865393\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.031086702173761274, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.031086702173761274\n",
      "[I 2025-06-03 16:35:56,563] Trial 39 finished with value: 0.6761649130244711 and parameters: {'n_estimators': 400, 'learning_rate': 0.019033312339850637, 'num_leaves': 50, 'max_depth': 9, 'min_child_samples': 30, 'feature_fraction': 0.9902301373865393, 'bagging_fraction': 0.6543072926143603, 'bagging_freq': 5, 'reg_alpha': 0.02072152460024268, 'reg_lambda': 0.015201208880868476, 'min_gain_to_split': 0.031086702173761274}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9067182938850711, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9067182938850711\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7772423657683665, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7772423657683665\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.05488892838120793, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.05488892838120793\n",
      "[I 2025-06-03 16:35:56,650] Trial 40 finished with value: 0.6811905064182082 and parameters: {'n_estimators': 150, 'learning_rate': 0.04275677459026624, 'num_leaves': 14, 'max_depth': 10, 'min_child_samples': 49, 'feature_fraction': 0.7772423657683665, 'bagging_fraction': 0.9067182938850711, 'bagging_freq': 4, 'reg_alpha': 0.24574523894220296, 'reg_lambda': 0.11512237393037413, 'min_gain_to_split': 0.05488892838120793}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9820378149041505, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9820378149041505\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9115997748876921, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9115997748876921\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0398101860818647, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0398101860818647\n",
      "[I 2025-06-03 16:35:56,924] Trial 41 finished with value: 0.6510013284197472 and parameters: {'n_estimators': 400, 'learning_rate': 0.02579651715786897, 'num_leaves': 37, 'max_depth': 10, 'min_child_samples': 5, 'feature_fraction': 0.9115997748876921, 'bagging_fraction': 0.9820378149041505, 'bagging_freq': 4, 'reg_alpha': 0.11439426839620673, 'reg_lambda': 0.0011768469197385326, 'min_gain_to_split': 0.0398101860818647}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9727631298166123, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9727631298166123\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9349438093568126, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9349438093568126\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.046971169224014654, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.046971169224014654\n",
      "[I 2025-06-03 16:35:57,156] Trial 42 finished with value: 0.6457868621900941 and parameters: {'n_estimators': 300, 'learning_rate': 0.02962594421901723, 'num_leaves': 40, 'max_depth': 9, 'min_child_samples': 8, 'feature_fraction': 0.9349438093568126, 'bagging_fraction': 0.9727631298166123, 'bagging_freq': 4, 'reg_alpha': 0.08271382570457132, 'reg_lambda': 0.0015472631872498219, 'min_gain_to_split': 0.046971169224014654}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9991251315373986, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9991251315373986\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8606175933140942, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8606175933140942\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.040713777953444566, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.040713777953444566\n",
      "[I 2025-06-03 16:35:57,365] Trial 43 finished with value: 0.6624146795058505 and parameters: {'n_estimators': 450, 'learning_rate': 0.025963492062483023, 'num_leaves': 37, 'max_depth': 10, 'min_child_samples': 12, 'feature_fraction': 0.8606175933140942, 'bagging_fraction': 0.9991251315373986, 'bagging_freq': 3, 'reg_alpha': 0.8686145653286205, 'reg_lambda': 0.0022957329007536604, 'min_gain_to_split': 0.040713777953444566}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8622189550093293, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8622189550093293\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7321414450080631, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7321414450080631\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04755876963889479, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04755876963889479\n",
      "[I 2025-06-03 16:35:57,464] Trial 44 finished with value: 0.6720516191271971 and parameters: {'n_estimators': 900, 'learning_rate': 0.03274989601541992, 'num_leaves': 43, 'max_depth': 5, 'min_child_samples': 14, 'feature_fraction': 0.7321414450080631, 'bagging_fraction': 0.8622189550093293, 'bagging_freq': 0, 'reg_alpha': 0.045983820480767025, 'reg_lambda': 0.0010656052921059424, 'min_gain_to_split': 0.04755876963889479}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9323106524173164, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9323106524173164\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9633768742955305, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9633768742955305\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.06212033588055907, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.06212033588055907\n",
      "[I 2025-06-03 16:35:57,633] Trial 45 finished with value: 0.6701307717754992 and parameters: {'n_estimators': 700, 'learning_rate': 0.03651908671383635, 'num_leaves': 34, 'max_depth': 8, 'min_child_samples': 9, 'feature_fraction': 0.9633768742955305, 'bagging_fraction': 0.9323106524173164, 'bagging_freq': 5, 'reg_alpha': 0.13134630649735277, 'reg_lambda': 0.0256851764465738, 'min_gain_to_split': 0.06212033588055907}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9646141565125865, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9646141565125865\n",
      "[LightGBM] [Warning] feature_fraction is set=0.904595647062191, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.904595647062191\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.01590601893248146, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.01590601893248146\n",
      "[I 2025-06-03 16:35:57,922] Trial 46 finished with value: 0.6431764089880301 and parameters: {'n_estimators': 300, 'learning_rate': 0.02366560075779784, 'num_leaves': 41, 'max_depth': 9, 'min_child_samples': 5, 'feature_fraction': 0.904595647062191, 'bagging_fraction': 0.9646141565125865, 'bagging_freq': 2, 'reg_alpha': 0.2423564678918263, 'reg_lambda': 0.0010017231842563928, 'min_gain_to_split': 0.01590601893248146}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8849079834954062, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8849079834954062\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9989781238249458, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9989781238249458\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.09818281284166896, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.09818281284166896\n",
      "[I 2025-06-03 16:35:58,101] Trial 47 finished with value: 0.6636280017430894 and parameters: {'n_estimators': 500, 'learning_rate': 0.019500382826393808, 'num_leaves': 36, 'max_depth': 10, 'min_child_samples': 15, 'feature_fraction': 0.9989781238249458, 'bagging_fraction': 0.8849079834954062, 'bagging_freq': 4, 'reg_alpha': 0.009337191893540826, 'reg_lambda': 0.8265379924056608, 'min_gain_to_split': 0.09818281284166896}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7989458522121001, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7989458522121001\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8225718510383362, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8225718510383362\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.024198727569351416, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.024198727569351416\n",
      "[I 2025-06-03 16:35:58,301] Trial 48 finished with value: 0.6636732856947387 and parameters: {'n_estimators': 400, 'learning_rate': 0.026975242390426743, 'num_leaves': 44, 'max_depth': 6, 'min_child_samples': 7, 'feature_fraction': 0.8225718510383362, 'bagging_fraction': 0.7989458522121001, 'bagging_freq': 6, 'reg_alpha': 0.03201958470443153, 'reg_lambda': 0.00423200438956861, 'min_gain_to_split': 0.024198727569351416}. Best is trial 11 with value: 0.6285006989813784.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6867208302634296, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6867208302634296\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9376330015311435, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9376330015311435\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.03360979467794702, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.03360979467794702\n",
      "[I 2025-06-03 16:35:58,401] Trial 49 finished with value: 0.6653725108602163 and parameters: {'n_estimators': 650, 'learning_rate': 0.06005739219390964, 'num_leaves': 47, 'max_depth': 8, 'min_child_samples': 20, 'feature_fraction': 0.9376330015311435, 'bagging_fraction': 0.6867208302634296, 'bagging_freq': 3, 'reg_alpha': 0.05551657759685546, 'reg_lambda': 0.0014510181752053603, 'min_gain_to_split': 0.03360979467794702}. Best is trial 11 with value: 0.6285006989813784.\n",
      "Best Optuna trial for LightGBM: Value=0.6285, Params={'n_estimators': 400, 'learning_rate': 0.02200564705239667, 'num_leaves': 36, 'max_depth': 8, 'min_child_samples': 5, 'feature_fraction': 0.9783511874232185, 'bagging_fraction': 0.7698106221621432, 'bagging_freq': 5, 'reg_alpha': 0.001071252646627996, 'reg_lambda': 0.0011616943547235255, 'min_gain_to_split': 0.03399228919409271}\n",
      "\n",
      "--- LightGBM Model Training: AAPL ---\n",
      "Training LightGBM model...\n",
      "Using Optuna-optimized parameters.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7698106221621432, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7698106221621432\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9783511874232185, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9783511874232185\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.03399228919409271, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.03399228919409271\n",
      "\n",
      "Top 10 features:\n",
      "                        Feature  Importance\n",
      "10                      ADX_14          14\n",
      "7                MACDh_12_26_9          12\n",
      "0   close_trans_seq_volatility          11\n",
      "3         close_entropy_sample          11\n",
      "14                      SMA_50          10\n",
      "5                       RSI_14           9\n",
      "8                  log_returns           8\n",
      "31        close_quarterly_mean           8\n",
      "26               STCD_23_50_05           6\n",
      "2                close_cwt_std           6\n",
      "\n",
      "ðŸŽ¯ Accuracy on mapped test data: 0.6071\n",
      "ðŸ“Š AUC: 0.6778\n",
      "\n",
      "Classification Report (on mapped and valid test labels):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.51      0.63        75\n",
      "           1       0.45      0.81      0.58        37\n",
      "\n",
      "    accuracy                           0.61       112\n",
      "   macro avg       0.65      0.66      0.61       112\n",
      "weighted avg       0.71      0.61      0.61       112\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The maximum opset needed by this model is only 9.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance plot saved to feature_importance_AAPL.png\n",
      "\n",
      "Exporting LGBM model to ONNX: lgbm_model_AAPL.onnx (opset=12)\n",
      "Model exported to ONNX: lgbm_model_AAPL.onnx\n",
      "ONNX model check OK.\n",
      "\n",
      "--- Foundation Model Forecasting (PatchTST): AAPL ---\n",
      "\n",
      "ðŸ Workflow completed for AAPL. Final Status: LightGBM Model Trained\n",
      "\n",
      "--- Results Summary for AAPL ---\n",
      "  Overall Status: LightGBM Model Trained\n",
      "  Top LGBM Features: ['ADX_14', 'MACDh_12_26_9', 'close_trans_seq_volatility']\n",
      "  ONNX Model Path: lgbm_model_AAPL.onnx\n",
      "----------------------------------------\n",
      "DEBUG: Starting main loop for symbol: GOOGL\n",
      "\n",
      "========================================\n",
      "ðŸš€ ENHANCED WORKFLOW FOR: GOOGL\n",
      "========================================\n",
      "Fetching data for GOOGL from Twelve Data (interval=1day, from 2022-06-04 to 2025-06-03)...\n",
      "Successfully fetched/processed 750 data points for GOOGL.\n",
      "\n",
      "--- ðŸ”§ Feature Engineering: GOOGL ---\n",
      "Adding technical indicators...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38379/4212630612.py:192: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[3.06651079e+09 4.35166168e+09 5.49533105e+09 5.25738335e+09\n",
      " 3.43865086e+09 3.17167519e+09 4.78883455e+09 4.24405295e+09\n",
      " 4.35871985e+09 4.26272497e+09 4.81415547e+09 4.53681124e+09\n",
      " 5.23540262e+09 4.10838675e+09 4.05583490e+09 6.68380011e+09\n",
      " 3.71943344e+09 4.29421548e+09 3.34559263e+09 2.54813406e+09\n",
      " 2.26042461e+09 3.70164717e+09 2.30950652e+09 2.37180236e+09\n",
      " 1.96404962e+09 3.09653760e+09 2.52002904e+09 2.55685047e+09\n",
      " 2.54479800e+09 2.36472752e+09 3.22347074e+09 2.74643330e+09\n",
      " 2.92220136e+09 2.26916019e+09 2.13695967e+09 3.37931161e+09\n",
      " 2.48702991e+09 2.83936227e+09 2.43983518e+09 2.54650099e+09\n",
      " 2.76519870e+09 4.22385634e+09 4.15080653e+09 4.37850685e+09\n",
      " 3.00166951e+09 2.70625383e+09 4.81684360e+09 3.16852568e+09\n",
      " 4.09870394e+09 2.87297429e+09 1.85102348e+09 4.31310047e+09\n",
      " 2.90208266e+09 3.92662859e+09 2.21159676e+09 2.03844918e+09\n",
      " 2.05264799e+09 2.51952407e+09 3.58487934e+09 2.57100238e+09\n",
      " 2.43665204e+09 2.75096559e+09 2.40859522e+09 2.66476038e+09\n",
      " 3.41994701e+09 6.13669823e+09 3.96490547e+09 2.91573786e+09\n",
      " 3.35410020e+09 2.93270533e+09 3.54363504e+09 7.48222552e+09\n",
      " 5.21043773e+09 4.81973583e+09 2.46943152e+09 2.71890885e+09\n",
      " 2.81100061e+09 2.93993968e+09 3.27961693e+09 3.10182148e+09\n",
      " 3.23083296e+09 3.37393822e+09 4.82057384e+09 6.48392631e+09\n",
      " 6.19561328e+09 4.36880323e+09 4.53512338e+09 4.30226400e+09\n",
      " 2.92141720e+09 3.79275236e+09 2.59383434e+09 2.55873809e+09\n",
      " 5.22706869e+09 2.65627771e+09 2.87618257e+09 2.92498618e+09\n",
      " 2.71924454e+09 2.49004348e+09 5.33449980e+09 3.85226170e+09\n",
      " 2.86392474e+09 2.30929528e+09 2.80060552e+09 2.83574772e+09\n",
      " 3.93525362e+09 7.00202797e+09 9.17607000e+09 4.81453391e+09\n",
      " 5.35458272e+09 4.01438301e+09 4.30996479e+09 5.10952722e+09\n",
      " 4.39745926e+09 5.22981697e+09 4.42265276e+09 3.80309542e+09\n",
      " 3.36661267e+09 4.06273970e+09 3.38270883e+09 2.91407458e+09\n",
      " 3.48498964e+09 2.76377164e+09 4.38170248e+09 3.28116083e+09\n",
      " 4.25821598e+09 3.22766940e+09 2.87051101e+09 3.54698877e+09\n",
      " 3.34318581e+09 2.60260586e+09 3.62491392e+09 5.46074979e+09\n",
      " 4.16600136e+09 3.22266845e+09 3.61685547e+09 6.41282236e+09\n",
      " 7.98927140e+09 5.86520002e+09 4.84089403e+09 3.72231022e+09\n",
      " 3.37538621e+09 2.97339637e+09 3.07879618e+09 2.71978761e+09\n",
      " 3.22409298e+09 4.36139752e+09 3.22897208e+09 2.85170955e+09\n",
      " 3.66895477e+09 2.69589715e+09 5.79194561e+09 3.83424254e+09\n",
      " 4.10695185e+09 3.20306507e+09 2.83897961e+09 2.82027197e+09\n",
      " 3.40771785e+09 3.01671373e+09 2.95712075e+09 2.96722650e+09\n",
      " 2.96995029e+09 3.59675774e+09 2.67085635e+09 3.76740535e+09\n",
      " 2.65375899e+09 3.84483182e+09 3.62587274e+09 3.95910411e+09\n",
      " 3.26712832e+09 6.20812498e+09 3.59445689e+09 3.77694559e+09\n",
      " 3.46652388e+09 3.39854837e+09 2.47215133e+09 3.89582572e+09\n",
      " 3.47789963e+09 3.54179028e+09 2.41844600e+09 4.34786788e+09\n",
      " 3.20714475e+09 3.81418393e+09 3.76927840e+09 3.09432096e+09\n",
      " 2.46651265e+09 3.20947243e+09 3.57262110e+09 3.07858023e+09\n",
      " 7.76176539e+09 3.99029109e+09 6.72945677e+09 4.35891580e+09\n",
      " 3.48352772e+09 6.84407058e+09 3.85101911e+09 3.74907632e+09\n",
      " 2.37939055e+09 3.35442506e+09 2.95453670e+09 3.47121339e+09\n",
      " 3.02881868e+09 3.42476104e+09 3.23651630e+09 3.68757122e+09\n",
      " 4.85313302e+09 4.72508680e+09 3.75684776e+09 4.41569993e+09\n",
      " 3.96729252e+09 4.24840451e+09 5.73288889e+09 5.54387901e+09\n",
      " 4.20077201e+09 3.65604660e+09 3.28935084e+09 3.97631067e+09\n",
      " 3.30383947e+09 3.31020757e+09 3.91463653e+09 2.80756199e+09\n",
      " 5.81042159e+09 5.04345884e+09 5.34410436e+09 4.46452265e+09\n",
      " 3.81211009e+09 3.26759626e+09 6.08737189e+09 1.03217808e+10\n",
      " 3.15651745e+09 3.67100792e+09 4.38928956e+09 3.34384955e+09\n",
      " 3.69345919e+09 4.88485110e+09 3.81058157e+09 3.19314556e+09\n",
      " 4.87799995e+09 4.30632717e+09 3.38873232e+09 3.09889807e+09\n",
      " 4.12543087e+09 3.33849797e+09 3.61329685e+09 1.11291137e+10\n",
      " 5.51599506e+09 4.02673239e+09 5.75684720e+09 3.66176602e+09\n",
      " 4.77983410e+09 2.60295379e+09 4.27487352e+09 4.63216021e+09\n",
      " 4.84892161e+09 4.29220045e+09 3.99791778e+09 3.01171094e+09\n",
      " 3.62098143e+09 4.74914288e+09 4.65491953e+09 3.86926996e+09\n",
      " 4.10480897e+09 3.78561685e+09 4.96279586e+09 3.17919347e+09\n",
      " 3.46861626e+09 3.55207522e+09 1.04880610e+10 3.28619881e+09\n",
      " 4.24235576e+09 3.64290324e+09 3.49006237e+09 3.19868272e+09\n",
      " 1.89789529e+09 3.96467475e+09 2.86832796e+09 3.04139724e+09\n",
      " 3.07344441e+09 4.37500779e+09 6.62758167e+09 3.43607857e+09\n",
      " 3.22289614e+09 4.42368979e+09 4.02544402e+09 4.14316884e+09\n",
      " 4.63853022e+09 2.59109275e+09 3.04161305e+09 5.07068366e+09\n",
      " 3.94721229e+09 3.71824974e+09 3.07305719e+09 2.30562115e+09\n",
      " 2.35641057e+09 3.60418826e+09 2.94452934e+09 4.44293146e+09\n",
      " 4.57461373e+09 4.64631178e+09 2.89733046e+09 3.30241197e+09\n",
      " 3.77976533e+09 4.31846277e+09 6.67129338e+09 3.31265549e+09\n",
      " 3.46258265e+09 3.37006264e+09 4.72991237e+09 3.17945549e+09\n",
      " 2.49794980e+09 3.13505763e+09 3.35948291e+09 2.72760630e+09\n",
      " 3.26700956e+09 5.35021987e+09 7.12866925e+09 1.21831086e+10\n",
      " 3.09574524e+09 5.91861446e+09 4.55524007e+09 3.13740747e+09\n",
      " 4.55360613e+09 3.52699461e+09 4.14446206e+09 5.54342513e+09\n",
      " 3.45897450e+09 3.25430454e+09 4.06111673e+09 3.81839606e+09\n",
      " 5.48353369e+09 3.70907361e+09 3.73195374e+09 4.44554953e+09\n",
      " 1.00961892e+10 1.30324988e+10 6.71803674e+09 8.81143432e+09\n",
      " 8.58217909e+09 4.96280633e+09 2.03211127e+09 2.35603391e+09\n",
      " 3.56414769e+09 5.80372896e+09 5.21877319e+09 3.26294475e+09\n",
      " 4.23739184e+09 5.42664967e+09 5.97637705e+09 4.76841634e+09\n",
      " 4.68228410e+09 3.55639422e+09 4.87045282e+09 6.52909158e+09\n",
      " 9.00868936e+09 5.59013943e+09 4.31712732e+09 3.96345763e+09\n",
      " 3.78942933e+09 3.61220787e+09 7.71611114e+09 5.32279826e+09\n",
      " 4.88360453e+09 4.73865626e+09 4.71246649e+09 5.27324449e+09\n",
      " 5.60703115e+09 5.97147068e+09 5.15825072e+09 4.10569960e+09\n",
      " 4.79304977e+09 3.92445169e+09 1.08465447e+10 5.24309885e+09\n",
      " 4.83659692e+09 4.06554923e+09 4.84249999e+09 7.26134853e+09\n",
      " 9.13541004e+09 4.85425830e+09 4.20757401e+09 3.50233173e+09\n",
      " 6.97113524e+09 6.72829979e+09 7.99564256e+09 5.44202543e+09\n",
      " 7.15771761e+09 1.23647227e+10 1.29353208e+10 6.53614684e+09\n",
      " 6.02389312e+09]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  mfi_series = df_feat.ta.mfi(high=df_feat[h], low=df_feat[l], close=df_feat[c], volume=df_feat[v], append=False)\n",
      "/tmp/ipykernel_38379/4212630612.py:192: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[3.68436805e+09 2.99179836e+09 4.62613301e+09 5.06073645e+09\n",
      " 3.60025021e+09 5.50985898e+09 4.07032589e+09 2.76600463e+09\n",
      " 4.69978839e+09 3.83989859e+09 3.62918666e+09 3.44224921e+09\n",
      " 4.90780907e+09 4.06838630e+09 4.76364524e+09 3.70330267e+09\n",
      " 5.32599270e+09 3.74291684e+09 5.05969108e+09 3.00192835e+09\n",
      " 2.32848907e+09 2.24536567e+09 2.22459520e+09 2.90625533e+09\n",
      " 2.31521951e+09 2.74331545e+09 2.15678519e+09 2.56712355e+09\n",
      " 2.45835418e+09 1.81081265e+09 2.01784279e+09 4.16717307e+09\n",
      " 2.32516910e+09 3.00115076e+09 3.12225147e+09 2.62425470e+09\n",
      " 2.67913277e+09 2.62694695e+09 4.02991300e+09 2.76215923e+09\n",
      " 3.53883093e+09 4.37772559e+09 2.68733130e+09 3.16373895e+09\n",
      " 3.43180359e+09 3.11900237e+09 2.66981943e+09 2.95084338e+09\n",
      " 3.02213445e+09 3.18555286e+09 2.23365737e+09 2.72745268e+09\n",
      " 1.78587755e+09 2.58656282e+09 2.37769705e+09 3.24760113e+09\n",
      " 8.45689760e+09 5.65522034e+09 3.00405699e+09 4.35252559e+09\n",
      " 5.03895355e+09 5.17454584e+09 2.79610439e+09 2.88717217e+09\n",
      " 2.55688901e+09 2.76526374e+09 2.08796389e+09 2.25259615e+09\n",
      " 9.49120300e+08 2.52964486e+09 1.90867014e+09 2.14885763e+09\n",
      " 2.43937110e+09 2.43443968e+09 2.96524964e+09 3.03599615e+09\n",
      " 2.63286531e+09 2.73096053e+09 2.73486405e+09 3.66992422e+09\n",
      " 5.24252637e+09 2.62694151e+09 2.07675579e+09 2.42120758e+09\n",
      " 1.76414101e+09 1.69201065e+09 2.10359851e+09 3.09045896e+09\n",
      " 2.35385662e+09 2.67019799e+09 2.97323024e+09 3.24731634e+09\n",
      " 4.02332540e+09 2.64665890e+09 6.86117736e+09 3.29745211e+09\n",
      " 9.49171967e+09 1.14943583e+10 5.23361283e+09 4.73469526e+09\n",
      " 5.14042257e+09 4.07541881e+09 3.23550480e+09 3.10073354e+09\n",
      " 2.82454769e+09 3.74415086e+09 3.26339092e+09 2.62637221e+09\n",
      " 2.68685888e+09 3.28045487e+09 3.39878412e+09 3.32724251e+09\n",
      " 3.18896635e+09 3.20700874e+09 3.24019880e+09 3.32880258e+09\n",
      " 2.96324125e+09 2.87952024e+09 2.77817526e+09 2.56481161e+09\n",
      " 3.96564016e+09 2.79359720e+09 2.17579557e+09 4.89008659e+09\n",
      " 5.56042035e+09 3.26741485e+09 2.45327069e+09 4.24321562e+09\n",
      " 4.19223441e+09 4.13233451e+09 4.34167700e+09 4.59642341e+09\n",
      " 6.54390746e+09 3.59145820e+09 3.76458649e+09 5.65769363e+09\n",
      " 3.21703881e+09 3.67019393e+09 4.06064861e+09 4.65877867e+09\n",
      " 1.73089133e+09 2.95960840e+09 4.13070123e+09 2.69406525e+09\n",
      " 3.24682832e+09 4.57867607e+09 4.56789571e+09 8.74622664e+09\n",
      " 3.04921414e+09 3.38076063e+09 2.57989359e+09 3.24448995e+09\n",
      " 2.52873196e+09 2.57335431e+09 3.25161609e+09 3.89363737e+09\n",
      " 3.73929194e+09 3.46035367e+09 2.92713050e+09 2.63086633e+09\n",
      " 2.52028999e+09 2.53637990e+09 2.49946690e+09 5.34860792e+09\n",
      " 2.80250091e+09 4.04460199e+09 4.12229075e+09 3.44819139e+09\n",
      " 2.62060383e+09 3.31042542e+09 4.06444286e+09 3.05398837e+09\n",
      " 3.45437441e+09 3.23187286e+09 3.24180091e+09 3.60397205e+09\n",
      " 3.58369176e+09 3.58019939e+09 1.06579820e+10 6.99721249e+09\n",
      " 5.43250780e+09 3.26147219e+09 3.10985370e+09 5.02772912e+09\n",
      " 1.71366657e+09 2.55895503e+09 3.26049789e+09 5.19012250e+09\n",
      " 4.13466959e+09 4.73286175e+09 4.35608924e+09 4.13087077e+09\n",
      " 3.83987803e+09 5.10296695e+09 2.76331432e+09 2.25238839e+09\n",
      " 2.61450439e+09 3.27293597e+09 3.72600630e+09 3.06111600e+09\n",
      " 2.67558905e+09 2.95105080e+09 3.17127517e+09 5.52570358e+09\n",
      " 1.01625611e+10 8.77711930e+09 3.19321193e+09 4.04110612e+09\n",
      " 5.34727720e+09 4.44351624e+09 3.54280451e+09 7.46336262e+09\n",
      " 4.58114990e+09 5.09908669e+09 4.28339171e+09 7.48376000e+09\n",
      " 5.31348473e+09 4.66005381e+09 6.99686100e+09 3.54556985e+09\n",
      " 2.87330621e+09 3.44263315e+09 3.78140867e+09 5.27526068e+09\n",
      " 3.56054179e+09 3.55848219e+09 4.01121654e+09 4.23953549e+09\n",
      " 3.21029401e+09 4.97288113e+09 8.82078372e+09 7.61550170e+09\n",
      " 5.52087352e+09 3.31546215e+09 5.01323718e+09 5.24653760e+09\n",
      " 3.15129040e+09 3.67760542e+09 2.89692824e+09 4.11220116e+09\n",
      " 3.97277337e+09 6.46134447e+09 3.45142776e+09 4.16001237e+09\n",
      " 3.67223613e+09 3.83875069e+09 5.33799033e+09 2.92468800e+09\n",
      " 3.97564485e+09 4.79638109e+09 4.24895091e+09 3.38969343e+09\n",
      " 3.76303177e+09 4.52864119e+09 3.36777562e+09 8.60534634e+09\n",
      " 7.59329409e+09 6.87777113e+09 4.20051156e+09 4.85346296e+09\n",
      " 8.55606275e+09 7.76347208e+09 6.51216561e+09 3.79692273e+09\n",
      " 3.70914517e+09 1.95289907e+09 2.67976606e+09 3.20450080e+09\n",
      " 6.17529672e+09 3.04119168e+09 5.80434251e+09 5.88030682e+09\n",
      " 4.64987483e+09 3.93667736e+09 3.78135560e+09 3.05429457e+09\n",
      " 2.94884441e+09 2.49419610e+09 3.70049622e+09 3.78376898e+09\n",
      " 5.05602503e+09 2.28922258e+09 2.70534945e+09 3.51878652e+09\n",
      " 3.23506002e+09 3.42957049e+09 2.98904456e+09 3.63874404e+09\n",
      " 7.74399254e+09 5.43179610e+09 3.63928728e+09 3.94153050e+09\n",
      " 4.16060064e+09 5.46476201e+09 5.61083581e+09 3.33994341e+09\n",
      " 1.00644499e+10 6.37130306e+09 2.40252825e+09 4.79561984e+09\n",
      " 6.52731952e+09 6.12952748e+09 1.20477969e+10 3.64433991e+09\n",
      " 2.72412981e+09 3.31737187e+09 3.86318635e+09 4.82890984e+09\n",
      " 5.14522805e+09 4.14322933e+09 3.45143683e+09 5.20850461e+09\n",
      " 5.33170562e+09 8.05951975e+09 5.61479961e+09 1.34205373e+10\n",
      " 9.20152147e+09 3.94021051e+09 4.05068998e+09 5.49938772e+09\n",
      " 3.76525616e+09 6.38372694e+09 5.38629476e+09 7.38863400e+09\n",
      " 6.14603143e+09 6.81180033e+09 8.14466013e+09 6.88083707e+09\n",
      " 7.23855484e+09 6.49444484e+09 5.19743129e+09 5.13771727e+09\n",
      " 6.75496889e+09 4.58329277e+09 4.81221272e+09 4.00024481e+09\n",
      " 7.62128808e+09 8.38782546e+09 7.09330214e+09 9.17393768e+09\n",
      " 1.12664627e+10 7.65550749e+09 7.37089678e+09 4.32714675e+09\n",
      " 4.32892684e+09 4.98842592e+09 3.84415260e+09 4.78272803e+09\n",
      " 4.27532068e+09 5.52083369e+09 3.46974393e+09 1.97278181e+10\n",
      " 8.87607394e+09 4.97341270e+09 5.04464728e+09 7.69617399e+09\n",
      " 5.94274547e+09 5.06120365e+09 8.97317317e+09 6.50289001e+09]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  mfi_series = df_feat.ta.mfi(high=df_feat[h], low=df_feat[l], close=df_feat[c], volume=df_feat[v], append=False)\n",
      "/tmp/ipykernel_38379/4212630612.py:312: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  df_new.ta.stc(close=df_new[price_col], tclength=23, fast=50, slow=100, factor=0.5, append=True, col_names=('STC_23_50_05', 'STCD_23_50_05', 'STCK_23_50_05'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Cleaning (Inf/NaN Handling & Imputation): GOOGL ---\n",
      "NaNs before imputation: 1879\n",
      "NaNs after imputation: 0\n",
      "\n",
      "--- ðŸ“Š Regime Detection (Simplified): GOOGL ---\n",
      "Simple Regimes (0:Med,1:Low,2:High):\n",
      "regime_simple\n",
      "0    34.666667\n",
      "1    32.666667\n",
      "2    32.666667\n",
      "Name: proportion, dtype: float64 %\n",
      "\n",
      "--- ðŸŽ¯ Target Definition: GOOGL ---\n",
      "Target distribution:\n",
      "target\n",
      "0    55.333333\n",
      "1    44.666667\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- Causal Feature Discovery and Effect Estimation for GOOGL (Subset Analysis) ---\n",
      "Iteratively estimating causal effects for 11 selected features: ['RSI_14', 'MACDh_12_26_9', 'ADX_14', 'close_cwt_mean', 'close_cwt_std', 'close_entropy_sample', 'regime_simple', 'volatility_20', 'log_returns', 'close_trans_seq_volatility', 'close_trans_seq_autocorr1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "[I 2025-06-03 16:36:01,791] A new study created in memory with name: no-name-fcc34a5c-7051-4142-af33-91f37187bd8e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal features ranked by absolute effect strength (top 5 of 11):\n",
      "  close_trans_seq_volatility: 0.2783\n",
      "  close_trans_seq_autocorr1: 0.2377\n",
      "  RSI_14: 0.1078\n",
      "  volatility_20: 0.0659\n",
      "  MACDh_12_26_9: 0.0634\n",
      "\n",
      "--- ML Preparation & Feature Selection: GOOGL ---\n",
      "Train shapes: X_train=(638, 100), y_train=(638,); Test shapes: X_test=(112, 100), y_test=(112,)\n",
      "Selected 35 features: ['close_trans_seq_volatility', 'close_trans_seq_autocorr1', 'RSI_14', 'volatility_20', 'MACDh_12_26_9', 'close_cwt_std', 'close_cwt_mean', 'ADX_14', 'close_entropy_sample', 'log_returns']...\n",
      "Class distribution before SMOTE: \n",
      "target\n",
      "0    0.543887\n",
      "1    0.456113\n",
      "Name: proportion, dtype: float64\n",
      "Class distribution after SMOTE: \n",
      "target\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n",
      "Shape of X_train after SMOTE: (694, 35)\n",
      "\n",
      "--- Hyperparameter Optimization (Optuna for LightGBM): GOOGL ---\n",
      "Optimizing LightGBM HPs with Optuna (50 trials)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae8f887451f49eaa07eba801a92a47b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.8190686123306552, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8190686123306552\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5986152731636356, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5986152731636356\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.050811038618215426, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.050811038618215426\n",
      "[I 2025-06-03 16:36:02,175] Trial 0 finished with value: 0.5240301272528531 and parameters: {'n_estimators': 100, 'learning_rate': 0.014323961151037607, 'num_leaves': 45, 'max_depth': 0, 'min_child_samples': 9, 'feature_fraction': 0.5986152731636356, 'bagging_fraction': 0.8190686123306552, 'bagging_freq': 7, 'reg_alpha': 0.10854349078811594, 'reg_lambda': 0.032934774375577236, 'min_gain_to_split': 0.050811038618215426}. Best is trial 0 with value: 0.5240301272528531.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8861022617514336, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8861022617514336\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8381192594249507, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8381192594249507\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.018588434556399393, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.018588434556399393\n",
      "[I 2025-06-03 16:36:02,357] Trial 1 finished with value: 0.5831725429650833 and parameters: {'n_estimators': 350, 'learning_rate': 0.02070684280786433, 'num_leaves': 34, 'max_depth': 10, 'min_child_samples': 37, 'feature_fraction': 0.8381192594249507, 'bagging_fraction': 0.8861022617514336, 'bagging_freq': 4, 'reg_alpha': 0.14801071727502124, 'reg_lambda': 0.019915331065390486, 'min_gain_to_split': 0.018588434556399393}. Best is trial 0 with value: 0.5240301272528531.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7282109850917364, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7282109850917364\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8543632407728478, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8543632407728478\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.030900068270952943, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.030900068270952943\n",
      "[I 2025-06-03 16:36:02,430] Trial 2 finished with value: 0.5561797534711506 and parameters: {'n_estimators': 300, 'learning_rate': 0.08690105937834214, 'num_leaves': 40, 'max_depth': 2, 'min_child_samples': 11, 'feature_fraction': 0.8543632407728478, 'bagging_fraction': 0.7282109850917364, 'bagging_freq': 6, 'reg_alpha': 0.22636837955963246, 'reg_lambda': 0.2923668175233883, 'min_gain_to_split': 0.030900068270952943}. Best is trial 0 with value: 0.5240301272528531.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6761132767882727, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6761132767882727\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7842922588478147, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7842922588478147\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.05575561606787843, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.05575561606787843\n",
      "[I 2025-06-03 16:36:02,530] Trial 3 finished with value: 0.5607103195807824 and parameters: {'n_estimators': 650, 'learning_rate': 0.056154603804515806, 'num_leaves': 16, 'max_depth': 0, 'min_child_samples': 31, 'feature_fraction': 0.7842922588478147, 'bagging_fraction': 0.6761132767882727, 'bagging_freq': 3, 'reg_alpha': 0.0010206226111419572, 'reg_lambda': 0.002601718213665049, 'min_gain_to_split': 0.05575561606787843}. Best is trial 0 with value: 0.5240301272528531.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6043080435404738, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6043080435404738\n",
      "[LightGBM] [Warning] feature_fraction is set=0.728832541345335, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.728832541345335\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.03196439938871647, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.03196439938871647\n",
      "[I 2025-06-03 16:36:02,644] Trial 4 finished with value: 0.5900743606833629 and parameters: {'n_estimators': 150, 'learning_rate': 0.02661174824130562, 'num_leaves': 43, 'max_depth': 0, 'min_child_samples': 49, 'feature_fraction': 0.728832541345335, 'bagging_fraction': 0.6043080435404738, 'bagging_freq': 5, 'reg_alpha': 0.895459910123315, 'reg_lambda': 0.11627001736973708, 'min_gain_to_split': 0.03196439938871647}. Best is trial 0 with value: 0.5240301272528531.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9587413740717503, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9587413740717503\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9527345517250083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9527345517250083\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.052523982225470923, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.052523982225470923\n",
      "[I 2025-06-03 16:36:02,695] Trial 5 finished with value: 0.6465022999116791 and parameters: {'n_estimators': 800, 'learning_rate': 0.06850786485258652, 'num_leaves': 26, 'max_depth': 1, 'min_child_samples': 34, 'feature_fraction': 0.9527345517250083, 'bagging_fraction': 0.9587413740717503, 'bagging_freq': 5, 'reg_alpha': 0.2115678363684965, 'reg_lambda': 0.274660463528775, 'min_gain_to_split': 0.052523982225470923}. Best is trial 0 with value: 0.5240301272528531.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9672878026207858, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9672878026207858\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5098079899464175, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5098079899464175\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04691238735300923, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04691238735300923\n",
      "[I 2025-06-03 16:36:03,467] Trial 6 finished with value: 0.5033733725230142 and parameters: {'n_estimators': 950, 'learning_rate': 0.013113884480147, 'num_leaves': 26, 'max_depth': -1, 'min_child_samples': 13, 'feature_fraction': 0.5098079899464175, 'bagging_fraction': 0.9672878026207858, 'bagging_freq': 3, 'reg_alpha': 0.019036907431344455, 'reg_lambda': 0.0033039761657419203, 'min_gain_to_split': 0.04691238735300923}. Best is trial 6 with value: 0.5033733725230142.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5105914082779045, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5105914082779045\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7485490243608678, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7485490243608678\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.060193152373388785, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.060193152373388785\n",
      "[I 2025-06-03 16:36:03,613] Trial 7 finished with value: 0.5999788680020733 and parameters: {'n_estimators': 950, 'learning_rate': 0.01715759832645029, 'num_leaves': 28, 'max_depth': 2, 'min_child_samples': 47, 'feature_fraction': 0.7485490243608678, 'bagging_fraction': 0.5105914082779045, 'bagging_freq': 4, 'reg_alpha': 0.032943672061034865, 'reg_lambda': 0.0026531832563556766, 'min_gain_to_split': 0.060193152373388785}. Best is trial 6 with value: 0.5033733725230142.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9187539222412922, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9187539222412922\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5954526777575972, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5954526777575972\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.023033246283310896, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.023033246283310896\n",
      "[I 2025-06-03 16:36:03,740] Trial 8 finished with value: 0.5213670757801531 and parameters: {'n_estimators': 100, 'learning_rate': 0.04029323706842093, 'num_leaves': 48, 'max_depth': 4, 'min_child_samples': 21, 'feature_fraction': 0.5954526777575972, 'bagging_fraction': 0.9187539222412922, 'bagging_freq': 4, 'reg_alpha': 0.0029852096763580294, 'reg_lambda': 0.017279467260680515, 'min_gain_to_split': 0.023033246283310896}. Best is trial 6 with value: 0.5033733725230142.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.588092244740648, subsample=1.0 will be ignored. Current value: bagging_fraction=0.588092244740648\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5024634459721566, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5024634459721566\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.09398409166691261, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.09398409166691261\n",
      "[I 2025-06-03 16:36:03,847] Trial 9 finished with value: 0.5519800267090407 and parameters: {'n_estimators': 600, 'learning_rate': 0.04865431543126699, 'num_leaves': 14, 'max_depth': 5, 'min_child_samples': 37, 'feature_fraction': 0.5024634459721566, 'bagging_fraction': 0.588092244740648, 'bagging_freq': 6, 'reg_alpha': 0.1314601950577407, 'reg_lambda': 0.018181335037232203, 'min_gain_to_split': 0.09398409166691261}. Best is trial 6 with value: 0.5033733725230142.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9866187264840641, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9866187264840641\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6340061925100171, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6340061925100171\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.002939786013754088, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.002939786013754088\n",
      "[I 2025-06-03 16:36:04,819] Trial 10 finished with value: 0.49999976229464765 and parameters: {'n_estimators': 1000, 'learning_rate': 0.010217720825835783, 'num_leaves': 22, 'max_depth': 8, 'min_child_samples': 20, 'feature_fraction': 0.6340061925100171, 'bagging_fraction': 0.9866187264840641, 'bagging_freq': 0, 'reg_alpha': 0.007072683225716879, 'reg_lambda': 0.0011266831417255743, 'min_gain_to_split': 0.002939786013754088}. Best is trial 10 with value: 0.49999976229464765.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9632077743400165, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9632077743400165\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5083328149466422, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5083328149466422\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.00047986742839443985, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.00047986742839443985\n",
      "[I 2025-06-03 16:36:05,757] Trial 11 finished with value: 0.5119146985481755 and parameters: {'n_estimators': 1000, 'learning_rate': 0.010584418766171465, 'num_leaves': 21, 'max_depth': 9, 'min_child_samples': 20, 'feature_fraction': 0.5083328149466422, 'bagging_fraction': 0.9632077743400165, 'bagging_freq': 0, 'reg_alpha': 0.00857214730660489, 'reg_lambda': 0.0011975825120997528, 'min_gain_to_split': 0.00047986742839443985}. Best is trial 10 with value: 0.49999976229464765.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8318329225882564, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8318329225882564\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6243264780892734, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6243264780892734\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.08025402168795402, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.08025402168795402\n",
      "[I 2025-06-03 16:36:06,501] Trial 12 finished with value: 0.5179304932762098 and parameters: {'n_estimators': 800, 'learning_rate': 0.011026118056290644, 'num_leaves': 22, 'max_depth': 7, 'min_child_samples': 18, 'feature_fraction': 0.6243264780892734, 'bagging_fraction': 0.8318329225882564, 'bagging_freq': 0, 'reg_alpha': 0.013207494788538145, 'reg_lambda': 0.005152602538703502, 'min_gain_to_split': 0.08025402168795402}. Best is trial 10 with value: 0.49999976229464765.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9927633457341344, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9927633457341344\n",
      "[LightGBM] [Warning] feature_fraction is set=0.680062928688823, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.680062928688823\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0009789540234115082, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0009789540234115082\n",
      "[I 2025-06-03 16:36:07,303] Trial 13 finished with value: 0.5396381222826623 and parameters: {'n_estimators': 850, 'learning_rate': 0.013910975704021753, 'num_leaves': 34, 'max_depth': 7, 'min_child_samples': 6, 'feature_fraction': 0.680062928688823, 'bagging_fraction': 0.9927633457341344, 'bagging_freq': 2, 'reg_alpha': 0.020495741366328795, 'reg_lambda': 0.0011682698144400345, 'min_gain_to_split': 0.0009789540234115082}. Best is trial 10 with value: 0.49999976229464765.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8577199916014533, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8577199916014533\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5635779659050404, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5635779659050404\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.07005438092527028, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.07005438092527028\n",
      "[I 2025-06-03 16:36:07,655] Trial 14 finished with value: 0.5165116333619304 and parameters: {'n_estimators': 1000, 'learning_rate': 0.02711473451751294, 'num_leaves': 20, 'max_depth': 5, 'min_child_samples': 15, 'feature_fraction': 0.5635779659050404, 'bagging_fraction': 0.8577199916014533, 'bagging_freq': 2, 'reg_alpha': 0.0033284910523512165, 'reg_lambda': 0.005719051106754041, 'min_gain_to_split': 0.07005438092527028}. Best is trial 10 with value: 0.49999976229464765.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7730059854640505, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7730059854640505\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6643654120317488, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6643654120317488\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.03970560269162995, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.03970560269162995\n",
      "[I 2025-06-03 16:36:07,934] Trial 15 finished with value: 0.5792183803866389 and parameters: {'n_estimators': 700, 'learning_rate': 0.02008776844758648, 'num_leaves': 11, 'max_depth': 8, 'min_child_samples': 25, 'feature_fraction': 0.6643654120317488, 'bagging_fraction': 0.7730059854640505, 'bagging_freq': 1, 'reg_alpha': 0.04550794560237922, 'reg_lambda': 0.006474446575626309, 'min_gain_to_split': 0.03970560269162995}. Best is trial 10 with value: 0.49999976229464765.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9127915807002883, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9127915807002883\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5339730698385651, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5339730698385651\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.009307267215525057, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.009307267215525057\n",
      "[I 2025-06-03 16:36:08,405] Trial 16 finished with value: 0.5634591346100573 and parameters: {'n_estimators': 500, 'learning_rate': 0.010318909121635383, 'num_leaves': 33, 'max_depth': 4, 'min_child_samples': 25, 'feature_fraction': 0.5339730698385651, 'bagging_fraction': 0.9127915807002883, 'bagging_freq': 2, 'reg_alpha': 0.006652530474948486, 'reg_lambda': 0.0762426076549957, 'min_gain_to_split': 0.009307267215525057}. Best is trial 10 with value: 0.49999976229464765.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9868831988394098, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9868831988394098\n",
      "[LightGBM] [Warning] feature_fraction is set=0.681296181955922, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.681296181955922\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.09998281021710918, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.09998281021710918\n",
      "[I 2025-06-03 16:36:09,105] Trial 17 finished with value: 0.542391172925792 and parameters: {'n_estimators': 900, 'learning_rate': 0.014204038013556472, 'num_leaves': 25, 'max_depth': 6, 'min_child_samples': 15, 'feature_fraction': 0.681296181955922, 'bagging_fraction': 0.9868831988394098, 'bagging_freq': 1, 'reg_alpha': 0.003926171344580518, 'reg_lambda': 0.9294765307622235, 'min_gain_to_split': 0.09998281021710918}. Best is trial 10 with value: 0.49999976229464765.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.760102513973228, subsample=1.0 will be ignored. Current value: bagging_fraction=0.760102513973228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5650850324695763, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5650850324695763\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.03988899141830561, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.03988899141830561\n",
      "[I 2025-06-03 16:36:09,442] Trial 18 finished with value: 0.4988878060883922 and parameters: {'n_estimators': 500, 'learning_rate': 0.031630665177139104, 'num_leaves': 17, 'max_depth': -1, 'min_child_samples': 13, 'feature_fraction': 0.5650850324695763, 'bagging_fraction': 0.760102513973228, 'bagging_freq': 3, 'reg_alpha': 0.0011016614280217661, 'reg_lambda': 0.0026888224156451303, 'min_gain_to_split': 0.03988899141830561}. Best is trial 18 with value: 0.4988878060883922.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7306593607709413, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7306593607709413\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6519007347723962, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6519007347723962\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.011663853683306462, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.011663853683306462\n",
      "[I 2025-06-03 16:36:09,601] Trial 19 finished with value: 0.552223661350358 and parameters: {'n_estimators': 450, 'learning_rate': 0.029490799904759828, 'num_leaves': 16, 'max_depth': 3, 'min_child_samples': 6, 'feature_fraction': 0.6519007347723962, 'bagging_fraction': 0.7306593607709413, 'bagging_freq': 1, 'reg_alpha': 0.0010463189384260225, 'reg_lambda': 0.0011151539508354824, 'min_gain_to_split': 0.011663853683306462}. Best is trial 18 with value: 0.4988878060883922.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6627895613247468, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6627895613247468\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5595361471371114, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5595361471371114\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.06734741702356456, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.06734741702356456\n",
      "[I 2025-06-03 16:36:09,736] Trial 20 finished with value: 0.5997137098979886 and parameters: {'n_estimators': 400, 'learning_rate': 0.03514638645685011, 'num_leaves': 10, 'max_depth': 10, 'min_child_samples': 28, 'feature_fraction': 0.5595361471371114, 'bagging_fraction': 0.6627895613247468, 'bagging_freq': 0, 'reg_alpha': 0.0017473084741604292, 'reg_lambda': 0.009861396745104452, 'min_gain_to_split': 0.06734741702356456}. Best is trial 18 with value: 0.4988878060883922.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7936811936217107, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7936811936217107\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5706516414948379, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5706516414948379\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.03895953984716969, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.03895953984716969\n",
      "[I 2025-06-03 16:36:10,167] Trial 21 finished with value: 0.4985895329539449 and parameters: {'n_estimators': 750, 'learning_rate': 0.021836746943640486, 'num_leaves': 19, 'max_depth': -1, 'min_child_samples': 13, 'feature_fraction': 0.5706516414948379, 'bagging_fraction': 0.7936811936217107, 'bagging_freq': 3, 'reg_alpha': 0.008279547889468967, 'reg_lambda': 0.002856921244279496, 'min_gain_to_split': 0.03895953984716969}. Best is trial 21 with value: 0.4985895329539449.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7864117930647259, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7864117930647259\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5986577139826512, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5986577139826512\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.03926035100219944, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.03926035100219944\n",
      "[I 2025-06-03 16:36:10,523] Trial 22 finished with value: 0.5176209895195817 and parameters: {'n_estimators': 700, 'learning_rate': 0.022995899599540286, 'num_leaves': 19, 'max_depth': -1, 'min_child_samples': 18, 'feature_fraction': 0.5986577139826512, 'bagging_fraction': 0.7864117930647259, 'bagging_freq': 3, 'reg_alpha': 0.0058618735027943175, 'reg_lambda': 0.0019182647332218236, 'min_gain_to_split': 0.03926035100219944}. Best is trial 21 with value: 0.4985895329539449.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6813687133673195, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6813687133673195\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5684865260743053, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5684865260743053\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.028551573380460053, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.028551573380460053\n",
      "[I 2025-06-03 16:36:10,615] Trial 23 finished with value: 0.6457802272330765 and parameters: {'n_estimators': 550, 'learning_rate': 0.037494812826835505, 'num_leaves': 16, 'max_depth': 1, 'min_child_samples': 9, 'feature_fraction': 0.5684865260743053, 'bagging_fraction': 0.6813687133673195, 'bagging_freq': 5, 'reg_alpha': 0.0020884403468176774, 'reg_lambda': 0.0036896314062854554, 'min_gain_to_split': 0.028551573380460053}. Best is trial 21 with value: 0.4985895329539449.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8048511740821116, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8048511740821116\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7099523435072097, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7099523435072097\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.043693146574947175, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.043693146574947175\n",
      "[I 2025-06-03 16:36:10,849] Trial 24 finished with value: 0.5521687763730059 and parameters: {'n_estimators': 750, 'learning_rate': 0.046169580045366, 'num_leaves': 24, 'max_depth': -1, 'min_child_samples': 24, 'feature_fraction': 0.7099523435072097, 'bagging_fraction': 0.8048511740821116, 'bagging_freq': 2, 'reg_alpha': 0.010891868094986634, 'reg_lambda': 0.0018115219625490183, 'min_gain_to_split': 0.043693146574947175}. Best is trial 21 with value: 0.4985895329539449.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7410032927376835, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7410032927376835\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6334452931407254, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6334452931407254\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.01892486913359507, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.01892486913359507\n",
      "[I 2025-06-03 16:36:11,134] Trial 25 finished with value: 0.5415095739981104 and parameters: {'n_estimators': 600, 'learning_rate': 0.017167055243907593, 'num_leaves': 19, 'max_depth': 3, 'min_child_samples': 15, 'feature_fraction': 0.6334452931407254, 'bagging_fraction': 0.7410032927376835, 'bagging_freq': 3, 'reg_alpha': 0.04652373673304503, 'reg_lambda': 0.010723166412045744, 'min_gain_to_split': 0.01892486913359507}. Best is trial 21 with value: 0.4985895329539449.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8588377766706268, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8588377766706268\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5502355154557159, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5502355154557159\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.07955228919486286, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.07955228919486286\n",
      "[I 2025-06-03 16:36:11,615] Trial 26 finished with value: 0.5123937189084549 and parameters: {'n_estimators': 300, 'learning_rate': 0.02363601161326325, 'num_leaves': 15, 'max_depth': 8, 'min_child_samples': 21, 'feature_fraction': 0.5502355154557159, 'bagging_fraction': 0.8588377766706268, 'bagging_freq': 1, 'reg_alpha': 0.0018196346862994822, 'reg_lambda': 0.0018615019249890162, 'min_gain_to_split': 0.07955228919486286}. Best is trial 21 with value: 0.4985895329539449.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6125029454774823, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6125029454774823\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6196791005485185, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6196791005485185\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.03547998511513003, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.03547998511513003\n",
      "[I 2025-06-03 16:36:11,691] Trial 27 finished with value: 0.6553380607638069 and parameters: {'n_estimators': 850, 'learning_rate': 0.03289392849843017, 'num_leaves': 30, 'max_depth': 1, 'min_child_samples': 11, 'feature_fraction': 0.6196791005485185, 'bagging_fraction': 0.6125029454774823, 'bagging_freq': 4, 'reg_alpha': 0.0049363201026868895, 'reg_lambda': 0.008705919220596577, 'min_gain_to_split': 0.03547998511513003}. Best is trial 21 with value: 0.4985895329539449.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7065252865750431, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7065252865750431\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9453674414554892, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9453674414554892\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.011292846369827919, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.011292846369827919\n",
      "[I 2025-06-03 16:36:11,975] Trial 28 finished with value: 0.5629991723812785 and parameters: {'n_estimators': 450, 'learning_rate': 0.017610379960254798, 'num_leaves': 13, 'max_depth': 6, 'min_child_samples': 18, 'feature_fraction': 0.9453674414554892, 'bagging_fraction': 0.7065252865750431, 'bagging_freq': 2, 'reg_alpha': 0.016606304673707304, 'reg_lambda': 0.0010008723716259473, 'min_gain_to_split': 0.011292846369827919}. Best is trial 21 with value: 0.4985895329539449.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.837061790799276, subsample=1.0 will be ignored. Current value: bagging_fraction=0.837061790799276\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5902014429408798, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5902014429408798\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.025422390058377195, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.025422390058377195\n",
      "[I 2025-06-03 16:36:12,127] Trial 29 finished with value: 0.5999472598043792 and parameters: {'n_estimators': 200, 'learning_rate': 0.012312154882695965, 'num_leaves': 23, 'max_depth': 2, 'min_child_samples': 8, 'feature_fraction': 0.5902014429408798, 'bagging_fraction': 0.837061790799276, 'bagging_freq': 7, 'reg_alpha': 0.0720617115871951, 'reg_lambda': 0.029517964296714806, 'min_gain_to_split': 0.025422390058377195}. Best is trial 21 with value: 0.4985895329539449.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7651048078391648, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7651048078391648\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6932045625922924, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6932045625922924\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.05866361526028746, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.05866361526028746\n",
      "[I 2025-06-03 16:36:12,293] Trial 30 finished with value: 0.5510427492243469 and parameters: {'n_estimators': 550, 'learning_rate': 0.09745786061276228, 'num_leaves': 18, 'max_depth': 0, 'min_child_samples': 13, 'feature_fraction': 0.6932045625922924, 'bagging_fraction': 0.7651048078391648, 'bagging_freq': 6, 'reg_alpha': 0.007908968695887185, 'reg_lambda': 0.004280002030825474, 'min_gain_to_split': 0.05866361526028746}. Best is trial 21 with value: 0.4985895329539449.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9284953365154794, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9284953365154794\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5216332545083959, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5216332545083959\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0456999082409355, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0456999082409355\n",
      "[I 2025-06-03 16:36:13,046] Trial 31 finished with value: 0.4906637670384354 and parameters: {'n_estimators': 900, 'learning_rate': 0.012670479042552557, 'num_leaves': 28, 'max_depth': -1, 'min_child_samples': 13, 'feature_fraction': 0.5216332545083959, 'bagging_fraction': 0.9284953365154794, 'bagging_freq': 3, 'reg_alpha': 0.021152537074924642, 'reg_lambda': 0.003012482659250898, 'min_gain_to_split': 0.0456999082409355}. Best is trial 31 with value: 0.4906637670384354.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9143257452714398, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9143257452714398\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5341521799216946, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5341521799216946\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04740678035350599, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04740678035350599\n",
      "[I 2025-06-03 16:36:14,068] Trial 32 finished with value: 0.4963560296170161 and parameters: {'n_estimators': 900, 'learning_rate': 0.01181584442172949, 'num_leaves': 30, 'max_depth': -1, 'min_child_samples': 9, 'feature_fraction': 0.5341521799216946, 'bagging_fraction': 0.9143257452714398, 'bagging_freq': 3, 'reg_alpha': 0.02670603238322731, 'reg_lambda': 0.0018742878523266442, 'min_gain_to_split': 0.04740678035350599}. Best is trial 31 with value: 0.4906637670384354.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8937379427595752, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8937379427595752\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5460259576599464, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5460259576599464\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.048621494348925785, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.048621494348925785\n",
      "[I 2025-06-03 16:36:14,713] Trial 33 finished with value: 0.5029271665377639 and parameters: {'n_estimators': 900, 'learning_rate': 0.016117258977619512, 'num_leaves': 38, 'max_depth': -1, 'min_child_samples': 10, 'feature_fraction': 0.5460259576599464, 'bagging_fraction': 0.8937379427595752, 'bagging_freq': 3, 'reg_alpha': 0.028663101959349672, 'reg_lambda': 0.00213085963397897, 'min_gain_to_split': 0.048621494348925785}. Best is trial 31 with value: 0.4906637670384354.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9328277474247496, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9328277474247496\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7982406000573512, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7982406000573512\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.042994733772310056, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.042994733772310056\n",
      "[I 2025-06-03 16:36:15,227] Trial 34 finished with value: 0.5218682206795026 and parameters: {'n_estimators': 750, 'learning_rate': 0.020336730225601174, 'num_leaves': 31, 'max_depth': 0, 'min_child_samples': 5, 'feature_fraction': 0.7982406000573512, 'bagging_fraction': 0.9328277474247496, 'bagging_freq': 3, 'reg_alpha': 0.08421576946132597, 'reg_lambda': 0.007124554946519091, 'min_gain_to_split': 0.042994733772310056}. Best is trial 31 with value: 0.4906637670384354.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8710438319648582, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8710438319648582\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9079475435187665, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9079475435187665\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.034609164029993554, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.034609164029993554\n",
      "[I 2025-06-03 16:36:15,960] Trial 35 finished with value: 0.5140272916294305 and parameters: {'n_estimators': 850, 'learning_rate': 0.015258170905988676, 'num_leaves': 37, 'max_depth': -1, 'min_child_samples': 8, 'feature_fraction': 0.9079475435187665, 'bagging_fraction': 0.8710438319648582, 'bagging_freq': 4, 'reg_alpha': 0.546570111968397, 'reg_lambda': 0.003072041230283505, 'min_gain_to_split': 0.034609164029993554}. Best is trial 31 with value: 0.4906637670384354.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8051707825162523, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8051707825162523\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5319825917171215, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5319825917171215\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.05205380436869702, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.05205380436869702\n",
      "[I 2025-06-03 16:36:16,613] Trial 36 finished with value: 0.513567649385936 and parameters: {'n_estimators': 650, 'learning_rate': 0.011864151966778182, 'num_leaves': 27, 'max_depth': 0, 'min_child_samples': 13, 'feature_fraction': 0.5319825917171215, 'bagging_fraction': 0.8051707825162523, 'bagging_freq': 4, 'reg_alpha': 0.03191466104414953, 'reg_lambda': 0.014228827862474265, 'min_gain_to_split': 0.05205380436869702}. Best is trial 31 with value: 0.4906637670384354.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8910162633907049, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8910162633907049\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5874639603753224, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5874639603753224\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.06495750193668498, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.06495750193668498\n",
      "[I 2025-06-03 16:36:16,764] Trial 37 finished with value: 0.6107432207398195 and parameters: {'n_estimators': 900, 'learning_rate': 0.024437249647180335, 'num_leaves': 29, 'max_depth': 1, 'min_child_samples': 16, 'feature_fraction': 0.5874639603753224, 'bagging_fraction': 0.8910162633907049, 'bagging_freq': 3, 'reg_alpha': 0.052697639506671966, 'reg_lambda': 0.0043242146628812575, 'min_gain_to_split': 0.06495750193668498}. Best is trial 31 with value: 0.4906637670384354.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9332827773133121, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9332827773133121\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5301843184721728, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5301843184721728\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04699934735314269, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04699934735314269\n",
      "[I 2025-06-03 16:36:17,065] Trial 38 finished with value: 0.5516483209544637 and parameters: {'n_estimators': 800, 'learning_rate': 0.01879751476281642, 'num_leaves': 41, 'max_depth': 0, 'min_child_samples': 43, 'feature_fraction': 0.5301843184721728, 'bagging_fraction': 0.9332827773133121, 'bagging_freq': 5, 'reg_alpha': 0.02591131686779253, 'reg_lambda': 0.03131750114834335, 'min_gain_to_split': 0.04699934735314269}. Best is trial 31 with value: 0.4906637670384354.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7052581614264768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7052581614264768\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5783144531409473, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5783144531409473\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.05447626798141579, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.05447626798141579\n",
      "[I 2025-06-03 16:36:17,392] Trial 39 finished with value: 0.4467815251922387 and parameters: {'n_estimators': 950, 'learning_rate': 0.06683360059071286, 'num_leaves': 32, 'max_depth': 0, 'min_child_samples': 12, 'feature_fraction': 0.5783144531409473, 'bagging_fraction': 0.7052581614264768, 'bagging_freq': 2, 'reg_alpha': 0.01261910742202023, 'reg_lambda': 0.05660901399419315, 'min_gain_to_split': 0.05447626798141579}. Best is trial 39 with value: 0.4467815251922387.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6519028197404313, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6519028197404313\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8327403467929597, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8327403467929597\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.05550069428183284, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.05550069428183284\n",
      "[I 2025-06-03 16:36:17,536] Trial 40 finished with value: 0.5657798769025913 and parameters: {'n_estimators': 950, 'learning_rate': 0.07221794388715759, 'num_leaves': 32, 'max_depth': 2, 'min_child_samples': 11, 'feature_fraction': 0.8327403467929597, 'bagging_fraction': 0.6519028197404313, 'bagging_freq': 2, 'reg_alpha': 0.012712644702166657, 'reg_lambda': 0.05101252000926077, 'min_gain_to_split': 0.05550069428183284}. Best is trial 39 with value: 0.4467815251922387.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7078639673777454, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7078639673777454\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5000966171992426, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5000966171992426\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.06189646863406349, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.06189646863406349\n",
      "[I 2025-06-03 16:36:17,808] Trial 41 finished with value: 0.49386415708524894 and parameters: {'n_estimators': 950, 'learning_rate': 0.06479663186328746, 'num_leaves': 37, 'max_depth': -1, 'min_child_samples': 13, 'feature_fraction': 0.5000966171992426, 'bagging_fraction': 0.7078639673777454, 'bagging_freq': 3, 'reg_alpha': 0.017646125867115513, 'reg_lambda': 0.12505161395956552, 'min_gain_to_split': 0.06189646863406349}. Best is trial 39 with value: 0.4467815251922387.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7073997209078652, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7073997209078652\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5181214748956471, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5181214748956471\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.07314451757295226, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.07314451757295226\n",
      "[I 2025-06-03 16:36:18,142] Trial 42 finished with value: 0.4532899883196018 and parameters: {'n_estimators': 950, 'learning_rate': 0.06385235389994964, 'num_leaves': 36, 'max_depth': 0, 'min_child_samples': 8, 'feature_fraction': 0.5181214748956471, 'bagging_fraction': 0.7073997209078652, 'bagging_freq': 3, 'reg_alpha': 0.01819905984708562, 'reg_lambda': 0.23686383242865897, 'min_gain_to_split': 0.07314451757295226}. Best is trial 39 with value: 0.4467815251922387.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7139488808793714, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7139488808793714\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5027135909954369, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5027135909954369\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.07455515061986037, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.07455515061986037\n",
      "[I 2025-06-03 16:36:18,517] Trial 43 finished with value: 0.4887578280417922 and parameters: {'n_estimators': 950, 'learning_rate': 0.06690743613862711, 'num_leaves': 36, 'max_depth': 0, 'min_child_samples': 7, 'feature_fraction': 0.5027135909954369, 'bagging_fraction': 0.7139488808793714, 'bagging_freq': 2, 'reg_alpha': 0.0213993802089477, 'reg_lambda': 0.19325160693823523, 'min_gain_to_split': 0.07455515061986037}. Best is trial 39 with value: 0.4467815251922387.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7015572486330578, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7015572486330578\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5023857323928267, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5023857323928267\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.08080165884077029, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.08080165884077029\n",
      "[I 2025-06-03 16:36:18,707] Trial 44 finished with value: 0.5525172809086639 and parameters: {'n_estimators': 950, 'learning_rate': 0.06369953211231373, 'num_leaves': 36, 'max_depth': 1, 'min_child_samples': 7, 'feature_fraction': 0.5023857323928267, 'bagging_fraction': 0.7015572486330578, 'bagging_freq': 2, 'reg_alpha': 0.01739964496892723, 'reg_lambda': 0.18665294956571585, 'min_gain_to_split': 0.08080165884077029}. Best is trial 39 with value: 0.4467815251922387.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6319728754709272, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6319728754709272\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5204287105111469, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5204287105111469\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.07424669556341687, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.07424669556341687\n",
      "[I 2025-06-03 16:36:18,808] Trial 45 finished with value: 0.5648818417494463 and parameters: {'n_estimators': 1000, 'learning_rate': 0.07751345121537138, 'num_leaves': 45, 'max_depth': 2, 'min_child_samples': 5, 'feature_fraction': 0.5204287105111469, 'bagging_fraction': 0.6319728754709272, 'bagging_freq': 2, 'reg_alpha': 0.012651013383773627, 'reg_lambda': 0.4309930060888708, 'min_gain_to_split': 0.07424669556341687}. Best is trial 39 with value: 0.4467815251922387.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5661686807956066, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5661686807956066\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5060273368586884, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5060273368586884\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.08514781681492466, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.08514781681492466\n",
      "[I 2025-06-03 16:36:18,974] Trial 46 finished with value: 0.5558015234058631 and parameters: {'n_estimators': 950, 'learning_rate': 0.05749084334183713, 'num_leaves': 40, 'max_depth': 1, 'min_child_samples': 11, 'feature_fraction': 0.5060273368586884, 'bagging_fraction': 0.5661686807956066, 'bagging_freq': 1, 'reg_alpha': 0.020197856679328035, 'reg_lambda': 0.12138554201525585, 'min_gain_to_split': 0.08514781681492466}. Best is trial 39 with value: 0.4467815251922387.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7064861282433703, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7064861282433703\n",
      "[LightGBM] [Warning] feature_fraction is set=0.99791943054813, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.99791943054813\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.06353593455539687, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.06353593455539687\n",
      "[I 2025-06-03 16:36:19,121] Trial 47 finished with value: 0.5427210612742506 and parameters: {'n_estimators': 1000, 'learning_rate': 0.0508396802391821, 'num_leaves': 35, 'max_depth': 0, 'min_child_samples': 32, 'feature_fraction': 0.99791943054813, 'bagging_fraction': 0.7064861282433703, 'bagging_freq': 4, 'reg_alpha': 0.041893232239784174, 'reg_lambda': 0.18090861606903214, 'min_gain_to_split': 0.06353593455539687}. Best is trial 39 with value: 0.4467815251922387.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6787356993346981, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6787356993346981\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6022407787177657, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6022407787177657\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.07301197993964544, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.07301197993964544\n",
      "[I 2025-06-03 16:36:19,234] Trial 48 finished with value: 0.5438305393002516 and parameters: {'n_estimators': 850, 'learning_rate': 0.08190845546893101, 'num_leaves': 38, 'max_depth': 0, 'min_child_samples': 38, 'feature_fraction': 0.6022407787177657, 'bagging_fraction': 0.6787356993346981, 'bagging_freq': 2, 'reg_alpha': 0.010678366476684699, 'reg_lambda': 0.4155956340878673, 'min_gain_to_split': 0.07301197993964544}. Best is trial 39 with value: 0.4467815251922387.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7174032777906099, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7174032777906099\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5205209552832021, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5205209552832021\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.06010034177100687, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.06010034177100687\n",
      "[I 2025-06-03 16:36:19,386] Trial 49 finished with value: 0.518898816678194 and parameters: {'n_estimators': 900, 'learning_rate': 0.06281948234743095, 'num_leaves': 42, 'max_depth': 3, 'min_child_samples': 17, 'feature_fraction': 0.5205209552832021, 'bagging_fraction': 0.7174032777906099, 'bagging_freq': 4, 'reg_alpha': 0.015031330686893423, 'reg_lambda': 0.0679302281412711, 'min_gain_to_split': 0.06010034177100687}. Best is trial 39 with value: 0.4467815251922387.\n",
      "Best Optuna trial for LightGBM: Value=0.4468, Params={'n_estimators': 950, 'learning_rate': 0.06683360059071286, 'num_leaves': 32, 'max_depth': 0, 'min_child_samples': 12, 'feature_fraction': 0.5783144531409473, 'bagging_fraction': 0.7052581614264768, 'bagging_freq': 2, 'reg_alpha': 0.01261910742202023, 'reg_lambda': 0.05660901399419315, 'min_gain_to_split': 0.05447626798141579}\n",
      "\n",
      "--- LightGBM Model Training: GOOGL ---\n",
      "Training LightGBM model...\n",
      "Using Optuna-optimized parameters.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7052581614264768, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7052581614264768\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5783144531409473, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5783144531409473\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.05447626798141579, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.05447626798141579\n",
      "\n",
      "Top 10 features:\n",
      "                        Feature  Importance\n",
      "2                       RSI_14           8\n",
      "5                close_cwt_std           8\n",
      "32           sma_10_regime_adj           6\n",
      "24               volatility_50           6\n",
      "1    close_trans_seq_autocorr1           5\n",
      "3                volatility_20           5\n",
      "0   close_trans_seq_volatility           4\n",
      "7                       ADX_14           4\n",
      "12                      SMA_20           4\n",
      "29            close_weekly_min           3\n",
      "\n",
      "ðŸŽ¯ Accuracy on mapped test data: 0.6161\n",
      "ðŸ“Š AUC: 0.6975\n",
      "\n",
      "Classification Report (on mapped and valid test labels):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.87      0.73        68\n",
      "           1       0.53      0.23      0.32        44\n",
      "\n",
      "    accuracy                           0.62       112\n",
      "   macro avg       0.58      0.55      0.53       112\n",
      "weighted avg       0.59      0.62      0.57       112\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The maximum opset needed by this model is only 9.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance plot saved to feature_importance_GOOGL.png\n",
      "\n",
      "Exporting LGBM model to ONNX: lgbm_model_GOOGL.onnx (opset=12)\n",
      "Model exported to ONNX: lgbm_model_GOOGL.onnx\n",
      "ONNX model check OK.\n",
      "\n",
      "--- Foundation Model Forecasting (PatchTST): GOOGL ---\n",
      "\n",
      "ðŸ Workflow completed for GOOGL. Final Status: LightGBM Model Trained\n",
      "\n",
      "--- Results Summary for GOOGL ---\n",
      "  Overall Status: LightGBM Model Trained\n",
      "  Top LGBM Features: ['RSI_14', 'close_cwt_std', 'sma_10_regime_adj']\n",
      "  ONNX Model Path: lgbm_model_GOOGL.onnx\n",
      "----------------------------------------\n",
      "\n",
      "Total execution time for 2 symbol(s): 32.81 seconds.\n",
      "DEBUG: Script __main__ block finished.\n",
      "DEBUG: Script execution started, entering __main__ block.\n",
      "DEBUG: API_KEY appears to be set. Proceeding. Key ends with '...ef0a'\n",
      "DEBUG: Symbols to process: ['AAPL', 'GOOGL']\n",
      "DEBUG: Starting main loop for symbol: AAPL\n",
      "\n",
      "========================================\n",
      "ðŸš€ ENHANCED WORKFLOW FOR: AAPL\n",
      "========================================\n",
      "Fetching data for AAPL from Twelve Data (interval=1day, from 2022-06-04 to 2025-06-03)...\n",
      "Successfully fetched/processed 750 data points for AAPL.\n",
      "\n",
      "--- ðŸ”§ Feature Engineering: AAPL ---\n",
      "Adding technical indicators...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38379/4212630612.py:192: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[9.98611407e+09 8.00782855e+09 1.23548189e+10 1.76871775e+10\n",
      " 1.09688179e+10 9.95967086e+09 9.95940605e+09 1.25755687e+10\n",
      " 9.97233035e+09 9.79635737e+09 1.02719769e+10 1.05694694e+10\n",
      " 9.63284651e+09 9.45818898e+09 1.13631385e+10 1.14780874e+10\n",
      " 1.14194116e+10 1.24235717e+10 9.87757346e+09 1.00419950e+10\n",
      " 1.03075722e+10 1.22197531e+10 1.27330542e+10 1.64772630e+10\n",
      " 1.09881365e+10 1.35716587e+10 9.19816034e+09 9.98346202e+09\n",
      " 1.18232614e+10 9.67097850e+09 1.16499329e+10 9.33857157e+09\n",
      " 9.74196305e+09 1.38739808e+10 8.68182651e+09 1.20859660e+10\n",
      " 1.35908341e+10 1.06564850e+10 1.70375068e+10 1.24421969e+10\n",
      " 1.68017624e+10 1.41309629e+10 1.28473936e+10 1.61259518e+10\n",
      " 1.27816217e+10 1.15707123e+10 9.99105288e+09 1.58872146e+10\n",
      " 1.24359106e+10 1.20936924e+10 1.42446076e+10 9.28966227e+09\n",
      " 1.26294602e+10 1.12878711e+10 1.13142214e+10 2.53217851e+10\n",
      " 1.25398382e+10 1.71641023e+10 1.39115016e+10 1.09080380e+10\n",
      " 1.35458486e+10 1.20141956e+10 1.13232148e+10 7.72796261e+09\n",
      " 8.78874185e+09 1.62356225e+10 1.05462467e+10 1.01707065e+10\n",
      " 8.84851958e+09 1.08724857e+10 1.01008282e+10 1.37599618e+10\n",
      " 1.16005664e+10 9.78646964e+09 1.12560026e+10 9.28421318e+09\n",
      " 9.20237836e+09 9.49610381e+09 7.73378147e+09 8.64235945e+09\n",
      " 9.49524164e+09 1.09668336e+10 1.15099425e+10 9.43356275e+09\n",
      " 7.75704819e+09 1.02597539e+10 1.12187659e+10 1.77575559e+10\n",
      " 2.36531981e+10 1.27905743e+10 9.51706899e+09 1.01357423e+10\n",
      " 7.21041317e+09 6.66787773e+09 1.05973489e+10 1.35303380e+10\n",
      " 7.20927441e+09 1.27054446e+10 1.12039877e+10 1.17336367e+10\n",
      " 1.17783146e+10 1.53700545e+10 1.15216877e+10 1.17123191e+10\n",
      " 1.20562687e+10 1.07780607e+10 9.44065802e+09 8.22892145e+09\n",
      " 8.02092546e+09 1.12706128e+10 9.43297948e+09 7.67571884e+09\n",
      " 7.43822714e+09 1.12428743e+10 8.15453475e+09 8.31267856e+09\n",
      " 7.97452246e+09 1.08637797e+10 9.33790006e+09 8.89730492e+09\n",
      " 1.95897615e+10 9.69014555e+09 9.30311330e+09 8.59096550e+09\n",
      " 7.25714883e+09 1.14154201e+10 9.78787029e+09 9.69023774e+09\n",
      " 9.58351277e+09 9.94039710e+09 1.77120500e+10 1.23366373e+10\n",
      " 1.11913526e+10 2.20548372e+10 9.01976806e+09 8.85895485e+09\n",
      " 9.92597445e+09 1.00685038e+10 1.05417606e+10 1.21337144e+10\n",
      " 9.52684296e+09 9.89486590e+09 9.50559906e+09 9.67570713e+09\n",
      " 8.78435837e+09 1.64385657e+10 8.94379767e+09 1.15413230e+10\n",
      " 7.87584582e+09 9.76925981e+09 9.35613794e+09 1.57252226e+10\n",
      " 8.77168000e+09 7.22026451e+09 9.23265175e+09 9.44257841e+09\n",
      " 7.61211187e+09 9.77655958e+09 7.81007094e+09 8.11554314e+09\n",
      " 7.45114775e+09 9.50767827e+09 9.15082586e+09 7.87793938e+09\n",
      " 9.69123290e+09 1.13567933e+10 1.14388905e+10 8.65352272e+09\n",
      " 1.17164206e+10 1.05526049e+10 1.19610916e+10 9.25680273e+09\n",
      " 9.94434069e+09 8.11408236e+09 8.88820816e+09 9.02411056e+09\n",
      " 9.16968433e+09 8.46067744e+09 1.00840311e+10 7.54330174e+09\n",
      " 7.80824996e+09 8.51560981e+09 1.02532891e+10 7.57384537e+09\n",
      " 9.82804399e+09 8.69789424e+09 9.83683246e+09 1.36833622e+10\n",
      " 1.13822654e+10 1.27713375e+10 9.01167686e+09 9.82689253e+09\n",
      " 1.22677457e+10 1.12573015e+10 1.01357438e+10 1.03261176e+10\n",
      " 8.88619451e+09 7.59431678e+09 7.31019429e+09 8.70997681e+09\n",
      " 1.28370713e+10 7.93291445e+09 9.22428407e+09 1.04151571e+10\n",
      " 1.02082680e+10 1.38651689e+10 1.32302063e+10 8.00412063e+09\n",
      " 6.59898412e+09 1.08956000e+10 7.89745727e+09 8.68023877e+09\n",
      " 7.52069210e+09 1.46546896e+10 1.31141791e+10 1.16555532e+10\n",
      " 8.25595341e+09 1.04620365e+10 1.20606821e+10 1.30691700e+10\n",
      " 8.19627617e+09 1.01368440e+10 8.53192482e+09 7.55660592e+09\n",
      " 9.61810469e+09 9.88803149e+09 1.30249841e+10 1.04077419e+10\n",
      " 1.26227852e+10 1.32294752e+10 9.67591167e+09 9.48135364e+09\n",
      " 1.03875007e+10 8.09026070e+09 9.12187171e+09 7.18981841e+09\n",
      " 1.57448407e+10 1.79183493e+10 7.98523720e+09 8.23764065e+09\n",
      " 8.11482711e+09 8.57215960e+09 7.61983755e+09 1.18757914e+10\n",
      " 1.62470454e+10 3.00898188e+10 1.41371691e+10 9.00209590e+09\n",
      " 1.34003330e+10 9.81541667e+09 1.33229651e+10 1.00511572e+10\n",
      " 8.45777740e+09 8.12340494e+09 6.87903644e+09 9.96964115e+09\n",
      " 9.55836280e+09 1.43985768e+10 9.70793814e+09 9.22052856e+09\n",
      " 1.06082338e+10 1.04078334e+10 3.49308741e+10 4.22798781e+10\n",
      " 2.09618648e+10 2.02559353e+10 1.68835139e+10 1.18921488e+10\n",
      " 1.40978938e+10 1.06545106e+10 1.30103818e+10 1.26879321e+10\n",
      " 8.24788855e+09 1.35815128e+10 1.33715335e+10 1.09685619e+10\n",
      " 1.45152221e+10 1.22335835e+10 1.47125096e+10 1.08426737e+10\n",
      " 9.06159537e+09 9.09569998e+09 1.11168985e+10 2.33366435e+10\n",
      " 1.33352684e+10 1.00034981e+10 9.07320347e+09 8.27288639e+09\n",
      " 9.74610492e+09 9.29357354e+09 1.04095772e+10 9.99915840e+09\n",
      " 6.85898652e+09 7.87357447e+09 8.75899028e+09 8.16637752e+09\n",
      " 8.65091399e+09 1.19659592e+10 8.16987215e+09 9.86326096e+09\n",
      " 8.32635044e+09 8.19221117e+09 9.83505139e+09 1.31956451e+10\n",
      " 1.52109995e+10 7.31805895e+10 9.90828696e+09 8.32178216e+09\n",
      " 7.76484651e+09 1.26473574e+10 8.42893858e+09 7.16753250e+09\n",
      " 7.66011433e+09 6.44190253e+09 9.19455241e+09 1.51887363e+10\n",
      " 7.66028053e+09 1.09143429e+10 8.55340346e+09 8.97885235e+09\n",
      " 8.42850296e+09 6.26450950e+09 1.21849487e+10 9.55022893e+09\n",
      " 8.71379728e+09 9.06510920e+09 1.09197860e+10 1.02140477e+10\n",
      " 1.01724702e+10 8.26956806e+09 8.76153324e+09 2.09112428e+10\n",
      " 1.07908152e+10 7.86575919e+09 6.73214852e+09 1.15134314e+10\n",
      " 9.38246991e+09 1.07759670e+10 9.73771216e+09 1.09500676e+10\n",
      " 9.12189330e+09 1.12042575e+10 8.21875680e+09 1.29249059e+10\n",
      " 1.29604723e+10 3.71285007e+10 1.04104135e+10 5.97673925e+09\n",
      " 7.05213913e+09 1.10441301e+10 9.22615134e+09 9.44363611e+09\n",
      " 1.42732762e+10 1.35124727e+10 2.16917809e+10 1.78987905e+10\n",
      " 1.08140170e+10 1.32761743e+10 2.41444830e+10 1.04042367e+10\n",
      " 9.15770407e+09 6.95713340e+09 1.24623107e+10 1.06247858e+10\n",
      " 1.28573762e+10 9.96694950e+09 1.19043900e+10 7.87972360e+09\n",
      " 7.93810776e+09 1.31124498e+10 1.26670135e+10 1.18744892e+10\n",
      " 1.12953777e+10 1.06353707e+10 1.10316262e+10 1.27632480e+10\n",
      " 1.02426788e+10 9.04276876e+09 1.17427143e+10 2.03438414e+10\n",
      " 9.75755554e+09 7.67977022e+09 8.27708662e+09 1.44524764e+10\n",
      " 8.08082752e+09 8.02018124e+09 3.51181998e+10 1.70135682e+10\n",
      " 2.08320751e+10 1.00996813e+10 1.05476014e+10 1.08575434e+10\n",
      " 9.77981451e+09 7.96590954e+09 8.12442820e+09 7.75613832e+09\n",
      " 1.10280943e+10 1.21764435e+10 9.96487144e+09 7.24958685e+09\n",
      " 1.33676201e+10 1.09931782e+10 1.04710452e+10 1.12273044e+10\n",
      " 9.11403521e+09 7.11515508e+09]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  mfi_series = df_feat.ta.mfi(high=df_feat[h], low=df_feat[l], close=df_feat[c], volume=df_feat[v], append=False)\n",
      "/tmp/ipykernel_38379/4212630612.py:192: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[1.00300486e+10 1.26473852e+10 1.62339912e+10 1.12517242e+10\n",
      " 1.41098082e+10 9.35097896e+09 9.19819886e+09 1.34875418e+10\n",
      " 9.16163569e+09 1.02996074e+10 1.20866617e+10 8.22715736e+09\n",
      " 8.37170893e+09 9.62605634e+09 9.33988618e+09 1.03961021e+10\n",
      " 1.08420068e+10 1.21129296e+10 1.16096177e+10 9.07126366e+09\n",
      " 9.00750348e+09 1.31135798e+10 1.18304358e+10 1.24439573e+10\n",
      " 1.39301643e+10 1.16553317e+10 1.14334111e+10 1.31207271e+10\n",
      " 1.91242078e+10 1.36645455e+10 1.38433070e+10 2.43645396e+10\n",
      " 1.57989375e+10 1.32324147e+10 1.44192093e+10 2.17748714e+10\n",
      " 1.83613494e+10 1.74603932e+10 1.21051987e+10 1.05078306e+10\n",
      " 1.07474853e+10 9.78699739e+09 8.85840469e+09 1.32109186e+10\n",
      " 1.59395814e+10 1.50016338e+10 1.21970226e+10 1.37973180e+10\n",
      " 1.37226250e+10 1.94995641e+10 1.14984687e+10 1.01890705e+10\n",
      " 9.54604987e+09 8.73227356e+09 5.21028372e+09 1.00231277e+10\n",
      " 1.19036739e+10 9.63080285e+09 9.32352165e+09 9.86111998e+09\n",
      " 1.18233256e+10 1.36634847e+10 2.16686936e+10 1.05828977e+10\n",
      " 1.02066755e+10 1.03047635e+10 8.37932180e+09 8.97469439e+09\n",
      " 1.09059270e+10 9.94537174e+09 1.42067818e+10 1.12906932e+10\n",
      " 1.01891557e+10 8.30884865e+09 7.87348777e+09 9.27989439e+09\n",
      " 9.20540035e+09 9.46199338e+09 1.06093803e+10 9.78194872e+09\n",
      " 8.50598497e+09 8.64767076e+09 9.41719684e+09 1.05294415e+10\n",
      " 8.99778928e+09 8.79456346e+09 7.58402001e+09 8.12851537e+09\n",
      " 7.46949871e+09 8.09161205e+09 7.60099180e+09 8.55389694e+09\n",
      " 8.17122252e+09 1.02184305e+10 8.32866063e+09 7.23794582e+09\n",
      " 8.42405159e+09 7.70055332e+09 7.68135658e+09 8.05371554e+09\n",
      " 6.84532943e+09 8.74413200e+09 9.64432271e+09 6.91916716e+09\n",
      " 8.01850324e+09 7.45937645e+09 8.17474226e+09 1.09760674e+10\n",
      " 1.34617885e+10 7.80997596e+09 7.85034219e+09 6.41918929e+09\n",
      " 9.96844769e+09 7.58656525e+09 8.73225707e+09 7.74632364e+09\n",
      " 1.16026757e+10 1.10746627e+10 1.87684057e+10 9.22146542e+09\n",
      " 9.11022876e+09 8.95331451e+09 6.06199514e+09 8.99196269e+09\n",
      " 8.61346230e+09 1.12981311e+10 8.76765207e+09 7.91982781e+09\n",
      " 1.15607388e+10 1.38595190e+10 9.22262426e+09 6.88986409e+09\n",
      " 9.73538072e+09 1.17210296e+10 2.12797393e+10 1.75419067e+10\n",
      " 1.21550120e+10 1.07902414e+10 9.23585909e+09 7.76453203e+09\n",
      " 8.32233683e+09 1.15607871e+10 1.06247043e+10 9.77101170e+09\n",
      " 8.56229707e+09 1.50765874e+10 1.98471489e+10 1.60030566e+10\n",
      " 1.47606465e+10 1.06652908e+10 1.91229052e+10 1.03352211e+10\n",
      " 1.10142578e+10 1.11700046e+10 1.14329200e+10 9.57642801e+09\n",
      " 8.54432300e+09 9.23836424e+09 9.35345262e+09 1.01741582e+10\n",
      " 9.64820944e+09 1.04469967e+10 1.11462580e+10 9.64611734e+09\n",
      " 9.80833169e+09 1.18636379e+10 7.61739177e+09 1.40070377e+10\n",
      " 8.07166933e+09 9.65222821e+09 7.26970698e+09 4.57013885e+09\n",
      " 7.69634251e+09 8.17886331e+09 9.24588837e+09 8.19960298e+09\n",
      " 1.17436478e+10 2.53507918e+10 1.09071159e+10 1.02280952e+10\n",
      " 9.06811586e+09 7.20157584e+09 5.58884755e+09 9.26072904e+09\n",
      " 8.22252699e+09 1.53420733e+10 1.07786433e+10 1.30981358e+10\n",
      " 1.12999491e+10 9.10938798e+09 1.20014124e+10 8.61034696e+09\n",
      " 1.06638122e+10 8.60842561e+09 9.01282435e+09 1.05631986e+10\n",
      " 1.02772591e+10 1.88780066e+10 7.71737748e+09 7.83563797e+09\n",
      " 1.04534350e+10 1.00541970e+10 1.19897820e+10 9.09253995e+09\n",
      " 9.73118307e+09 8.26878652e+09 7.41838640e+09 8.88790690e+09\n",
      " 2.47327169e+10 1.31683147e+10 1.42857316e+10 1.62289530e+10\n",
      " 1.16379609e+10 1.21574866e+10 1.03352366e+10 9.01195969e+09\n",
      " 2.09072672e+10 1.83941409e+10 1.22158769e+10 9.26954644e+09\n",
      " 9.76961011e+09 1.12567384e+10 7.87259924e+09 8.32698403e+09\n",
      " 7.13424413e+09 6.31106219e+09 8.35083094e+09 1.27901213e+10\n",
      " 1.25655480e+10 8.59636416e+09 7.21920459e+09 1.11932399e+10\n",
      " 1.13258406e+10 8.58393686e+09 1.43083283e+10 8.21931644e+09\n",
      " 9.31047667e+09 7.84182433e+09 6.62929750e+09 9.59778020e+09\n",
      " 1.01191828e+10 8.03251009e+09 1.88873269e+10 1.49352004e+10\n",
      " 1.72123252e+10 1.81757912e+10 5.14601818e+10 1.75262917e+10\n",
      " 1.47913335e+10 1.01372906e+10 1.31318290e+10 1.48993866e+10\n",
      " 1.10490934e+10 8.98613886e+09 1.36007503e+10 1.11854663e+10\n",
      " 7.90737873e+09 1.37473034e+10 2.46576957e+10 1.43567972e+10\n",
      " 9.15367003e+09 9.85722258e+09 6.91966735e+09 1.21327734e+10\n",
      " 1.12584208e+10 9.64645162e+09 1.07471624e+10 1.47547436e+10\n",
      " 1.13211290e+10 1.28105143e+10 1.23043175e+10 9.55725319e+09\n",
      " 1.43361622e+10 7.42191868e+09 7.66902357e+09 8.80588534e+09\n",
      " 7.24427352e+09 7.88139514e+09 9.12480749e+09 1.20897998e+10\n",
      " 7.15311124e+09 8.26779129e+09 1.08752609e+10 1.46143729e+10\n",
      " 1.45464776e+10 9.95535648e+09 9.40127334e+09 1.08018422e+10\n",
      " 8.02828171e+09 9.60616662e+09 8.96631543e+09 8.11112026e+09\n",
      " 1.41948500e+10 1.51978453e+10 1.08337799e+10 8.96582362e+09\n",
      " 9.91136640e+09 1.36521986e+10 9.78508449e+09 9.92950633e+09\n",
      " 9.11196373e+09 1.46051189e+10 1.15604788e+10 1.66074473e+10\n",
      " 1.57694308e+10 2.17860125e+10 1.22125825e+10 1.66959381e+10\n",
      " 9.11796420e+09 7.56625218e+09 1.07302701e+10 9.83324129e+09\n",
      " 1.35288119e+10 1.27444110e+10 1.10437021e+10 1.65249652e+10\n",
      " 1.68550438e+10 1.36278493e+10 1.29883959e+10 1.04861415e+10\n",
      " 7.66319272e+09 8.75199529e+09 2.10950632e+10 2.41581047e+10\n",
      " 2.94311241e+10 2.14312098e+10 2.30840720e+10 1.03620546e+10\n",
      " 1.16944093e+10 8.98655724e+09 2.06903386e+10 1.38313079e+10\n",
      " 1.01780841e+10 1.34546680e+10 9.51548384e+09 1.15606445e+10\n",
      " 9.57446120e+09 8.78773024e+09 1.20365747e+10 9.40784285e+09\n",
      " 1.53318021e+10 1.03182501e+10 1.41543018e+10]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  mfi_series = df_feat.ta.mfi(high=df_feat[h], low=df_feat[l], close=df_feat[c], volume=df_feat[v], append=False)\n",
      "/tmp/ipykernel_38379/4212630612.py:312: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  df_new.ta.stc(close=df_new[price_col], tclength=23, fast=50, slow=100, factor=0.5, append=True, col_names=('STC_23_50_05', 'STCD_23_50_05', 'STCK_23_50_05'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Cleaning (Inf/NaN Handling & Imputation): AAPL ---\n",
      "NaNs before imputation: 1858\n",
      "NaNs after imputation: 0\n",
      "\n",
      "--- ðŸ“Š Regime Detection (Simplified): AAPL ---\n",
      "Simple Regimes (0:Med,1:Low,2:High):\n",
      "regime_simple\n",
      "0    34.666667\n",
      "1    32.666667\n",
      "2    32.666667\n",
      "Name: proportion, dtype: float64 %\n",
      "\n",
      "--- ðŸŽ¯ Target Definition: AAPL ---\n",
      "Target distribution:\n",
      "target\n",
      "0    55.333333\n",
      "1    44.666667\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- Causal Feature Discovery and Effect Estimation for AAPL (Subset Analysis) ---\n",
      "Iteratively estimating causal effects for 11 selected features: ['RSI_14', 'MACDh_12_26_9', 'ADX_14', 'close_cwt_mean', 'close_cwt_std', 'close_entropy_sample', 'regime_simple', 'volatility_20', 'log_returns', 'close_trans_seq_volatility', 'close_trans_seq_autocorr1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "[I 2025-06-03 16:36:22,963] A new study created in memory with name: no-name-1bf5c7d3-70c4-44f8-bc9b-036edd5a1d85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal features ranked by absolute effect strength (top 5 of 11):\n",
      "  close_trans_seq_volatility: 0.1427\n",
      "  close_trans_seq_autocorr1: 0.0892\n",
      "  close_cwt_std: 0.0830\n",
      "  close_entropy_sample: 0.0773\n",
      "  close_cwt_mean: 0.0707\n",
      "\n",
      "--- ML Preparation & Feature Selection: AAPL ---\n",
      "Train shapes: X_train=(638, 100), y_train=(638,); Test shapes: X_test=(112, 100), y_test=(112,)\n",
      "Selected 35 features: ['close_trans_seq_volatility', 'close_trans_seq_autocorr1', 'close_cwt_std', 'close_entropy_sample', 'close_cwt_mean', 'RSI_14', 'regime_simple', 'MACDh_12_26_9', 'log_returns', 'volatility_20']...\n",
      "Class distribution before SMOTE: \n",
      "target\n",
      "0    0.532915\n",
      "1    0.467085\n",
      "Name: proportion, dtype: float64\n",
      "Class distribution after SMOTE: \n",
      "target\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n",
      "Shape of X_train after SMOTE: (680, 35)\n",
      "\n",
      "--- Hyperparameter Optimization (Optuna for LightGBM): AAPL ---\n",
      "Optimizing LightGBM HPs with Optuna (50 trials)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2dab613419549008d75296f8916fad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.9344899633417024, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9344899633417024\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7222061197670793, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7222061197670793\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.008930268274096343, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.008930268274096343\n",
      "[I 2025-06-03 16:36:23,196] Trial 0 finished with value: 0.6644854373311424 and parameters: {'n_estimators': 100, 'learning_rate': 0.01120862874245066, 'num_leaves': 40, 'max_depth': 5, 'min_child_samples': 9, 'feature_fraction': 0.7222061197670793, 'bagging_fraction': 0.9344899633417024, 'bagging_freq': 3, 'reg_alpha': 0.00492746658910613, 'reg_lambda': 0.07482502956214401, 'min_gain_to_split': 0.008930268274096343}. Best is trial 0 with value: 0.6644854373311424.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.607555288933014, subsample=1.0 will be ignored. Current value: bagging_fraction=0.607555288933014\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6871956600713562, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6871956600713562\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.07023224000864338, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.07023224000864338\n",
      "[I 2025-06-03 16:36:23,240] Trial 1 finished with value: 0.6833815749830187 and parameters: {'n_estimators': 650, 'learning_rate': 0.06521597429388001, 'num_leaves': 21, 'max_depth': 8, 'min_child_samples': 41, 'feature_fraction': 0.6871956600713562, 'bagging_fraction': 0.607555288933014, 'bagging_freq': 6, 'reg_alpha': 0.2807813964093168, 'reg_lambda': 0.07797322002335917, 'min_gain_to_split': 0.07023224000864338}. Best is trial 0 with value: 0.6644854373311424.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9866762735171364, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9866762735171364\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8842314476140736, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8842314476140736\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.09575097001614485, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.09575097001614485\n",
      "[I 2025-06-03 16:36:23,298] Trial 2 finished with value: 0.674794210448104 and parameters: {'n_estimators': 650, 'learning_rate': 0.025372787744050673, 'num_leaves': 11, 'max_depth': -1, 'min_child_samples': 45, 'feature_fraction': 0.8842314476140736, 'bagging_fraction': 0.9866762735171364, 'bagging_freq': 5, 'reg_alpha': 0.0026662334669994843, 'reg_lambda': 0.0016207991893912204, 'min_gain_to_split': 0.09575097001614485}. Best is trial 0 with value: 0.6644854373311424.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5383390397886498, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5383390397886498\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9857285186675724, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9857285186675724\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04260093086415894, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04260093086415894\n",
      "[I 2025-06-03 16:36:23,415] Trial 3 finished with value: 0.6557295289201063 and parameters: {'n_estimators': 900, 'learning_rate': 0.034606841766965754, 'num_leaves': 36, 'max_depth': -1, 'min_child_samples': 9, 'feature_fraction': 0.9857285186675724, 'bagging_fraction': 0.5383390397886498, 'bagging_freq': 4, 'reg_alpha': 0.9821526070429388, 'reg_lambda': 0.012318314887413424, 'min_gain_to_split': 0.04260093086415894}. Best is trial 3 with value: 0.6557295289201063.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9668711606901053, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9668711606901053\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7514373022628937, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7514373022628937\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0033781787593478497, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0033781787593478497\n",
      "[I 2025-06-03 16:36:23,500] Trial 4 finished with value: 0.6607836051695893 and parameters: {'n_estimators': 850, 'learning_rate': 0.04402889980725345, 'num_leaves': 47, 'max_depth': 9, 'min_child_samples': 36, 'feature_fraction': 0.7514373022628937, 'bagging_fraction': 0.9668711606901053, 'bagging_freq': 5, 'reg_alpha': 0.06999777684927823, 'reg_lambda': 0.5277509466196448, 'min_gain_to_split': 0.0033781787593478497}. Best is trial 3 with value: 0.6557295289201063.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7068060251657038, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7068060251657038\n",
      "[LightGBM] [Warning] feature_fraction is set=0.910700119517811, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.910700119517811\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04390978715842395, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04390978715842395\n",
      "[I 2025-06-03 16:36:23,616] Trial 5 finished with value: 0.6578515867602469 and parameters: {'n_estimators': 250, 'learning_rate': 0.02701345016338296, 'num_leaves': 44, 'max_depth': 0, 'min_child_samples': 20, 'feature_fraction': 0.910700119517811, 'bagging_fraction': 0.7068060251657038, 'bagging_freq': 1, 'reg_alpha': 0.17060472713418998, 'reg_lambda': 0.003171351301477125, 'min_gain_to_split': 0.04390978715842395}. Best is trial 3 with value: 0.6557295289201063.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5504063004260225, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5504063004260225\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5629512062538933, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5629512062538933\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.078872690067138, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.078872690067138\n",
      "[I 2025-06-03 16:36:23,694] Trial 6 finished with value: 0.6572735780658776 and parameters: {'n_estimators': 750, 'learning_rate': 0.04078345201723643, 'num_leaves': 25, 'max_depth': 0, 'min_child_samples': 36, 'feature_fraction': 0.5629512062538933, 'bagging_fraction': 0.5504063004260225, 'bagging_freq': 7, 'reg_alpha': 0.3474253116110815, 'reg_lambda': 0.34763755663010915, 'min_gain_to_split': 0.078872690067138}. Best is trial 3 with value: 0.6557295289201063.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5978259485391713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5978259485391713\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5024350258430643, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5024350258430643\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0873836500472554, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0873836500472554\n",
      "[I 2025-06-03 16:36:23,743] Trial 7 finished with value: 0.6825365991164929 and parameters: {'n_estimators': 650, 'learning_rate': 0.09612808577283329, 'num_leaves': 17, 'max_depth': -1, 'min_child_samples': 36, 'feature_fraction': 0.5024350258430643, 'bagging_fraction': 0.5978259485391713, 'bagging_freq': 6, 'reg_alpha': 0.0014884321479383652, 'reg_lambda': 0.0018801385691780022, 'min_gain_to_split': 0.0873836500472554}. Best is trial 3 with value: 0.6557295289201063.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6485025871893397, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6485025871893397\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5787172484112845, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5787172484112845\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.012340749267351615, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.012340749267351615\n",
      "[I 2025-06-03 16:36:23,837] Trial 8 finished with value: 0.675591335875368 and parameters: {'n_estimators': 200, 'learning_rate': 0.01465222114879912, 'num_leaves': 16, 'max_depth': 6, 'min_child_samples': 25, 'feature_fraction': 0.5787172484112845, 'bagging_fraction': 0.6485025871893397, 'bagging_freq': 2, 'reg_alpha': 0.004660336483764069, 'reg_lambda': 0.1853105418843941, 'min_gain_to_split': 0.012340749267351615}. Best is trial 3 with value: 0.6557295289201063.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8914795036311831, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8914795036311831\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6950877864951952, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6950877864951952\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.002494876758409481, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.002494876758409481\n",
      "[I 2025-06-03 16:36:23,860] Trial 9 finished with value: 0.6934579436474461 and parameters: {'n_estimators': 450, 'learning_rate': 0.03636810369257951, 'num_leaves': 18, 'max_depth': 1, 'min_child_samples': 30, 'feature_fraction': 0.6950877864951952, 'bagging_fraction': 0.8914795036311831, 'bagging_freq': 2, 'reg_alpha': 0.014548546306421304, 'reg_lambda': 0.18201111693061603, 'min_gain_to_split': 0.002494876758409481}. Best is trial 3 with value: 0.6557295289201063.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8182590169828704, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8182590169828704\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9896811983193884, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9896811983193884\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04502667828385, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04502667828385\n",
      "[I 2025-06-03 16:36:23,940] Trial 10 finished with value: 0.6756122252906468 and parameters: {'n_estimators': 1000, 'learning_rate': 0.017730648256825726, 'num_leaves': 34, 'max_depth': 3, 'min_child_samples': 7, 'feature_fraction': 0.9896811983193884, 'bagging_fraction': 0.8182590169828704, 'bagging_freq': 0, 'reg_alpha': 0.8010072894497718, 'reg_lambda': 0.009420969653752735, 'min_gain_to_split': 0.04502667828385}. Best is trial 3 with value: 0.6557295289201063.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.524388995268006, subsample=1.0 will be ignored. Current value: bagging_fraction=0.524388995268006\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8147109235782063, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8147109235782063\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0633811398687479, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0633811398687479\n",
      "[I 2025-06-03 16:36:23,998] Trial 11 finished with value: 0.6796343954615697 and parameters: {'n_estimators': 900, 'learning_rate': 0.04805965333541182, 'num_leaves': 28, 'max_depth': 2, 'min_child_samples': 16, 'feature_fraction': 0.8147109235782063, 'bagging_fraction': 0.524388995268006, 'bagging_freq': 7, 'reg_alpha': 0.655515770680229, 'reg_lambda': 0.01771080544143642, 'min_gain_to_split': 0.0633811398687479}. Best is trial 3 with value: 0.6557295289201063.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5199266706183717, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5199266706183717\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5856957743212613, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5856957743212613\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.029330238636063495, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.029330238636063495\n",
      "[I 2025-06-03 16:36:24,069] Trial 12 finished with value: 0.6716454048273032 and parameters: {'n_estimators': 800, 'learning_rate': 0.06111495877371494, 'num_leaves': 28, 'max_depth': 3, 'min_child_samples': 30, 'feature_fraction': 0.5856957743212613, 'bagging_fraction': 0.5199266706183717, 'bagging_freq': 4, 'reg_alpha': 0.14600951646743357, 'reg_lambda': 0.985249466452842, 'min_gain_to_split': 0.029330238636063495}. Best is trial 3 with value: 0.6557295289201063.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5010791402131343, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5010791402131343\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6060836716759883, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6060836716759883\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.07243446478729063, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.07243446478729063\n",
      "[I 2025-06-03 16:36:24,132] Trial 13 finished with value: 0.6914462829296953 and parameters: {'n_estimators': 1000, 'learning_rate': 0.022326906469455447, 'num_leaves': 36, 'max_depth': 1, 'min_child_samples': 50, 'feature_fraction': 0.6060836716759883, 'bagging_fraction': 0.5010791402131343, 'bagging_freq': 7, 'reg_alpha': 0.9976268142281904, 'reg_lambda': 0.0076851352430520915, 'min_gain_to_split': 0.07243446478729063}. Best is trial 3 with value: 0.6557295289201063.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7627873841389562, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7627873841389562\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9984235595531994, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9984235595531994\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.029919757957898432, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.029919757957898432\n",
      "[I 2025-06-03 16:36:24,284] Trial 14 finished with value: 0.6436951152689664 and parameters: {'n_estimators': 450, 'learning_rate': 0.03593537914302616, 'num_leaves': 25, 'max_depth': -1, 'min_child_samples': 14, 'feature_fraction': 0.9984235595531994, 'bagging_fraction': 0.7627873841389562, 'bagging_freq': 4, 'reg_alpha': 0.03248329143943518, 'reg_lambda': 0.03574135457372048, 'min_gain_to_split': 0.029919757957898432}. Best is trial 14 with value: 0.6436951152689664.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7885422621882554, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7885422621882554\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9997474919872789, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9997474919872789\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02787551761702942, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02787551761702942\n",
      "[I 2025-06-03 16:36:24,368] Trial 15 finished with value: 0.6815125198894683 and parameters: {'n_estimators': 450, 'learning_rate': 0.03243658599236316, 'num_leaves': 34, 'max_depth': 4, 'min_child_samples': 13, 'feature_fraction': 0.9997474919872789, 'bagging_fraction': 0.7885422621882554, 'bagging_freq': 4, 'reg_alpha': 0.0234007150276931, 'reg_lambda': 0.033546380019853765, 'min_gain_to_split': 0.02787551761702942}. Best is trial 14 with value: 0.6436951152689664.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7056010509144073, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7056010509144073\n",
      "[LightGBM] [Warning] feature_fraction is set=0.92542740487468, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.92542740487468\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02942070279688847, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02942070279688847\n",
      "[I 2025-06-03 16:36:24,514] Trial 16 finished with value: 0.6589714155224826 and parameters: {'n_estimators': 450, 'learning_rate': 0.070132125363208, 'num_leaves': 50, 'max_depth': 7, 'min_child_samples': 5, 'feature_fraction': 0.92542740487468, 'bagging_fraction': 0.7056010509144073, 'bagging_freq': 3, 'reg_alpha': 0.06715114314704694, 'reg_lambda': 0.030744745699270017, 'min_gain_to_split': 0.02942070279688847}. Best is trial 14 with value: 0.6436951152689664.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8506285956966093, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8506285956966093\n",
      "[LightGBM] [Warning] feature_fraction is set=0.843904914241557, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.843904914241557\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.056571470287916745, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.056571470287916745\n",
      "[I 2025-06-03 16:36:24,683] Trial 17 finished with value: 0.6591720284620302 and parameters: {'n_estimators': 400, 'learning_rate': 0.01982892374153371, 'num_leaves': 40, 'max_depth': -1, 'min_child_samples': 20, 'feature_fraction': 0.843904914241557, 'bagging_fraction': 0.8506285956966093, 'bagging_freq': 5, 'reg_alpha': 0.01107506667889906, 'reg_lambda': 0.0049503424589200425, 'min_gain_to_split': 0.056571470287916745}. Best is trial 14 with value: 0.6436951152689664.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.730875659617236, subsample=1.0 will be ignored. Current value: bagging_fraction=0.730875659617236\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9557658266980931, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9557658266980931\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.01981096605444812, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.01981096605444812\n",
      "[I 2025-06-03 16:36:24,848] Trial 18 finished with value: 0.6678734472201541 and parameters: {'n_estimators': 550, 'learning_rate': 0.030861333488290146, 'num_leaves': 23, 'max_depth': 10, 'min_child_samples': 12, 'feature_fraction': 0.9557658266980931, 'bagging_fraction': 0.730875659617236, 'bagging_freq': 2, 'reg_alpha': 0.051559678073778786, 'reg_lambda': 0.015931966716013303, 'min_gain_to_split': 0.01981096605444812}. Best is trial 14 with value: 0.6436951152689664.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6734523754983521, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6734523754983521\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8121806789587075, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8121806789587075\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.036743131883650394, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.036743131883650394\n",
      "[I 2025-06-03 16:36:24,912] Trial 19 finished with value: 0.6756040482302509 and parameters: {'n_estimators': 300, 'learning_rate': 0.04959569945785059, 'num_leaves': 31, 'max_depth': 3, 'min_child_samples': 18, 'feature_fraction': 0.8121806789587075, 'bagging_fraction': 0.6734523754983521, 'bagging_freq': 4, 'reg_alpha': 0.038985719281712045, 'reg_lambda': 0.05970795376839483, 'min_gain_to_split': 0.036743131883650394}. Best is trial 14 with value: 0.6436951152689664.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7723812720568131, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7723812720568131\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8733649443711572, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8733649443711572\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.05572913996866436, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.05572913996866436\n",
      "[I 2025-06-03 16:36:24,955] Trial 20 finished with value: 0.6940978514786877 and parameters: {'n_estimators': 550, 'learning_rate': 0.09702728415440875, 'num_leaves': 38, 'max_depth': 1, 'min_child_samples': 25, 'feature_fraction': 0.8733649443711572, 'bagging_fraction': 0.7723812720568131, 'bagging_freq': 3, 'reg_alpha': 0.10934351171293585, 'reg_lambda': 0.019521992630408283, 'min_gain_to_split': 0.05572913996866436}. Best is trial 14 with value: 0.6436951152689664.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5806472136860429, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5806472136860429\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5045147664825391, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5045147664825391\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.08355181238769838, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.08355181238769838\n",
      "[I 2025-06-03 16:36:25,136] Trial 21 finished with value: 0.6233457326572641 and parameters: {'n_estimators': 750, 'learning_rate': 0.040026254173546855, 'num_leaves': 25, 'max_depth': 0, 'min_child_samples': 12, 'feature_fraction': 0.5045147664825391, 'bagging_fraction': 0.5806472136860429, 'bagging_freq': 6, 'reg_alpha': 0.3859867866680439, 'reg_lambda': 0.14598890822246882, 'min_gain_to_split': 0.08355181238769838}. Best is trial 21 with value: 0.6233457326572641.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5720654692650093, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5720654692650093\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6429254787895309, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6429254787895309\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.03830097350316561, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.03830097350316561\n",
      "[I 2025-06-03 16:36:25,299] Trial 22 finished with value: 0.6355160399996339 and parameters: {'n_estimators': 700, 'learning_rate': 0.034043584158119165, 'num_leaves': 31, 'max_depth': 0, 'min_child_samples': 12, 'feature_fraction': 0.6429254787895309, 'bagging_fraction': 0.5720654692650093, 'bagging_freq': 6, 'reg_alpha': 0.38935418182175746, 'reg_lambda': 0.12912958097473531, 'min_gain_to_split': 0.03830097350316561}. Best is trial 21 with value: 0.6233457326572641.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5872862086647881, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5872862086647881\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5238140580935599, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5238140580935599\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.09905549875145116, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.09905549875145116\n",
      "[I 2025-06-03 16:36:25,449] Trial 23 finished with value: 0.6556367279180199 and parameters: {'n_estimators': 700, 'learning_rate': 0.054754084075631294, 'num_leaves': 26, 'max_depth': 0, 'min_child_samples': 15, 'feature_fraction': 0.5238140580935599, 'bagging_fraction': 0.5872862086647881, 'bagging_freq': 6, 'reg_alpha': 0.45686643667860605, 'reg_lambda': 0.1454750051856139, 'min_gain_to_split': 0.09905549875145116}. Best is trial 21 with value: 0.6233457326572641.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6329158040508845, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6329158040508845\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6402485917462735, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6402485917462735\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02139271036731114, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02139271036731114\n",
      "[I 2025-06-03 16:36:25,508] Trial 24 finished with value: 0.6857854890048026 and parameters: {'n_estimators': 600, 'learning_rate': 0.027756400266298133, 'num_leaves': 30, 'max_depth': 2, 'min_child_samples': 11, 'feature_fraction': 0.6402485917462735, 'bagging_fraction': 0.6329158040508845, 'bagging_freq': 6, 'reg_alpha': 0.22180018223939763, 'reg_lambda': 0.11781692926204346, 'min_gain_to_split': 0.02139271036731114}. Best is trial 21 with value: 0.6233457326572641.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5711102338415737, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5711102338415737\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6585918363552996, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6585918363552996\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.05204920845442132, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.05204920845442132\n",
      "[I 2025-06-03 16:36:25,623] Trial 25 finished with value: 0.6686791772639334 and parameters: {'n_estimators': 750, 'learning_rate': 0.03833204095562665, 'num_leaves': 20, 'max_depth': 0, 'min_child_samples': 22, 'feature_fraction': 0.6585918363552996, 'bagging_fraction': 0.5711102338415737, 'bagging_freq': 5, 'reg_alpha': 0.4616935211003712, 'reg_lambda': 0.04207623473873178, 'min_gain_to_split': 0.05204920845442132}. Best is trial 21 with value: 0.6233457326572641.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6697293499912562, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6697293499912562\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7681777002192045, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7681777002192045\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.03504427656566091, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.03504427656566091\n",
      "[I 2025-06-03 16:36:25,690] Trial 26 finished with value: 0.6824868383389872 and parameters: {'n_estimators': 350, 'learning_rate': 0.024349837763973925, 'num_leaves': 13, 'max_depth': 2, 'min_child_samples': 5, 'feature_fraction': 0.7681777002192045, 'bagging_fraction': 0.6697293499912562, 'bagging_freq': 6, 'reg_alpha': 0.023285025891160348, 'reg_lambda': 0.26291843219145816, 'min_gain_to_split': 0.03504427656566091}. Best is trial 21 with value: 0.6233457326572641.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7389626578665782, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7389626578665782\n",
      "[LightGBM] [Warning] feature_fraction is set=0.624541467821699, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.624541467821699\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.06213325750450625, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.06213325750450625\n",
      "[I 2025-06-03 16:36:25,738] Trial 27 finished with value: 0.6939224047559921 and parameters: {'n_estimators': 500, 'learning_rate': 0.07790742439796741, 'num_leaves': 23, 'max_depth': 1, 'min_child_samples': 15, 'feature_fraction': 0.624541467821699, 'bagging_fraction': 0.7389626578665782, 'bagging_freq': 7, 'reg_alpha': 0.11253109919606519, 'reg_lambda': 0.10360659949459249, 'min_gain_to_split': 0.06213325750450625}. Best is trial 21 with value: 0.6233457326572641.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8470872410211087, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8470872410211087\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5195499892412813, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5195499892412813\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.01931989781163808, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.01931989781163808\n",
      "[I 2025-06-03 16:36:25,836] Trial 28 finished with value: 0.676691662575221 and parameters: {'n_estimators': 750, 'learning_rate': 0.03023153660658611, 'num_leaves': 31, 'max_depth': 4, 'min_child_samples': 10, 'feature_fraction': 0.5195499892412813, 'bagging_fraction': 0.8470872410211087, 'bagging_freq': 5, 'reg_alpha': 0.010532716651035938, 'reg_lambda': 0.5074598478328638, 'min_gain_to_split': 0.01931989781163808}. Best is trial 21 with value: 0.6233457326572641.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6177722538348241, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6177722538348241\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7129665969204825, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7129665969204825\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.012177540882103547, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.012177540882103547\n",
      "[I 2025-06-03 16:36:25,958] Trial 29 finished with value: 0.6749158302431365 and parameters: {'n_estimators': 150, 'learning_rate': 0.01069813822197826, 'num_leaves': 26, 'max_depth': 6, 'min_child_samples': 24, 'feature_fraction': 0.7129665969204825, 'bagging_fraction': 0.6177722538348241, 'bagging_freq': 6, 'reg_alpha': 0.004729348605812183, 'reg_lambda': 0.05682566070181168, 'min_gain_to_split': 0.012177540882103547}. Best is trial 21 with value: 0.6233457326572641.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9015336497698856, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9015336497698856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5451639771142164, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5451639771142164\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.08548360524135595, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.08548360524135595\n",
      "[I 2025-06-03 16:36:26,284] Trial 30 finished with value: 0.6561898417942672 and parameters: {'n_estimators': 600, 'learning_rate': 0.016025914523097845, 'num_leaves': 33, 'max_depth': -1, 'min_child_samples': 8, 'feature_fraction': 0.5451639771142164, 'bagging_fraction': 0.9015336497698856, 'bagging_freq': 5, 'reg_alpha': 0.21197148190558013, 'reg_lambda': 0.09570174487040785, 'min_gain_to_split': 0.08548360524135595}. Best is trial 21 with value: 0.6233457326572641.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5763604786309202, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5763604786309202\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5384383158704549, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5384383158704549\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.09342117574104025, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.09342117574104025\n",
      "[I 2025-06-03 16:36:26,426] Trial 31 finished with value: 0.6730246360451616 and parameters: {'n_estimators': 700, 'learning_rate': 0.05256279951276481, 'num_leaves': 25, 'max_depth': 0, 'min_child_samples': 16, 'feature_fraction': 0.5384383158704549, 'bagging_fraction': 0.5763604786309202, 'bagging_freq': 6, 'reg_alpha': 0.488365739038703, 'reg_lambda': 0.15960577011282062, 'min_gain_to_split': 0.09342117574104025}. Best is trial 21 with value: 0.6233457326572641.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5854596029604242, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5854596029604242\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5007880248000153, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5007880248000153\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.08850929932584493, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.08850929932584493\n",
      "[I 2025-06-03 16:36:26,609] Trial 32 finished with value: 0.6438299023057191 and parameters: {'n_estimators': 700, 'learning_rate': 0.05405101312663973, 'num_leaves': 28, 'max_depth': 0, 'min_child_samples': 14, 'feature_fraction': 0.5007880248000153, 'bagging_fraction': 0.5854596029604242, 'bagging_freq': 6, 'reg_alpha': 0.3922767439437376, 'reg_lambda': 0.32503523490963665, 'min_gain_to_split': 0.08850929932584493}. Best is trial 21 with value: 0.6233457326572641.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6615392263897545, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6615392263897545\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6654217942993299, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6654217942993299\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.07396261226362182, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.07396261226362182\n",
      "[I 2025-06-03 16:36:26,673] Trial 33 finished with value: 0.6932624714386116 and parameters: {'n_estimators': 800, 'learning_rate': 0.04406012399966528, 'num_leaves': 22, 'max_depth': 1, 'min_child_samples': 12, 'feature_fraction': 0.6654217942993299, 'bagging_fraction': 0.6615392263897545, 'bagging_freq': 7, 'reg_alpha': 0.3061955555153971, 'reg_lambda': 0.2903018096227334, 'min_gain_to_split': 0.07396261226362182}. Best is trial 21 with value: 0.6233457326572641.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5590592429489827, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5590592429489827\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5918842044659156, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5918842044659156\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.08378041045565267, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.08378041045565267\n",
      "[I 2025-06-03 16:36:26,783] Trial 34 finished with value: 0.6605628425951693 and parameters: {'n_estimators': 600, 'learning_rate': 0.062181143095574695, 'num_leaves': 29, 'max_depth': -1, 'min_child_samples': 19, 'feature_fraction': 0.5918842044659156, 'bagging_fraction': 0.5590592429489827, 'bagging_freq': 5, 'reg_alpha': 0.09063759103464109, 'reg_lambda': 0.6998268510961868, 'min_gain_to_split': 0.08378041045565267}. Best is trial 21 with value: 0.6233457326572641.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6988942386607534, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6988942386607534\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7794074624187414, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7794074624187414\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0914881392081003, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0914881392081003\n",
      "[I 2025-06-03 16:36:26,938] Trial 35 finished with value: 0.6652102182001246 and parameters: {'n_estimators': 700, 'learning_rate': 0.041210357636745486, 'num_leaves': 20, 'max_depth': 0, 'min_child_samples': 9, 'feature_fraction': 0.7794074624187414, 'bagging_fraction': 0.6988942386607534, 'bagging_freq': 6, 'reg_alpha': 0.30088043619027505, 'reg_lambda': 0.3504315510172672, 'min_gain_to_split': 0.0914881392081003}. Best is trial 21 with value: 0.6233457326572641.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.602729599959047, subsample=1.0 will be ignored. Current value: bagging_fraction=0.602729599959047\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5568693962513371, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5568693962513371\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.03822555276083543, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.03822555276083543\n",
      "[I 2025-06-03 16:36:27,018] Trial 36 finished with value: 0.6856236585361701 and parameters: {'n_estimators': 900, 'learning_rate': 0.03500129209697786, 'num_leaves': 41, 'max_depth': 2, 'min_child_samples': 14, 'feature_fraction': 0.5568693962513371, 'bagging_fraction': 0.602729599959047, 'bagging_freq': 4, 'reg_alpha': 0.0010138310190098447, 'reg_lambda': 0.07169074377118921, 'min_gain_to_split': 0.03822555276083543}. Best is trial 21 with value: 0.6233457326572641.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7561264314180905, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7561264314180905\n",
      "[LightGBM] [Warning] feature_fraction is set=0.729111158380607, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.729111158380607\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0665788858447828, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0665788858447828\n",
      "[I 2025-06-03 16:36:27,136] Trial 37 finished with value: 0.66544925372283 and parameters: {'n_estimators': 850, 'learning_rate': 0.07528887964829889, 'num_leaves': 27, 'max_depth': -1, 'min_child_samples': 17, 'feature_fraction': 0.729111158380607, 'bagging_fraction': 0.7561264314180905, 'bagging_freq': 5, 'reg_alpha': 0.6052906524591631, 'reg_lambda': 0.24387493056548445, 'min_gain_to_split': 0.0665788858447828}. Best is trial 21 with value: 0.6233457326572641.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6313269260971082, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6313269260971082\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5025812102452154, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5025812102452154\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.07767412831232012, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.07767412831232012\n",
      "[I 2025-06-03 16:36:27,229] Trial 38 finished with value: 0.6765965636474036 and parameters: {'n_estimators': 650, 'learning_rate': 0.04529904041553438, 'num_leaves': 32, 'max_depth': 0, 'min_child_samples': 22, 'feature_fraction': 0.5025812102452154, 'bagging_fraction': 0.6313269260971082, 'bagging_freq': 7, 'reg_alpha': 0.17954481403273453, 'reg_lambda': 0.47186583676209076, 'min_gain_to_split': 0.07767412831232012}. Best is trial 21 with value: 0.6233457326572641.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5457356687527559, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5457356687527559\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6203815772872763, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6203815772872763\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.044516977683612874, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.044516977683612874\n",
      "[I 2025-06-03 16:36:27,340] Trial 39 finished with value: 0.6625891964683894 and parameters: {'n_estimators': 500, 'learning_rate': 0.057059261302615974, 'num_leaves': 15, 'max_depth': -1, 'min_child_samples': 7, 'feature_fraction': 0.6203815772872763, 'bagging_fraction': 0.5457356687527559, 'bagging_freq': 3, 'reg_alpha': 0.0029904449903578006, 'reg_lambda': 0.025640898124872816, 'min_gain_to_split': 0.044516977683612874}. Best is trial 21 with value: 0.6233457326572641.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8099217339046627, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8099217339046627\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5691808067128441, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5691808067128441\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02603980497390994, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02603980497390994\n",
      "[I 2025-06-03 16:36:27,474] Trial 40 finished with value: 0.6693224049673379 and parameters: {'n_estimators': 800, 'learning_rate': 0.01315234406012638, 'num_leaves': 24, 'max_depth': 0, 'min_child_samples': 30, 'feature_fraction': 0.5691808067128441, 'bagging_fraction': 0.8099217339046627, 'bagging_freq': 5, 'reg_alpha': 0.25186679801989914, 'reg_lambda': 0.0506616811084146, 'min_gain_to_split': 0.02603980497390994}. Best is trial 21 with value: 0.6233457326572641.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5863557235708263, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5863557235708263\n",
      "[LightGBM] [Warning] feature_fraction is set=0.533174733917035, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.533174733917035\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.09885513920592574, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.09885513920592574\n",
      "[I 2025-06-03 16:36:27,529] Trial 41 finished with value: 0.6934253911424372 and parameters: {'n_estimators': 650, 'learning_rate': 0.053565336904791955, 'num_leaves': 26, 'max_depth': 1, 'min_child_samples': 14, 'feature_fraction': 0.533174733917035, 'bagging_fraction': 0.5863557235708263, 'bagging_freq': 6, 'reg_alpha': 0.6058884664368636, 'reg_lambda': 0.13829782939516697, 'min_gain_to_split': 0.09885513920592574}. Best is trial 21 with value: 0.6233457326572641.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6064608537253012, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6064608537253012\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5133233323651502, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5133233323651502\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.09870338166960732, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.09870338166960732\n",
      "[I 2025-06-03 16:36:27,690] Trial 42 finished with value: 0.6616601457115294 and parameters: {'n_estimators': 700, 'learning_rate': 0.03708520850594898, 'num_leaves': 21, 'max_depth': 0, 'min_child_samples': 11, 'feature_fraction': 0.5133233323651502, 'bagging_fraction': 0.6064608537253012, 'bagging_freq': 6, 'reg_alpha': 0.45183416231768075, 'reg_lambda': 0.08392981771920396, 'min_gain_to_split': 0.09870338166960732}. Best is trial 21 with value: 0.6233457326572641.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5298482050668573, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5298482050668573\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5296282840013476, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5296282840013476\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.08155955786234084, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.08155955786234084\n",
      "[I 2025-06-03 16:36:27,824] Trial 43 finished with value: 0.6422918334012221 and parameters: {'n_estimators': 700, 'learning_rate': 0.045185864154717356, 'num_leaves': 28, 'max_depth': -1, 'min_child_samples': 18, 'feature_fraction': 0.5296282840013476, 'bagging_fraction': 0.5298482050668573, 'bagging_freq': 6, 'reg_alpha': 0.363110910026895, 'reg_lambda': 0.1803829077044813, 'min_gain_to_split': 0.08155955786234084}. Best is trial 21 with value: 0.6233457326572641.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5258225484481129, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5258225484481129\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5002390586207845, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5002390586207845\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.07962607480740524, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.07962607480740524\n",
      "[I 2025-06-03 16:36:27,932] Trial 44 finished with value: 0.6674728065076706 and parameters: {'n_estimators': 750, 'learning_rate': 0.041596866504315495, 'num_leaves': 29, 'max_depth': -1, 'min_child_samples': 18, 'feature_fraction': 0.5002390586207845, 'bagging_fraction': 0.5258225484481129, 'bagging_freq': 1, 'reg_alpha': 0.3697635644397712, 'reg_lambda': 0.21181615916146534, 'min_gain_to_split': 0.07962607480740524}. Best is trial 21 with value: 0.6233457326572641.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5405686311620601, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5405686311620601\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5455032048225362, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5455032048225362\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.09224712574091284, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.09224712574091284\n",
      "[I 2025-06-03 16:36:28,072] Trial 45 finished with value: 0.6720320100029737 and parameters: {'n_estimators': 850, 'learning_rate': 0.027108442115370262, 'num_leaves': 37, 'max_depth': -1, 'min_child_samples': 21, 'feature_fraction': 0.5455032048225362, 'bagging_fraction': 0.5405686311620601, 'bagging_freq': 7, 'reg_alpha': 0.12795219342296094, 'reg_lambda': 0.393630199124519, 'min_gain_to_split': 0.09224712574091284}. Best is trial 21 with value: 0.6233457326572641.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5003065128292753, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5003065128292753\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6036781665897569, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6036781665897569\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.08740242317392899, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.08740242317392899\n",
      "[I 2025-06-03 16:36:28,162] Trial 46 finished with value: 0.6876821118292109 and parameters: {'n_estimators': 500, 'learning_rate': 0.03427102328612297, 'num_leaves': 18, 'max_depth': 1, 'min_child_samples': 27, 'feature_fraction': 0.6036781665897569, 'bagging_fraction': 0.5003065128292753, 'bagging_freq': 6, 'reg_alpha': 0.1659510320699533, 'reg_lambda': 0.7204997557365346, 'min_gain_to_split': 0.08740242317392899}. Best is trial 21 with value: 0.6233457326572641.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5672939574231547, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5672939574231547\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5722254595060541, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5722254595060541\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.080122785744285, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.080122785744285\n",
      "[I 2025-06-03 16:36:28,249] Trial 47 finished with value: 0.6784271658957559 and parameters: {'n_estimators': 950, 'learning_rate': 0.04680691639530718, 'num_leaves': 28, 'max_depth': 2, 'min_child_samples': 42, 'feature_fraction': 0.5722254595060541, 'bagging_fraction': 0.5672939574231547, 'bagging_freq': 7, 'reg_alpha': 0.7057775637298477, 'reg_lambda': 0.193965720187953, 'min_gain_to_split': 0.080122785744285}. Best is trial 21 with value: 0.6233457326572641.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6430925650742266, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6430925650742266\n",
      "[LightGBM] [Warning] feature_fraction is set=0.664333878849878, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.664333878849878\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0488246382582524, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0488246382582524\n",
      "[I 2025-06-03 16:36:28,431] Trial 48 finished with value: 0.6534195441376374 and parameters: {'n_estimators': 600, 'learning_rate': 0.03917530844009906, 'num_leaves': 35, 'max_depth': 0, 'min_child_samples': 7, 'feature_fraction': 0.664333878849878, 'bagging_fraction': 0.6430925650742266, 'bagging_freq': 5, 'reg_alpha': 0.9742912026718844, 'reg_lambda': 0.0742798755427961, 'min_gain_to_split': 0.0488246382582524}. Best is trial 21 with value: 0.6233457326572641.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5504598189931199, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5504598189931199\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6911100970239697, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6911100970239697\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.06964646387012254, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.06964646387012254\n",
      "[I 2025-06-03 16:36:28,615] Trial 49 finished with value: 0.662910993106512 and parameters: {'n_estimators': 850, 'learning_rate': 0.022137093360237243, 'num_leaves': 32, 'max_depth': -1, 'min_child_samples': 10, 'feature_fraction': 0.6911100970239697, 'bagging_fraction': 0.5504598189931199, 'bagging_freq': 4, 'reg_alpha': 0.3391423138387385, 'reg_lambda': 0.0024271124719204046, 'min_gain_to_split': 0.06964646387012254}. Best is trial 21 with value: 0.6233457326572641.\n",
      "Best Optuna trial for LightGBM: Value=0.6233, Params={'n_estimators': 750, 'learning_rate': 0.040026254173546855, 'num_leaves': 25, 'max_depth': 0, 'min_child_samples': 12, 'feature_fraction': 0.5045147664825391, 'bagging_fraction': 0.5806472136860429, 'bagging_freq': 6, 'reg_alpha': 0.3859867866680439, 'reg_lambda': 0.14598890822246882, 'min_gain_to_split': 0.08355181238769838}\n",
      "\n",
      "--- LightGBM Model Training: AAPL ---\n",
      "Training LightGBM model...\n",
      "Using Optuna-optimized parameters.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5806472136860429, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5806472136860429\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5045147664825391, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5045147664825391\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.08355181238769838, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.08355181238769838\n",
      "\n",
      "Top 10 features:\n",
      "                        Feature  Importance\n",
      "5                       RSI_14           9\n",
      "11                      SMA_10           9\n",
      "32        close_quarterly_mean           8\n",
      "0   close_trans_seq_volatility           7\n",
      "10                      ADX_14           6\n",
      "3         close_entropy_sample           5\n",
      "26                     skew_50           5\n",
      "20                   BBL_20_20           4\n",
      "30            close_weekly_min           4\n",
      "24                     skew_10           4\n",
      "\n",
      "ðŸŽ¯ Accuracy on mapped test data: 0.5893\n",
      "ðŸ“Š AUC: 0.6058\n",
      "\n",
      "Classification Report (on mapped and valid test labels):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.55      0.64        75\n",
      "           1       0.42      0.68      0.52        37\n",
      "\n",
      "    accuracy                           0.59       112\n",
      "   macro avg       0.60      0.61      0.58       112\n",
      "weighted avg       0.66      0.59      0.60       112\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The maximum opset needed by this model is only 9.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance plot saved to feature_importance_AAPL.png\n",
      "\n",
      "Exporting LGBM model to ONNX: lgbm_model_AAPL.onnx (opset=12)\n",
      "Model exported to ONNX: lgbm_model_AAPL.onnx\n",
      "ONNX model check OK.\n",
      "\n",
      "--- Foundation Model Forecasting (PatchTST): AAPL ---\n",
      "\n",
      "ðŸ Workflow completed for AAPL. Final Status: LightGBM Model Trained\n",
      "\n",
      "--- Results Summary for AAPL ---\n",
      "  Overall Status: LightGBM Model Trained\n",
      "  Raw Data Shape: (750, 5)\n",
      "  Featured Data Shape: (750, 105)\n",
      "  Selected Features Count: 35\n",
      "  Top LGBM Features: ['RSI_14', 'SMA_10', 'close_quarterly_mean']\n",
      "  ONNX Model Path: lgbm_model_AAPL.onnx\n",
      "----------------------------------------\n",
      "DEBUG: Starting main loop for symbol: GOOGL\n",
      "\n",
      "========================================\n",
      "ðŸš€ ENHANCED WORKFLOW FOR: GOOGL\n",
      "========================================\n",
      "Fetching data for GOOGL from Twelve Data (interval=1day, from 2022-06-04 to 2025-06-03)...\n",
      "Successfully fetched/processed 750 data points for GOOGL.\n",
      "\n",
      "--- ðŸ”§ Feature Engineering: GOOGL ---\n",
      "Adding technical indicators...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38379/4212630612.py:192: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[3.06651079e+09 4.35166168e+09 5.49533105e+09 5.25738335e+09\n",
      " 3.43865086e+09 3.17167519e+09 4.78883455e+09 4.24405295e+09\n",
      " 4.35871985e+09 4.26272497e+09 4.81415547e+09 4.53681124e+09\n",
      " 5.23540262e+09 4.10838675e+09 4.05583490e+09 6.68380011e+09\n",
      " 3.71943344e+09 4.29421548e+09 3.34559263e+09 2.54813406e+09\n",
      " 2.26042461e+09 3.70164717e+09 2.30950652e+09 2.37180236e+09\n",
      " 1.96404962e+09 3.09653760e+09 2.52002904e+09 2.55685047e+09\n",
      " 2.54479800e+09 2.36472752e+09 3.22347074e+09 2.74643330e+09\n",
      " 2.92220136e+09 2.26916019e+09 2.13695967e+09 3.37931161e+09\n",
      " 2.48702991e+09 2.83936227e+09 2.43983518e+09 2.54650099e+09\n",
      " 2.76519870e+09 4.22385634e+09 4.15080653e+09 4.37850685e+09\n",
      " 3.00166951e+09 2.70625383e+09 4.81684360e+09 3.16852568e+09\n",
      " 4.09870394e+09 2.87297429e+09 1.85102348e+09 4.31310047e+09\n",
      " 2.90208266e+09 3.92662859e+09 2.21159676e+09 2.03844918e+09\n",
      " 2.05264799e+09 2.51952407e+09 3.58487934e+09 2.57100238e+09\n",
      " 2.43665204e+09 2.75096559e+09 2.40859522e+09 2.66476038e+09\n",
      " 3.41994701e+09 6.13669823e+09 3.96490547e+09 2.91573786e+09\n",
      " 3.35410020e+09 2.93270533e+09 3.54363504e+09 7.48222552e+09\n",
      " 5.21043773e+09 4.81973583e+09 2.46943152e+09 2.71890885e+09\n",
      " 2.81100061e+09 2.93993968e+09 3.27961693e+09 3.10182148e+09\n",
      " 3.23083296e+09 3.37393822e+09 4.82057384e+09 6.48392631e+09\n",
      " 6.19561328e+09 4.36880323e+09 4.53512338e+09 4.30226400e+09\n",
      " 2.92141720e+09 3.79275236e+09 2.59383434e+09 2.55873809e+09\n",
      " 5.22706869e+09 2.65627771e+09 2.87618257e+09 2.92498618e+09\n",
      " 2.71924454e+09 2.49004348e+09 5.33449980e+09 3.85226170e+09\n",
      " 2.86392474e+09 2.30929528e+09 2.80060552e+09 2.83574772e+09\n",
      " 3.93525362e+09 7.00202797e+09 9.17607000e+09 4.81453391e+09\n",
      " 5.35458272e+09 4.01438301e+09 4.30996479e+09 5.10952722e+09\n",
      " 4.39745926e+09 5.22981697e+09 4.42265276e+09 3.80309542e+09\n",
      " 3.36661267e+09 4.06273970e+09 3.38270883e+09 2.91407458e+09\n",
      " 3.48498964e+09 2.76377164e+09 4.38170248e+09 3.28116083e+09\n",
      " 4.25821598e+09 3.22766940e+09 2.87051101e+09 3.54698877e+09\n",
      " 3.34318581e+09 2.60260586e+09 3.62491392e+09 5.46074979e+09\n",
      " 4.16600136e+09 3.22266845e+09 3.61685547e+09 6.41282236e+09\n",
      " 7.98927140e+09 5.86520002e+09 4.84089403e+09 3.72231022e+09\n",
      " 3.37538621e+09 2.97339637e+09 3.07879618e+09 2.71978761e+09\n",
      " 3.22409298e+09 4.36139752e+09 3.22897208e+09 2.85170955e+09\n",
      " 3.66895477e+09 2.69589715e+09 5.79194561e+09 3.83424254e+09\n",
      " 4.10695185e+09 3.20306507e+09 2.83897961e+09 2.82027197e+09\n",
      " 3.40771785e+09 3.01671373e+09 2.95712075e+09 2.96722650e+09\n",
      " 2.96995029e+09 3.59675774e+09 2.67085635e+09 3.76740535e+09\n",
      " 2.65375899e+09 3.84483182e+09 3.62587274e+09 3.95910411e+09\n",
      " 3.26712832e+09 6.20812498e+09 3.59445689e+09 3.77694559e+09\n",
      " 3.46652388e+09 3.39854837e+09 2.47215133e+09 3.89582572e+09\n",
      " 3.47789963e+09 3.54179028e+09 2.41844600e+09 4.34786788e+09\n",
      " 3.20714475e+09 3.81418393e+09 3.76927840e+09 3.09432096e+09\n",
      " 2.46651265e+09 3.20947243e+09 3.57262110e+09 3.07858023e+09\n",
      " 7.76176539e+09 3.99029109e+09 6.72945677e+09 4.35891580e+09\n",
      " 3.48352772e+09 6.84407058e+09 3.85101911e+09 3.74907632e+09\n",
      " 2.37939055e+09 3.35442506e+09 2.95453670e+09 3.47121339e+09\n",
      " 3.02881868e+09 3.42476104e+09 3.23651630e+09 3.68757122e+09\n",
      " 4.85313302e+09 4.72508680e+09 3.75684776e+09 4.41569993e+09\n",
      " 3.96729252e+09 4.24840451e+09 5.73288889e+09 5.54387901e+09\n",
      " 4.20077201e+09 3.65604660e+09 3.28935084e+09 3.97631067e+09\n",
      " 3.30383947e+09 3.31020757e+09 3.91463653e+09 2.80756199e+09\n",
      " 5.81042159e+09 5.04345884e+09 5.34410436e+09 4.46452265e+09\n",
      " 3.81211009e+09 3.26759626e+09 6.08737189e+09 1.03217808e+10\n",
      " 3.15651745e+09 3.67100792e+09 4.38928956e+09 3.34384955e+09\n",
      " 3.69345919e+09 4.88485110e+09 3.81058157e+09 3.19314556e+09\n",
      " 4.87799995e+09 4.30632717e+09 3.38873232e+09 3.09889807e+09\n",
      " 4.12543087e+09 3.33849797e+09 3.61329685e+09 1.11291137e+10\n",
      " 5.51599506e+09 4.02673239e+09 5.75684720e+09 3.66176602e+09\n",
      " 4.77983410e+09 2.60295379e+09 4.27487352e+09 4.63216021e+09\n",
      " 4.84892161e+09 4.29220045e+09 3.99791778e+09 3.01171094e+09\n",
      " 3.62098143e+09 4.74914288e+09 4.65491953e+09 3.86926996e+09\n",
      " 4.10480897e+09 3.78561685e+09 4.96279586e+09 3.17919347e+09\n",
      " 3.46861626e+09 3.55207522e+09 1.04880610e+10 3.28619881e+09\n",
      " 4.24235576e+09 3.64290324e+09 3.49006237e+09 3.19868272e+09\n",
      " 1.89789529e+09 3.96467475e+09 2.86832796e+09 3.04139724e+09\n",
      " 3.07344441e+09 4.37500779e+09 6.62758167e+09 3.43607857e+09\n",
      " 3.22289614e+09 4.42368979e+09 4.02544402e+09 4.14316884e+09\n",
      " 4.63853022e+09 2.59109275e+09 3.04161305e+09 5.07068366e+09\n",
      " 3.94721229e+09 3.71824974e+09 3.07305719e+09 2.30562115e+09\n",
      " 2.35641057e+09 3.60418826e+09 2.94452934e+09 4.44293146e+09\n",
      " 4.57461373e+09 4.64631178e+09 2.89733046e+09 3.30241197e+09\n",
      " 3.77976533e+09 4.31846277e+09 6.67129338e+09 3.31265549e+09\n",
      " 3.46258265e+09 3.37006264e+09 4.72991237e+09 3.17945549e+09\n",
      " 2.49794980e+09 3.13505763e+09 3.35948291e+09 2.72760630e+09\n",
      " 3.26700956e+09 5.35021987e+09 7.12866925e+09 1.21831086e+10\n",
      " 3.09574524e+09 5.91861446e+09 4.55524007e+09 3.13740747e+09\n",
      " 4.55360613e+09 3.52699461e+09 4.14446206e+09 5.54342513e+09\n",
      " 3.45897450e+09 3.25430454e+09 4.06111673e+09 3.81839606e+09\n",
      " 5.48353369e+09 3.70907361e+09 3.73195374e+09 4.44554953e+09\n",
      " 1.00961892e+10 1.30324988e+10 6.71803674e+09 8.81143432e+09\n",
      " 8.58217909e+09 4.96280633e+09 2.03211127e+09 2.35603391e+09\n",
      " 3.56414769e+09 5.80372896e+09 5.21877319e+09 3.26294475e+09\n",
      " 4.23739184e+09 5.42664967e+09 5.97637705e+09 4.76841634e+09\n",
      " 4.68228410e+09 3.55639422e+09 4.87045282e+09 6.52909158e+09\n",
      " 9.00868936e+09 5.59013943e+09 4.31712732e+09 3.96345763e+09\n",
      " 3.78942933e+09 3.61220787e+09 7.71611114e+09 5.32279826e+09\n",
      " 4.88360453e+09 4.73865626e+09 4.71246649e+09 5.27324449e+09\n",
      " 5.60703115e+09 5.97147068e+09 5.15825072e+09 4.10569960e+09\n",
      " 4.79304977e+09 3.92445169e+09 1.08465447e+10 5.24309885e+09\n",
      " 4.83659692e+09 4.06554923e+09 4.84249999e+09 7.26134853e+09\n",
      " 9.13541004e+09 4.85425830e+09 4.20757401e+09 3.50233173e+09\n",
      " 6.97113524e+09 6.72829979e+09 7.99564256e+09 5.44202543e+09\n",
      " 7.15771761e+09 1.23647227e+10 1.29353208e+10 6.53614684e+09\n",
      " 6.02389312e+09]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  mfi_series = df_feat.ta.mfi(high=df_feat[h], low=df_feat[l], close=df_feat[c], volume=df_feat[v], append=False)\n",
      "/tmp/ipykernel_38379/4212630612.py:192: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[3.68436805e+09 2.99179836e+09 4.62613301e+09 5.06073645e+09\n",
      " 3.60025021e+09 5.50985898e+09 4.07032589e+09 2.76600463e+09\n",
      " 4.69978839e+09 3.83989859e+09 3.62918666e+09 3.44224921e+09\n",
      " 4.90780907e+09 4.06838630e+09 4.76364524e+09 3.70330267e+09\n",
      " 5.32599270e+09 3.74291684e+09 5.05969108e+09 3.00192835e+09\n",
      " 2.32848907e+09 2.24536567e+09 2.22459520e+09 2.90625533e+09\n",
      " 2.31521951e+09 2.74331545e+09 2.15678519e+09 2.56712355e+09\n",
      " 2.45835418e+09 1.81081265e+09 2.01784279e+09 4.16717307e+09\n",
      " 2.32516910e+09 3.00115076e+09 3.12225147e+09 2.62425470e+09\n",
      " 2.67913277e+09 2.62694695e+09 4.02991300e+09 2.76215923e+09\n",
      " 3.53883093e+09 4.37772559e+09 2.68733130e+09 3.16373895e+09\n",
      " 3.43180359e+09 3.11900237e+09 2.66981943e+09 2.95084338e+09\n",
      " 3.02213445e+09 3.18555286e+09 2.23365737e+09 2.72745268e+09\n",
      " 1.78587755e+09 2.58656282e+09 2.37769705e+09 3.24760113e+09\n",
      " 8.45689760e+09 5.65522034e+09 3.00405699e+09 4.35252559e+09\n",
      " 5.03895355e+09 5.17454584e+09 2.79610439e+09 2.88717217e+09\n",
      " 2.55688901e+09 2.76526374e+09 2.08796389e+09 2.25259615e+09\n",
      " 9.49120300e+08 2.52964486e+09 1.90867014e+09 2.14885763e+09\n",
      " 2.43937110e+09 2.43443968e+09 2.96524964e+09 3.03599615e+09\n",
      " 2.63286531e+09 2.73096053e+09 2.73486405e+09 3.66992422e+09\n",
      " 5.24252637e+09 2.62694151e+09 2.07675579e+09 2.42120758e+09\n",
      " 1.76414101e+09 1.69201065e+09 2.10359851e+09 3.09045896e+09\n",
      " 2.35385662e+09 2.67019799e+09 2.97323024e+09 3.24731634e+09\n",
      " 4.02332540e+09 2.64665890e+09 6.86117736e+09 3.29745211e+09\n",
      " 9.49171967e+09 1.14943583e+10 5.23361283e+09 4.73469526e+09\n",
      " 5.14042257e+09 4.07541881e+09 3.23550480e+09 3.10073354e+09\n",
      " 2.82454769e+09 3.74415086e+09 3.26339092e+09 2.62637221e+09\n",
      " 2.68685888e+09 3.28045487e+09 3.39878412e+09 3.32724251e+09\n",
      " 3.18896635e+09 3.20700874e+09 3.24019880e+09 3.32880258e+09\n",
      " 2.96324125e+09 2.87952024e+09 2.77817526e+09 2.56481161e+09\n",
      " 3.96564016e+09 2.79359720e+09 2.17579557e+09 4.89008659e+09\n",
      " 5.56042035e+09 3.26741485e+09 2.45327069e+09 4.24321562e+09\n",
      " 4.19223441e+09 4.13233451e+09 4.34167700e+09 4.59642341e+09\n",
      " 6.54390746e+09 3.59145820e+09 3.76458649e+09 5.65769363e+09\n",
      " 3.21703881e+09 3.67019393e+09 4.06064861e+09 4.65877867e+09\n",
      " 1.73089133e+09 2.95960840e+09 4.13070123e+09 2.69406525e+09\n",
      " 3.24682832e+09 4.57867607e+09 4.56789571e+09 8.74622664e+09\n",
      " 3.04921414e+09 3.38076063e+09 2.57989359e+09 3.24448995e+09\n",
      " 2.52873196e+09 2.57335431e+09 3.25161609e+09 3.89363737e+09\n",
      " 3.73929194e+09 3.46035367e+09 2.92713050e+09 2.63086633e+09\n",
      " 2.52028999e+09 2.53637990e+09 2.49946690e+09 5.34860792e+09\n",
      " 2.80250091e+09 4.04460199e+09 4.12229075e+09 3.44819139e+09\n",
      " 2.62060383e+09 3.31042542e+09 4.06444286e+09 3.05398837e+09\n",
      " 3.45437441e+09 3.23187286e+09 3.24180091e+09 3.60397205e+09\n",
      " 3.58369176e+09 3.58019939e+09 1.06579820e+10 6.99721249e+09\n",
      " 5.43250780e+09 3.26147219e+09 3.10985370e+09 5.02772912e+09\n",
      " 1.71366657e+09 2.55895503e+09 3.26049789e+09 5.19012250e+09\n",
      " 4.13466959e+09 4.73286175e+09 4.35608924e+09 4.13087077e+09\n",
      " 3.83987803e+09 5.10296695e+09 2.76331432e+09 2.25238839e+09\n",
      " 2.61450439e+09 3.27293597e+09 3.72600630e+09 3.06111600e+09\n",
      " 2.67558905e+09 2.95105080e+09 3.17127517e+09 5.52570358e+09\n",
      " 1.01625611e+10 8.77711930e+09 3.19321193e+09 4.04110612e+09\n",
      " 5.34727720e+09 4.44351624e+09 3.54280451e+09 7.46336262e+09\n",
      " 4.58114990e+09 5.09908669e+09 4.28339171e+09 7.48376000e+09\n",
      " 5.31348473e+09 4.66005381e+09 6.99686100e+09 3.54556985e+09\n",
      " 2.87330621e+09 3.44263315e+09 3.78140867e+09 5.27526068e+09\n",
      " 3.56054179e+09 3.55848219e+09 4.01121654e+09 4.23953549e+09\n",
      " 3.21029401e+09 4.97288113e+09 8.82078372e+09 7.61550170e+09\n",
      " 5.52087352e+09 3.31546215e+09 5.01323718e+09 5.24653760e+09\n",
      " 3.15129040e+09 3.67760542e+09 2.89692824e+09 4.11220116e+09\n",
      " 3.97277337e+09 6.46134447e+09 3.45142776e+09 4.16001237e+09\n",
      " 3.67223613e+09 3.83875069e+09 5.33799033e+09 2.92468800e+09\n",
      " 3.97564485e+09 4.79638109e+09 4.24895091e+09 3.38969343e+09\n",
      " 3.76303177e+09 4.52864119e+09 3.36777562e+09 8.60534634e+09\n",
      " 7.59329409e+09 6.87777113e+09 4.20051156e+09 4.85346296e+09\n",
      " 8.55606275e+09 7.76347208e+09 6.51216561e+09 3.79692273e+09\n",
      " 3.70914517e+09 1.95289907e+09 2.67976606e+09 3.20450080e+09\n",
      " 6.17529672e+09 3.04119168e+09 5.80434251e+09 5.88030682e+09\n",
      " 4.64987483e+09 3.93667736e+09 3.78135560e+09 3.05429457e+09\n",
      " 2.94884441e+09 2.49419610e+09 3.70049622e+09 3.78376898e+09\n",
      " 5.05602503e+09 2.28922258e+09 2.70534945e+09 3.51878652e+09\n",
      " 3.23506002e+09 3.42957049e+09 2.98904456e+09 3.63874404e+09\n",
      " 7.74399254e+09 5.43179610e+09 3.63928728e+09 3.94153050e+09\n",
      " 4.16060064e+09 5.46476201e+09 5.61083581e+09 3.33994341e+09\n",
      " 1.00644499e+10 6.37130306e+09 2.40252825e+09 4.79561984e+09\n",
      " 6.52731952e+09 6.12952748e+09 1.20477969e+10 3.64433991e+09\n",
      " 2.72412981e+09 3.31737187e+09 3.86318635e+09 4.82890984e+09\n",
      " 5.14522805e+09 4.14322933e+09 3.45143683e+09 5.20850461e+09\n",
      " 5.33170562e+09 8.05951975e+09 5.61479961e+09 1.34205373e+10\n",
      " 9.20152147e+09 3.94021051e+09 4.05068998e+09 5.49938772e+09\n",
      " 3.76525616e+09 6.38372694e+09 5.38629476e+09 7.38863400e+09\n",
      " 6.14603143e+09 6.81180033e+09 8.14466013e+09 6.88083707e+09\n",
      " 7.23855484e+09 6.49444484e+09 5.19743129e+09 5.13771727e+09\n",
      " 6.75496889e+09 4.58329277e+09 4.81221272e+09 4.00024481e+09\n",
      " 7.62128808e+09 8.38782546e+09 7.09330214e+09 9.17393768e+09\n",
      " 1.12664627e+10 7.65550749e+09 7.37089678e+09 4.32714675e+09\n",
      " 4.32892684e+09 4.98842592e+09 3.84415260e+09 4.78272803e+09\n",
      " 4.27532068e+09 5.52083369e+09 3.46974393e+09 1.97278181e+10\n",
      " 8.87607394e+09 4.97341270e+09 5.04464728e+09 7.69617399e+09\n",
      " 5.94274547e+09 5.06120365e+09 8.97317317e+09 6.50289001e+09]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  mfi_series = df_feat.ta.mfi(high=df_feat[h], low=df_feat[l], close=df_feat[c], volume=df_feat[v], append=False)\n",
      "/tmp/ipykernel_38379/4212630612.py:312: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  df_new.ta.stc(close=df_new[price_col], tclength=23, fast=50, slow=100, factor=0.5, append=True, col_names=('STC_23_50_05', 'STCD_23_50_05', 'STCK_23_50_05'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Cleaning (Inf/NaN Handling & Imputation): GOOGL ---\n",
      "NaNs before imputation: 1879\n",
      "NaNs after imputation: 0\n",
      "\n",
      "--- ðŸ“Š Regime Detection (Simplified): GOOGL ---\n",
      "Simple Regimes (0:Med,1:Low,2:High):\n",
      "regime_simple\n",
      "0    34.666667\n",
      "1    32.666667\n",
      "2    32.666667\n",
      "Name: proportion, dtype: float64 %\n",
      "\n",
      "--- ðŸŽ¯ Target Definition: GOOGL ---\n",
      "Target distribution:\n",
      "target\n",
      "0    55.333333\n",
      "1    44.666667\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- Causal Feature Discovery and Effect Estimation for GOOGL (Subset Analysis) ---\n",
      "Iteratively estimating causal effects for 11 selected features: ['RSI_14', 'MACDh_12_26_9', 'ADX_14', 'close_cwt_mean', 'close_cwt_std', 'close_entropy_sample', 'regime_simple', 'volatility_20', 'log_returns', 'close_trans_seq_volatility', 'close_trans_seq_autocorr1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/dowhy/causal_estimators/regression_estimator.py:128: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  intercept_parameter = self.model.params[0]\n",
      "[I 2025-06-03 16:36:32,421] A new study created in memory with name: no-name-8fc7b438-93bf-4a22-b992-864636e794ee\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal features ranked by absolute effect strength (top 5 of 11):\n",
      "  close_trans_seq_volatility: 0.2783\n",
      "  close_trans_seq_autocorr1: 0.2377\n",
      "  RSI_14: 0.1078\n",
      "  volatility_20: 0.0659\n",
      "  MACDh_12_26_9: 0.0634\n",
      "\n",
      "--- ML Preparation & Feature Selection: GOOGL ---\n",
      "Train shapes: X_train=(638, 100), y_train=(638,); Test shapes: X_test=(112, 100), y_test=(112,)\n",
      "Selected 35 features: ['close_trans_seq_volatility', 'close_trans_seq_autocorr1', 'RSI_14', 'volatility_20', 'MACDh_12_26_9', 'close_cwt_std', 'close_cwt_mean', 'ADX_14', 'close_entropy_sample', 'log_returns']...\n",
      "Class distribution before SMOTE: \n",
      "target\n",
      "0    0.543887\n",
      "1    0.456113\n",
      "Name: proportion, dtype: float64\n",
      "Class distribution after SMOTE: \n",
      "target\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n",
      "Shape of X_train after SMOTE: (694, 35)\n",
      "\n",
      "--- Hyperparameter Optimization (Optuna for LightGBM): GOOGL ---\n",
      "Optimizing LightGBM HPs with Optuna (50 trials)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc4b05a83d14b1c81f3167fa1448312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.8552710719727215, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8552710719727215\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5865292609891545, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5865292609891545\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.022601228791095107, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.022601228791095107\n",
      "[I 2025-06-03 16:36:32,884] Trial 0 finished with value: 0.5389316759051009 and parameters: {'n_estimators': 550, 'learning_rate': 0.0402528584268907, 'num_leaves': 26, 'max_depth': 7, 'min_child_samples': 6, 'feature_fraction': 0.5865292609891545, 'bagging_fraction': 0.8552710719727215, 'bagging_freq': 6, 'reg_alpha': 0.0017747821361299824, 'reg_lambda': 0.025614252806587353, 'min_gain_to_split': 0.022601228791095107}. Best is trial 0 with value: 0.5389316759051009.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6530802394644688, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6530802394644688\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9753334361941939, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9753334361941939\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.08677302832297547, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.08677302832297547\n",
      "[I 2025-06-03 16:36:33,047] Trial 1 finished with value: 0.5625755063633294 and parameters: {'n_estimators': 950, 'learning_rate': 0.025348086254079954, 'num_leaves': 46, 'max_depth': 9, 'min_child_samples': 33, 'feature_fraction': 0.9753334361941939, 'bagging_fraction': 0.6530802394644688, 'bagging_freq': 3, 'reg_alpha': 0.010366330382332629, 'reg_lambda': 0.06230439324230827, 'min_gain_to_split': 0.08677302832297547}. Best is trial 0 with value: 0.5389316759051009.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6352967010778643, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6352967010778643\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9703666219283384, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9703666219283384\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.05661753770846981, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.05661753770846981\n",
      "[I 2025-06-03 16:36:33,137] Trial 2 finished with value: 0.5982457630002035 and parameters: {'n_estimators': 850, 'learning_rate': 0.05865279068343917, 'num_leaves': 24, 'max_depth': 7, 'min_child_samples': 35, 'feature_fraction': 0.9703666219283384, 'bagging_fraction': 0.6352967010778643, 'bagging_freq': 2, 'reg_alpha': 0.0036161945300405614, 'reg_lambda': 0.04863996105270691, 'min_gain_to_split': 0.05661753770846981}. Best is trial 0 with value: 0.5389316759051009.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9899828916291966, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9899828916291966\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9484280656709505, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9484280656709505\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.09824490067334826, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.09824490067334826\n",
      "[I 2025-06-03 16:36:33,384] Trial 3 finished with value: 0.5318406643292298 and parameters: {'n_estimators': 150, 'learning_rate': 0.06687349422971707, 'num_leaves': 38, 'max_depth': 6, 'min_child_samples': 7, 'feature_fraction': 0.9484280656709505, 'bagging_fraction': 0.9899828916291966, 'bagging_freq': 5, 'reg_alpha': 0.3257287809854301, 'reg_lambda': 0.0059219557962324094, 'min_gain_to_split': 0.09824490067334826}. Best is trial 3 with value: 0.5318406643292298.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7514897824784554, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7514897824784554\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9452423423439481, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9452423423439481\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0018042334104363623, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0018042334104363623\n",
      "[I 2025-06-03 16:36:33,493] Trial 4 finished with value: 0.5731142790724142 and parameters: {'n_estimators': 1000, 'learning_rate': 0.0905988541805624, 'num_leaves': 40, 'max_depth': 6, 'min_child_samples': 29, 'feature_fraction': 0.9452423423439481, 'bagging_fraction': 0.7514897824784554, 'bagging_freq': 6, 'reg_alpha': 0.25167865349280694, 'reg_lambda': 0.003110240170461255, 'min_gain_to_split': 0.0018042334104363623}. Best is trial 3 with value: 0.5318406643292298.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7694794608274248, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7694794608274248\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5504453160050382, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5504453160050382\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.06924108745449056, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.06924108745449056\n",
      "[I 2025-06-03 16:36:33,603] Trial 5 finished with value: 0.5614920355314841 and parameters: {'n_estimators': 850, 'learning_rate': 0.06156463626111424, 'num_leaves': 25, 'max_depth': 2, 'min_child_samples': 13, 'feature_fraction': 0.5504453160050382, 'bagging_fraction': 0.7694794608274248, 'bagging_freq': 5, 'reg_alpha': 0.2520640400923486, 'reg_lambda': 0.15763999012600638, 'min_gain_to_split': 0.06924108745449056}. Best is trial 3 with value: 0.5318406643292298.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8042245704985351, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8042245704985351\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9348078279120139, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9348078279120139\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0896773131754499, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0896773131754499\n",
      "[I 2025-06-03 16:36:33,911] Trial 6 finished with value: 0.608078900496807 and parameters: {'n_estimators': 800, 'learning_rate': 0.010163616295396603, 'num_leaves': 18, 'max_depth': 6, 'min_child_samples': 38, 'feature_fraction': 0.9348078279120139, 'bagging_fraction': 0.8042245704985351, 'bagging_freq': 0, 'reg_alpha': 0.03747220771972983, 'reg_lambda': 0.9388160708995377, 'min_gain_to_split': 0.0896773131754499}. Best is trial 3 with value: 0.5318406643292298.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6030930205272995, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6030930205272995\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6656828127954094, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6656828127954094\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.06151716534850946, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.06151716534850946\n",
      "[I 2025-06-03 16:36:34,039] Trial 7 finished with value: 0.5712360473817236 and parameters: {'n_estimators': 600, 'learning_rate': 0.025415789092942405, 'num_leaves': 49, 'max_depth': -1, 'min_child_samples': 35, 'feature_fraction': 0.6656828127954094, 'bagging_fraction': 0.6030930205272995, 'bagging_freq': 6, 'reg_alpha': 0.0011768954440970405, 'reg_lambda': 0.1600084668701704, 'min_gain_to_split': 0.06151716534850946}. Best is trial 3 with value: 0.5318406643292298.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.519440157465449, subsample=1.0 will be ignored. Current value: bagging_fraction=0.519440157465449\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7083789609289007, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7083789609289007\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04407798197620899, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04407798197620899\n",
      "[I 2025-06-03 16:36:34,122] Trial 8 finished with value: 0.5947441186949924 and parameters: {'n_estimators': 700, 'learning_rate': 0.094139171074546, 'num_leaves': 14, 'max_depth': 10, 'min_child_samples': 39, 'feature_fraction': 0.7083789609289007, 'bagging_fraction': 0.519440157465449, 'bagging_freq': 6, 'reg_alpha': 0.023502318283523078, 'reg_lambda': 0.08669760632188717, 'min_gain_to_split': 0.04407798197620899}. Best is trial 3 with value: 0.5318406643292298.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6337253088893047, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6337253088893047\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7719633806427497, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7719633806427497\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.08909840849218126, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.08909840849218126\n",
      "[I 2025-06-03 16:36:34,197] Trial 9 finished with value: 0.618041211831159 and parameters: {'n_estimators': 100, 'learning_rate': 0.0121228836440229, 'num_leaves': 39, 'max_depth': 3, 'min_child_samples': 47, 'feature_fraction': 0.7719633806427497, 'bagging_fraction': 0.6337253088893047, 'bagging_freq': 0, 'reg_alpha': 0.17401721869901685, 'reg_lambda': 0.9506889106890694, 'min_gain_to_split': 0.08909840849218126}. Best is trial 3 with value: 0.5318406643292298.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9990740473244628, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9990740473244628\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8496002771813924, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8496002771813924\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.09894949860857626, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.09894949860857626\n",
      "[I 2025-06-03 16:36:34,321] Trial 10 finished with value: 0.5580324254608022 and parameters: {'n_estimators': 100, 'learning_rate': 0.04558597938798619, 'num_leaves': 35, 'max_depth': 3, 'min_child_samples': 18, 'feature_fraction': 0.8496002771813924, 'bagging_fraction': 0.9990740473244628, 'bagging_freq': 4, 'reg_alpha': 0.9342534326789123, 'reg_lambda': 0.0012637878144334427, 'min_gain_to_split': 0.09894949860857626}. Best is trial 3 with value: 0.5318406643292298.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9708980778581892, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9708980778581892\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5337820115463129, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5337820115463129\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02642843937523142, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02642843937523142\n",
      "[I 2025-06-03 16:36:34,624] Trial 11 finished with value: 0.5502862033183723 and parameters: {'n_estimators': 350, 'learning_rate': 0.03845709605750262, 'num_leaves': 30, 'max_depth': 8, 'min_child_samples': 5, 'feature_fraction': 0.5337820115463129, 'bagging_fraction': 0.9708980778581892, 'bagging_freq': 7, 'reg_alpha': 0.06364513750108015, 'reg_lambda': 0.010680216152110733, 'min_gain_to_split': 0.02642843937523142}. Best is trial 3 with value: 0.5318406643292298.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8876006330702151, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8876006330702151\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6167804015475757, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6167804015475757\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.030042460533827215, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.030042460533827215\n",
      "[I 2025-06-03 16:36:35,080] Trial 12 finished with value: 0.5133645661956389 and parameters: {'n_estimators': 350, 'learning_rate': 0.01822332320170113, 'num_leaves': 31, 'max_depth': 5, 'min_child_samples': 7, 'feature_fraction': 0.6167804015475757, 'bagging_fraction': 0.8876006330702151, 'bagging_freq': 4, 'reg_alpha': 0.0013716760770818388, 'reg_lambda': 0.01212937382629486, 'min_gain_to_split': 0.030042460533827215}. Best is trial 12 with value: 0.5133645661956389.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9086351355134443, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9086351355134443\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8066978370087574, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8066978370087574\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.039533921038427434, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.039533921038427434\n",
      "[I 2025-06-03 16:36:35,480] Trial 13 finished with value: 0.5384974869621765 and parameters: {'n_estimators': 300, 'learning_rate': 0.01666114025826085, 'num_leaves': 34, 'max_depth': 5, 'min_child_samples': 19, 'feature_fraction': 0.8066978370087574, 'bagging_fraction': 0.9086351355134443, 'bagging_freq': 4, 'reg_alpha': 0.9807302419472472, 'reg_lambda': 0.008792394230544155, 'min_gain_to_split': 0.039533921038427434}. Best is trial 12 with value: 0.5133645661956389.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9086685070773438, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9086685070773438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6359225729489858, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6359225729489858\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.01603074347914333, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.01603074347914333\n",
      "[I 2025-06-03 16:36:35,871] Trial 14 finished with value: 0.5355981473037495 and parameters: {'n_estimators': 350, 'learning_rate': 0.01908330684112922, 'num_leaves': 41, 'max_depth': 4, 'min_child_samples': 12, 'feature_fraction': 0.6359225729489858, 'bagging_fraction': 0.9086685070773438, 'bagging_freq': 2, 'reg_alpha': 0.01160188687318829, 'reg_lambda': 0.0072876125566809285, 'min_gain_to_split': 0.01603074347914333}. Best is trial 12 with value: 0.5133645661956389.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9374117287618973, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9374117287618973\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8510231640922744, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8510231640922744\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.035220036553649664, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.035220036553649664\n",
      "[I 2025-06-03 16:36:36,002] Trial 15 finished with value: 0.6462670116482215 and parameters: {'n_estimators': 250, 'learning_rate': 0.016772538294189294, 'num_leaves': 32, 'max_depth': 1, 'min_child_samples': 21, 'feature_fraction': 0.8510231640922744, 'bagging_fraction': 0.9374117287618973, 'bagging_freq': 3, 'reg_alpha': 0.003735858515385836, 'reg_lambda': 0.00240965485336627, 'min_gain_to_split': 0.035220036553649664}. Best is trial 12 with value: 0.5133645661956389.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8691658508337513, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8691658508337513\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7165178104508212, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7165178104508212\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.06975020013792022, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.06975020013792022\n",
      "[I 2025-06-03 16:36:36,299] Trial 16 finished with value: 0.5261377008343517 and parameters: {'n_estimators': 450, 'learning_rate': 0.028521600362435826, 'num_leaves': 45, 'max_depth': 5, 'min_child_samples': 10, 'feature_fraction': 0.7165178104508212, 'bagging_fraction': 0.8691658508337513, 'bagging_freq': 5, 'reg_alpha': 0.08828907871678014, 'reg_lambda': 0.0208290631491237, 'min_gain_to_split': 0.06975020013792022}. Best is trial 12 with value: 0.5133645661956389.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8343166812322855, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8343166812322855\n",
      "[LightGBM] [Warning] feature_fraction is set=0.709682954114358, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.709682954114358\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.07757813304840819, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.07757813304840819\n",
      "[I 2025-06-03 16:36:36,533] Trial 17 finished with value: 0.5379238127396307 and parameters: {'n_estimators': 450, 'learning_rate': 0.02805294774698303, 'num_leaves': 47, 'max_depth': 4, 'min_child_samples': 13, 'feature_fraction': 0.709682954114358, 'bagging_fraction': 0.8343166812322855, 'bagging_freq': 2, 'reg_alpha': 0.08532596542929687, 'reg_lambda': 0.020495129192256368, 'min_gain_to_split': 0.07757813304840819}. Best is trial 12 with value: 0.5133645661956389.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8694190766582298, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8694190766582298\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6118223925148264, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6118223925148264\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04997590604253095, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04997590604253095\n",
      "[I 2025-06-03 16:36:36,809] Trial 18 finished with value: 0.5700474995532236 and parameters: {'n_estimators': 500, 'learning_rate': 0.021393625858700867, 'num_leaves': 44, 'max_depth': 0, 'min_child_samples': 25, 'feature_fraction': 0.6118223925148264, 'bagging_fraction': 0.8694190766582298, 'bagging_freq': 5, 'reg_alpha': 0.01447931165423627, 'reg_lambda': 0.016716495604797342, 'min_gain_to_split': 0.04997590604253095}. Best is trial 12 with value: 0.5133645661956389.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7118355988123646, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7118355988123646\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6827450910151815, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6827450910151815\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.06858343823812271, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.06858343823812271\n",
      "[I 2025-06-03 16:36:37,283] Trial 19 finished with value: 0.535695922634223 and parameters: {'n_estimators': 450, 'learning_rate': 0.013431455164093414, 'num_leaves': 12, 'max_depth': 5, 'min_child_samples': 12, 'feature_fraction': 0.6827450910151815, 'bagging_fraction': 0.7118355988123646, 'bagging_freq': 4, 'reg_alpha': 0.004802873025827746, 'reg_lambda': 0.0035066389808508726, 'min_gain_to_split': 0.06858343823812271}. Best is trial 12 with value: 0.5133645661956389.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.810776289191988, subsample=1.0 will be ignored. Current value: bagging_fraction=0.810776289191988\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7434539630849142, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7434539630849142\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.007026121849100075, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.007026121849100075\n",
      "[I 2025-06-03 16:36:37,423] Trial 20 finished with value: 0.5946884456319859 and parameters: {'n_estimators': 200, 'learning_rate': 0.030099532561174293, 'num_leaves': 20, 'max_depth': 2, 'min_child_samples': 24, 'feature_fraction': 0.7434539630849142, 'bagging_fraction': 0.810776289191988, 'bagging_freq': 7, 'reg_alpha': 0.1013562857244296, 'reg_lambda': 0.33748327213294144, 'min_gain_to_split': 0.007026121849100075}. Best is trial 12 with value: 0.5133645661956389.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.943382597785876, subsample=1.0 will be ignored. Current value: bagging_fraction=0.943382597785876\n",
      "[LightGBM] [Warning] feature_fraction is set=0.874021780888906, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.874021780888906\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.09779397853104711, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.09779397853104711\n",
      "[I 2025-06-03 16:36:37,674] Trial 21 finished with value: 0.5258273631119842 and parameters: {'n_estimators': 200, 'learning_rate': 0.05550149626662463, 'num_leaves': 38, 'max_depth': 6, 'min_child_samples': 8, 'feature_fraction': 0.874021780888906, 'bagging_fraction': 0.943382597785876, 'bagging_freq': 5, 'reg_alpha': 0.4543957343413572, 'reg_lambda': 0.005490275104890369, 'min_gain_to_split': 0.09779397853104711}. Best is trial 12 with value: 0.5133645661956389.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9063999354590612, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9063999354590612\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8788229449920982, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8788229449920982\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.07333918703073655, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.07333918703073655\n",
      "[I 2025-06-03 16:36:38,060] Trial 22 finished with value: 0.5323153215942645 and parameters: {'n_estimators': 400, 'learning_rate': 0.03491740932115693, 'num_leaves': 43, 'max_depth': 8, 'min_child_samples': 9, 'feature_fraction': 0.8788229449920982, 'bagging_fraction': 0.9063999354590612, 'bagging_freq': 5, 'reg_alpha': 0.5629335191135351, 'reg_lambda': 0.031050836681289957, 'min_gain_to_split': 0.07333918703073655}. Best is trial 12 with value: 0.5133645661956389.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9454253338893214, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9454253338893214\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7829616555505085, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7829616555505085\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.08063174625717144, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.08063174625717144\n",
      "[I 2025-06-03 16:36:38,294] Trial 23 finished with value: 0.5494988019840539 and parameters: {'n_estimators': 250, 'learning_rate': 0.050603616695176504, 'num_leaves': 36, 'max_depth': 5, 'min_child_samples': 16, 'feature_fraction': 0.7829616555505085, 'bagging_fraction': 0.9454253338893214, 'bagging_freq': 3, 'reg_alpha': 0.04243833334051194, 'reg_lambda': 0.012596126757445694, 'min_gain_to_split': 0.08063174625717144}. Best is trial 12 with value: 0.5133645661956389.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8697167203515686, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8697167203515686\n",
      "[LightGBM] [Warning] feature_fraction is set=0.501576267995721, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.501576267995721\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.057282817304179945, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.057282817304179945\n",
      "[I 2025-06-03 16:36:38,543] Trial 24 finished with value: 0.504129786130358 and parameters: {'n_estimators': 650, 'learning_rate': 0.07527346649654311, 'num_leaves': 28, 'max_depth': 7, 'min_child_samples': 9, 'feature_fraction': 0.501576267995721, 'bagging_fraction': 0.8697167203515686, 'bagging_freq': 4, 'reg_alpha': 0.1522854220887285, 'reg_lambda': 0.001378126641529411, 'min_gain_to_split': 0.057282817304179945}. Best is trial 24 with value: 0.504129786130358.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9433870140183924, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9433870140183924\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5030486652798054, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5030486652798054\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.03221583278912671, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.03221583278912671\n",
      "[I 2025-06-03 16:36:38,907] Trial 25 finished with value: 0.48195526032685715 and parameters: {'n_estimators': 650, 'learning_rate': 0.07348303462052715, 'num_leaves': 29, 'max_depth': 7, 'min_child_samples': 16, 'feature_fraction': 0.5030486652798054, 'bagging_fraction': 0.9433870140183924, 'bagging_freq': 4, 'reg_alpha': 0.5770630740525436, 'reg_lambda': 0.0010359243044904588, 'min_gain_to_split': 0.03221583278912671}. Best is trial 25 with value: 0.48195526032685715.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7132310114428849, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7132310114428849\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5120700588376296, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5120700588376296\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.031837690002745715, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.031837690002745715\n",
      "[I 2025-06-03 16:36:39,151] Trial 26 finished with value: 0.5005389577408056 and parameters: {'n_estimators': 650, 'learning_rate': 0.07424036693933149, 'num_leaves': 29, 'max_depth': 8, 'min_child_samples': 15, 'feature_fraction': 0.5120700588376296, 'bagging_fraction': 0.7132310114428849, 'bagging_freq': 1, 'reg_alpha': 0.16574200855214677, 'reg_lambda': 0.0010961107864428557, 'min_gain_to_split': 0.031837690002745715}. Best is trial 25 with value: 0.48195526032685715.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7294184402860954, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7294184402860954\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5047280920531082, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5047280920531082\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04889417160850156, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04889417160850156\n",
      "[I 2025-06-03 16:36:39,377] Trial 27 finished with value: 0.49479249371965855 and parameters: {'n_estimators': 650, 'learning_rate': 0.07701332462101554, 'num_leaves': 27, 'max_depth': 10, 'min_child_samples': 15, 'feature_fraction': 0.5047280920531082, 'bagging_fraction': 0.7294184402860954, 'bagging_freq': 1, 'reg_alpha': 0.1426939032715389, 'reg_lambda': 0.001064531304704277, 'min_gain_to_split': 0.04889417160850156}. Best is trial 25 with value: 0.48195526032685715.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6991651735728127, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6991651735728127\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5011666999233614, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5011666999233614\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04648432796766418, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04648432796766418\n",
      "[I 2025-06-03 16:36:39,703] Trial 28 finished with value: 0.4868053124701561 and parameters: {'n_estimators': 750, 'learning_rate': 0.07880926967669788, 'num_leaves': 20, 'max_depth': 10, 'min_child_samples': 16, 'feature_fraction': 0.5011666999233614, 'bagging_fraction': 0.6991651735728127, 'bagging_freq': 1, 'reg_alpha': 0.5242787749791654, 'reg_lambda': 0.0010303435001416989, 'min_gain_to_split': 0.04648432796766418}. Best is trial 25 with value: 0.48195526032685715.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6821123799405907, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6821123799405907\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5748632859414937, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5748632859414937\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.01848507637994175, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.01848507637994175\n",
      "[I 2025-06-03 16:36:39,865] Trial 29 finished with value: 0.5469545103663079 and parameters: {'n_estimators': 750, 'learning_rate': 0.0822337812065093, 'num_leaves': 22, 'max_depth': 10, 'min_child_samples': 21, 'feature_fraction': 0.5748632859414937, 'bagging_fraction': 0.6821123799405907, 'bagging_freq': 1, 'reg_alpha': 0.5095365290748961, 'reg_lambda': 0.0018892882592685472, 'min_gain_to_split': 0.01848507637994175}. Best is trial 25 with value: 0.48195526032685715.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5727742538591762, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5727742538591762\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5706017983705803, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5706017983705803\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.043816762353842714, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.043816762353842714\n",
      "[I 2025-06-03 16:36:40,010] Trial 30 finished with value: 0.5808209896717743 and parameters: {'n_estimators': 550, 'learning_rate': 0.09917441182847206, 'num_leaves': 17, 'max_depth': 9, 'min_child_samples': 25, 'feature_fraction': 0.5706017983705803, 'bagging_fraction': 0.5727742538591762, 'bagging_freq': 1, 'reg_alpha': 0.6724326289519736, 'reg_lambda': 0.003912752869994213, 'min_gain_to_split': 0.043816762353842714}. Best is trial 25 with value: 0.48195526032685715.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7190925116894183, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7190925116894183\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5031784343159359, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5031784343159359\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.03454571086511619, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.03454571086511619\n",
      "[I 2025-06-03 16:36:40,225] Trial 31 finished with value: 0.5106489361900999 and parameters: {'n_estimators': 650, 'learning_rate': 0.07458358502732346, 'num_leaves': 28, 'max_depth': 9, 'min_child_samples': 16, 'feature_fraction': 0.5031784343159359, 'bagging_fraction': 0.7190925116894183, 'bagging_freq': 1, 'reg_alpha': 0.15170172050690917, 'reg_lambda': 0.0010833698215961143, 'min_gain_to_split': 0.03454571086511619}. Best is trial 25 with value: 0.48195526032685715.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7077392218468588, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7077392218468588\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5329558134432274, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5329558134432274\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04751053659610885, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04751053659610885\n",
      "[I 2025-06-03 16:36:40,412] Trial 32 finished with value: 0.5284350746026026 and parameters: {'n_estimators': 750, 'learning_rate': 0.0675983662483975, 'num_leaves': 27, 'max_depth': 8, 'min_child_samples': 15, 'feature_fraction': 0.5329558134432274, 'bagging_fraction': 0.7077392218468588, 'bagging_freq': 1, 'reg_alpha': 0.3723372991018557, 'reg_lambda': 0.0018204133723322333, 'min_gain_to_split': 0.04751053659610885}. Best is trial 25 with value: 0.48195526032685715.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6747268478676524, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6747268478676524\n",
      "[LightGBM] [Warning] feature_fraction is set=0.52200713651277, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.52200713651277\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.022761587399076307, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.022761587399076307\n",
      "[I 2025-06-03 16:36:40,655] Trial 33 finished with value: 0.5503969266904097 and parameters: {'n_estimators': 600, 'learning_rate': 0.04759084222934757, 'num_leaves': 23, 'max_depth': 10, 'min_child_samples': 29, 'feature_fraction': 0.52200713651277, 'bagging_fraction': 0.6747268478676524, 'bagging_freq': 2, 'reg_alpha': 0.3031940056564644, 'reg_lambda': 0.001073869834921656, 'min_gain_to_split': 0.022761587399076307}. Best is trial 25 with value: 0.48195526032685715.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7626952289295339, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7626952289295339\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5596276393192308, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5596276393192308\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.05444721345104421, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.05444721345104421\n",
      "[I 2025-06-03 16:36:40,841] Trial 34 finished with value: 0.52335282370627 and parameters: {'n_estimators': 900, 'learning_rate': 0.08194219794377376, 'num_leaves': 20, 'max_depth': 9, 'min_child_samples': 21, 'feature_fraction': 0.5596276393192308, 'bagging_fraction': 0.7626952289295339, 'bagging_freq': 0, 'reg_alpha': 0.18349519169327905, 'reg_lambda': 0.0018873522774049854, 'min_gain_to_split': 0.05444721345104421}. Best is trial 25 with value: 0.48195526032685715.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7886563614759772, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7886563614759772\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5966816413502363, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5966816413502363\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.033476872773321065, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.033476872773321065\n",
      "[I 2025-06-03 16:36:41,071] Trial 35 finished with value: 0.5096948984848647 and parameters: {'n_estimators': 700, 'learning_rate': 0.06401614504217686, 'num_leaves': 26, 'max_depth': 7, 'min_child_samples': 19, 'feature_fraction': 0.5966816413502363, 'bagging_fraction': 0.7886563614759772, 'bagging_freq': 3, 'reg_alpha': 0.7134235377430822, 'reg_lambda': 0.002716042122053181, 'min_gain_to_split': 0.033476872773321065}. Best is trial 25 with value: 0.48195526032685715.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.733246176651626, subsample=1.0 will be ignored. Current value: bagging_fraction=0.733246176651626\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5005682323962471, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5005682323962471\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.039710315075228195, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.039710315075228195\n",
      "[I 2025-06-03 16:36:41,268] Trial 36 finished with value: 0.5567031415805922 and parameters: {'n_estimators': 800, 'learning_rate': 0.052743099370687434, 'num_leaves': 33, 'max_depth': 8, 'min_child_samples': 17, 'feature_fraction': 0.5005682323962471, 'bagging_fraction': 0.733246176651626, 'bagging_freq': 1, 'reg_alpha': 0.23244478972045607, 'reg_lambda': 0.005231039052291438, 'min_gain_to_split': 0.039710315075228195}. Best is trial 25 with value: 0.48195526032685715.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6777952334077411, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6777952334077411\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5414311603035118, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5414311603035118\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.010577484326578655, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.010577484326578655\n",
      "[I 2025-06-03 16:36:41,545] Trial 37 finished with value: 0.4797090176862695 and parameters: {'n_estimators': 600, 'learning_rate': 0.07219297529891865, 'num_leaves': 24, 'max_depth': 10, 'min_child_samples': 23, 'feature_fraction': 0.5414311603035118, 'bagging_fraction': 0.6777952334077411, 'bagging_freq': 0, 'reg_alpha': 0.38329758195435754, 'reg_lambda': 0.0010057843402142975, 'min_gain_to_split': 0.010577484326578655}. Best is trial 37 with value: 0.4797090176862695.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6645725136646125, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6645725136646125\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5611526695138518, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5611526695138518\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0133376051700605, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0133376051700605\n",
      "[I 2025-06-03 16:36:41,924] Trial 38 finished with value: 0.514909457392098 and parameters: {'n_estimators': 550, 'learning_rate': 0.04217474249781986, 'num_leaves': 24, 'max_depth': 10, 'min_child_samples': 23, 'feature_fraction': 0.5611526695138518, 'bagging_fraction': 0.6645725136646125, 'bagging_freq': 0, 'reg_alpha': 0.3798767086487626, 'reg_lambda': 0.002078801141663934, 'min_gain_to_split': 0.0133376051700605}. Best is trial 37 with value: 0.4797090176862695.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.618459649242017, subsample=1.0 will be ignored. Current value: bagging_fraction=0.618459649242017\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5415630479763968, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5415630479763968\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.005838741882649753, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.005838741882649753\n",
      "[I 2025-06-03 16:36:42,059] Trial 39 finished with value: 0.5711052158652429 and parameters: {'n_estimators': 950, 'learning_rate': 0.061053168467466064, 'num_leaves': 15, 'max_depth': 9, 'min_child_samples': 28, 'feature_fraction': 0.5415630479763968, 'bagging_fraction': 0.618459649242017, 'bagging_freq': 2, 'reg_alpha': 0.6982981899941737, 'reg_lambda': 0.0015342339033761793, 'min_gain_to_split': 0.005838741882649753}. Best is trial 37 with value: 0.4797090176862695.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5686964991264073, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5686964991264073\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6548364821002829, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6548364821002829\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.05975739996935142, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.05975739996935142\n",
      "[I 2025-06-03 16:36:42,206] Trial 40 finished with value: 0.5489883585712517 and parameters: {'n_estimators': 850, 'learning_rate': 0.08596399374198449, 'num_leaves': 21, 'max_depth': 10, 'min_child_samples': 32, 'feature_fraction': 0.6548364821002829, 'bagging_fraction': 0.5686964991264073, 'bagging_freq': 0, 'reg_alpha': 0.2788140829684349, 'reg_lambda': 0.003434376932066976, 'min_gain_to_split': 0.05975739996935142}. Best is trial 37 with value: 0.4797090176862695.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6936209017495535, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6936209017495535\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5886746326716056, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5886746326716056\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.025801589908857673, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.025801589908857673\n",
      "[I 2025-06-03 16:36:42,475] Trial 41 finished with value: 0.5083213212618497 and parameters: {'n_estimators': 650, 'learning_rate': 0.06991119158028952, 'num_leaves': 25, 'max_depth': 8, 'min_child_samples': 14, 'feature_fraction': 0.5886746326716056, 'bagging_fraction': 0.6936209017495535, 'bagging_freq': 1, 'reg_alpha': 0.11427599346658301, 'reg_lambda': 0.0010338032604658434, 'min_gain_to_split': 0.025801589908857673}. Best is trial 37 with value: 0.4797090176862695.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6516782745825657, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6516782745825657\n",
      "[LightGBM] [Warning] feature_fraction is set=0.526402094310128, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.526402094310128\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.053865735805210126, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.053865735805210126\n",
      "[I 2025-06-03 16:36:42,707] Trial 42 finished with value: 0.508900008562907 and parameters: {'n_estimators': 600, 'learning_rate': 0.07581696007133185, 'num_leaves': 29, 'max_depth': 9, 'min_child_samples': 19, 'feature_fraction': 0.526402094310128, 'bagging_fraction': 0.6516782745825657, 'bagging_freq': 0, 'reg_alpha': 0.06176318471853003, 'reg_lambda': 0.0010044187899442712, 'min_gain_to_split': 0.053865735805210126}. Best is trial 37 with value: 0.4797090176862695.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7445402410632886, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7445402410632886\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5202975170454693, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5202975170454693\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.041088165028031906, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.041088165028031906\n",
      "[I 2025-06-03 16:36:42,884] Trial 43 finished with value: 0.49108685467494023 and parameters: {'n_estimators': 700, 'learning_rate': 0.08741479024097285, 'num_leaves': 19, 'max_depth': 10, 'min_child_samples': 14, 'feature_fraction': 0.5202975170454693, 'bagging_fraction': 0.7445402410632886, 'bagging_freq': 2, 'reg_alpha': 0.20117207902486567, 'reg_lambda': 0.0014895340550839197, 'min_gain_to_split': 0.041088165028031906}. Best is trial 37 with value: 0.4797090176862695.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7435069474277469, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7435069474277469\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5535635490420885, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5535635490420885\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04530514998241662, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04530514998241662\n",
      "[I 2025-06-03 16:36:43,093] Trial 44 finished with value: 0.4860185974743539 and parameters: {'n_estimators': 700, 'learning_rate': 0.09220232497707687, 'num_leaves': 18, 'max_depth': 10, 'min_child_samples': 11, 'feature_fraction': 0.5535635490420885, 'bagging_fraction': 0.7435069474277469, 'bagging_freq': 2, 'reg_alpha': 0.487493398694511, 'reg_lambda': 0.0014986110158500799, 'min_gain_to_split': 0.04530514998241662}. Best is trial 37 with value: 0.4797090176862695.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784566520089741, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784566520089741\n",
      "[LightGBM] [Warning] feature_fraction is set=0.543896251745802, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.543896251745802\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.03944378703683038, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.03944378703683038\n",
      "[I 2025-06-03 16:36:43,231] Trial 45 finished with value: 0.5344903488310923 and parameters: {'n_estimators': 750, 'learning_rate': 0.09961075198755705, 'num_leaves': 10, 'max_depth': 9, 'min_child_samples': 22, 'feature_fraction': 0.543896251745802, 'bagging_fraction': 0.784566520089741, 'bagging_freq': 2, 'reg_alpha': 0.4155148596792489, 'reg_lambda': 0.04812046808818747, 'min_gain_to_split': 0.03944378703683038}. Best is trial 37 with value: 0.4797090176862695.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7510533698891141, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7510533698891141\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6078867707289987, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6078867707289987\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0005986619001371457, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0005986619001371457\n",
      "[I 2025-06-03 16:36:43,413] Trial 46 finished with value: 0.49634200414811774 and parameters: {'n_estimators': 800, 'learning_rate': 0.09026092180950386, 'num_leaves': 18, 'max_depth': 10, 'min_child_samples': 11, 'feature_fraction': 0.6078867707289987, 'bagging_fraction': 0.7510533698891141, 'bagging_freq': 3, 'reg_alpha': 0.22724065854992523, 'reg_lambda': 0.002792130065587362, 'min_gain_to_split': 0.0005986619001371457}. Best is trial 37 with value: 0.4797090176862695.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8379315433200896, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8379315433200896\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6268428828765981, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6268428828765981\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04343682519906161, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04343682519906161\n",
      "[I 2025-06-03 16:36:43,541] Trial 47 finished with value: 0.5884495554851626 and parameters: {'n_estimators': 700, 'learning_rate': 0.059555865284067214, 'num_leaves': 16, 'max_depth': 7, 'min_child_samples': 45, 'feature_fraction': 0.6268428828765981, 'bagging_fraction': 0.8379315433200896, 'bagging_freq': 2, 'reg_alpha': 0.49990950307291515, 'reg_lambda': 0.0015379270671289711, 'min_gain_to_split': 0.04343682519906161}. Best is trial 37 with value: 0.4797090176862695.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6460138400087807, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6460138400087807\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5805377101693661, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5805377101693661\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.06239944247583189, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.06239944247583189\n",
      "[I 2025-06-03 16:36:43,756] Trial 48 finished with value: 0.5241023383526083 and parameters: {'n_estimators': 850, 'learning_rate': 0.0892933318538564, 'num_leaves': 18, 'max_depth': 9, 'min_child_samples': 7, 'feature_fraction': 0.5805377101693661, 'bagging_fraction': 0.6460138400087807, 'bagging_freq': 3, 'reg_alpha': 0.9528749792506597, 'reg_lambda': 0.0041020757300239945, 'min_gain_to_split': 0.06239944247583189}. Best is trial 37 with value: 0.4797090176862695.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7430180844944033, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7430180844944033\n",
      "[LightGBM] [Warning] feature_fraction is set=0.55472819239226, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.55472819239226\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.026952942768020877, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.026952942768020877\n",
      "[I 2025-06-03 16:36:43,913] Trial 49 finished with value: 0.534587275915728 and parameters: {'n_estimators': 700, 'learning_rate': 0.08183889775436635, 'num_leaves': 13, 'max_depth': 10, 'min_child_samples': 5, 'feature_fraction': 0.55472819239226, 'bagging_fraction': 0.7430180844944033, 'bagging_freq': 0, 'reg_alpha': 0.6865279938304661, 'reg_lambda': 0.0023651713598002574, 'min_gain_to_split': 0.026952942768020877}. Best is trial 37 with value: 0.4797090176862695.\n",
      "Best Optuna trial for LightGBM: Value=0.4797, Params={'n_estimators': 600, 'learning_rate': 0.07219297529891865, 'num_leaves': 24, 'max_depth': 10, 'min_child_samples': 23, 'feature_fraction': 0.5414311603035118, 'bagging_fraction': 0.6777952334077411, 'bagging_freq': 0, 'reg_alpha': 0.38329758195435754, 'reg_lambda': 0.0010057843402142975, 'min_gain_to_split': 0.010577484326578655}\n",
      "\n",
      "--- LightGBM Model Training: GOOGL ---\n",
      "Training LightGBM model...\n",
      "Using Optuna-optimized parameters.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6777952334077411, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6777952334077411\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5414311603035118, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5414311603035118\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.010577484326578655, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.010577484326578655\n",
      "\n",
      "Top 10 features:\n",
      "                        Feature  Importance\n",
      "24                 kurtosis_20          21\n",
      "5                close_cwt_std          11\n",
      "7                       ADX_14          11\n",
      "3                volatility_20          10\n",
      "32           sma_10_regime_adj           9\n",
      "6               close_cwt_mean           9\n",
      "27        close_cwt_energy_s32           7\n",
      "23                   BBU_20_20           7\n",
      "0   close_trans_seq_volatility           6\n",
      "13                      SMA_20           6\n",
      "\n",
      "ðŸŽ¯ Accuracy on mapped test data: 0.5893\n",
      "ðŸ“Š AUC: 0.6255\n",
      "\n",
      "Classification Report (on mapped and valid test labels):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.72      0.68        68\n",
      "           1       0.47      0.39      0.42        44\n",
      "\n",
      "    accuracy                           0.59       112\n",
      "   macro avg       0.56      0.55      0.55       112\n",
      "weighted avg       0.58      0.59      0.58       112\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The maximum opset needed by this model is only 9.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance plot saved to feature_importance_GOOGL.png\n",
      "\n",
      "Exporting LGBM model to ONNX: lgbm_model_GOOGL.onnx (opset=12)\n",
      "Model exported to ONNX: lgbm_model_GOOGL.onnx\n",
      "ONNX model check OK.\n",
      "\n",
      "--- Foundation Model Forecasting (PatchTST): GOOGL ---\n",
      "\n",
      "ðŸ Workflow completed for GOOGL. Final Status: LightGBM Model Trained\n",
      "\n",
      "--- Results Summary for GOOGL ---\n",
      "  Overall Status: LightGBM Model Trained\n",
      "  Raw Data Shape: (750, 5)\n",
      "  Featured Data Shape: (750, 105)\n",
      "  Selected Features Count: 35\n",
      "  Top LGBM Features: ['kurtosis_20', 'close_cwt_std', 'ADX_14']\n",
      "  ONNX Model Path: lgbm_model_GOOGL.onnx\n",
      "----------------------------------------\n",
      "\n",
      "Total execution time for 2 symbol(s): 24.49 seconds.\n",
      "DEBUG: Script __main__ block finished.\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_ta\n",
    "# import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import pywt\n",
    "import antropy as ant\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss, roc_auc_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "# import gc # Unused\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Failed to load image Python extension.*_ZN3c1017RegisterOperatorsD1Ev\")\n",
    "\n",
    "# Foundational Model Imports\n",
    "from transformers import PatchTSTConfig, PatchTSTForPrediction\n",
    "\n",
    "# --- Global Flags and Initializations for Optional Libraries ---\n",
    "optuna_available = False\n",
    "torch_available = False\n",
    "dowhy_available = False\n",
    "CausalModel = None\n",
    "nx = None\n",
    "# ONNX related flags and types\n",
    "onnx_available = False\n",
    "skl2onnx_available = False\n",
    "onnxmltools_available = False\n",
    "FloatTensorType = None\n",
    "SKIP_CAUSAL_ANALYSIS_FOR_DEBUGGING = False\n",
    "\n",
    "# Optional: Filter specific torchvision warning if it's problematic and not needed\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", message=\"Failed to load image Python extension.*_ZN3c1017RegisterOperatorsD1Ev\")\n",
    "\n",
    "if hasattr(pd.DataFrame, 'ta') is False and pandas_ta is not None:\n",
    "    try:\n",
    "        pd.DataFrame.ta = pandas_ta.Core(df=None)\n",
    "        print(\"pandas_ta DataFrame accessor registered globally.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not globally register pandas_ta accessor: {e}\")\n",
    "\n",
    "try:\n",
    "    import optuna\n",
    "    optuna_available = True\n",
    "    print(\"Optuna imported successfully.\")\n",
    "except ImportError:\n",
    "    print(\"Optuna not found. LightGBM hyperparameter optimization with Optuna will be skipped.\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    # import torch.nn as nn # No longer needed directly if Autoformer is removed\n",
    "    # import torch.optim as optim # No longer needed directly if Autoformer is removed\n",
    "    torch_available = True\n",
    "    print(\"PyTorch imported successfully.\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"PyTorch CUDA available: True, Version: {torch.version.cuda}\")\n",
    "        print(f\"Using PyTorch on GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"PyTorch CUDA available: False.\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch not found. Foundation Model features will be SKIPPED.\")\n",
    "\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    imblearn_available = True\n",
    "    print(\"imblearn (for SMOTE) imported successfully.\")\n",
    "except ImportError:\n",
    "    imblearn_available = False\n",
    "    print(\"imblearn not found. SMOTE oversampling will be skipped.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    import dowhy\n",
    "    from dowhy import CausalModel\n",
    "    import networkx as nx\n",
    "    dowhy_available = True\n",
    "    print(f\"DoWhy {dowhy.__version__} and NetworkX {nx.__version__} imported successfully.\")\n",
    "except ImportError:\n",
    "    print(\"DoWhy or NetworkX not found. Causal Discovery will be skipped.\")\n",
    "\n",
    "try:\n",
    "    import onnx\n",
    "    onnx_available = True\n",
    "    # import onnxruntime as ort\n",
    "    import skl2onnx\n",
    "    skl2onnx_available = True\n",
    "    from skl2onnx.common.data_types import FloatTensorType\n",
    "    import onnxmltools\n",
    "    onnxmltools_available = True\n",
    "    print(\"ONNX, ONNXRuntime, skl2onnx, and onnxmltools imported successfully.\")\n",
    "    if hasattr(onnxmltools, '__version__'):\n",
    "          print(f\"Onnxmltools version: {onnxmltools.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"One or more ONNX components not found: {e}. ONNX features will be skipped.\")\n",
    "\n",
    "print(\"\\nAll libraries and modules conditional imports attempted.\")\n",
    "\n",
    "# --- Constants ---\n",
    "TWELVE_DATA_API_KEY = \"b6dbb92e551a46f2b20de27540aeef0a\" # Replace with your actual key\n",
    "API_KEY = TWELVE_DATA_API_KEY\n",
    "DEFAULT_SYMBOL = \"MSFT\"\n",
    "START_DATE = (datetime.now() - timedelta(days=3*365)).strftime('%Y-%m-%d')\n",
    "END_DATE = datetime.now().strftime('%Y-%m-%d')\n",
    "# AUTOFORMER_MODEL_PATH constant removed\n",
    "\n",
    "# --- Custom AutoformerPredictor and its helpers REMOVED ---\n",
    "\n",
    "# --- Data Fetching and Feature Engineering Functions ---\n",
    "def fetch_twelve_data(symbol: str, api_key: str, start_date_str: str = None, end_date_str: str = None) -> pd.DataFrame | None:\n",
    "    base_url = \"https://api.twelvedata.com/time_series\"\n",
    "    params = {\"symbol\": symbol, \"interval\": \"1day\", \"apikey\": api_key, \"format\": \"JSON\", \"outputsize\": 5000}\n",
    "    if start_date_str: params[\"start_date\"] = start_date_str\n",
    "    if end_date_str: params[\"end_date\"] = end_date_str\n",
    "    print(f\"Fetching data for {symbol} from Twelve Data (interval=1day, from {start_date_str} to {end_date_str})...\")\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "    except requests.exceptions.RequestException as e: print(f\"Request failed for {symbol}: {e}\"); return None\n",
    "    except ValueError as e: print(f\"Failed to parse JSON for {symbol}: {e}. Response: {response.text[:200]}...\"); return None\n",
    "\n",
    "    if isinstance(data, dict) and (data.get(\"status\") == \"error\" or \"values\" not in data):\n",
    "        print(f\"API Error for {symbol}: {data.get('message', 'Unknown error')}\"); return None\n",
    "    if not isinstance(data, dict) or \"values\" not in data or not data[\"values\"]:\n",
    "        print(f\"No data values for {symbol}, or unexpected format.\"); return None\n",
    "\n",
    "    df = pd.DataFrame(data[\"values\"]).rename(columns={'datetime': 'date'})\n",
    "    for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').astype('float64')\n",
    "        else:\n",
    "            if col in ['open', 'high', 'low', 'close']: print(f\"Critical column '{col}' missing.\"); return None\n",
    "            df[col] = 0.0 # Default for volume if missing\n",
    "    if 'date' not in df.columns: print(\"Critical 'date' column missing.\"); return None\n",
    "\n",
    "    df.index = pd.to_datetime(df['date'])\n",
    "    df.drop(columns=['date'], inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    df.dropna(subset=[col for col in ['open', 'high', 'low', 'close'] if col in df.columns], inplace=True)\n",
    "    if df.empty: print(f\"No data remaining for {symbol} after initial processing.\"); return None\n",
    "    print(f\"Successfully fetched/processed {len(df)} data points for {symbol}.\")\n",
    "    return df\n",
    "\n",
    "def add_technical_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_feat = df.copy()\n",
    "    if not hasattr(df_feat.ta, 'rsi'): print(\"pandas_ta not registered. Skipping TIs.\"); return df_feat\n",
    "    print(\"Adding technical indicators...\")\n",
    "    try:\n",
    "        for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "            if col in df_feat.columns: df_feat[col] = df_feat[col].astype('float64')\n",
    "            else:\n",
    "                if col in ['high', 'low', 'open', 'close'] and 'close' in df_feat: df_feat[col] = df_feat['close']\n",
    "                elif col == 'volume': df_feat[col] = 0.0\n",
    "\n",
    "        c, h, l, v = 'close', 'high', 'low', 'volume'\n",
    "        required_cols_for_all_ta = [c,h,l,v]\n",
    "        if not all(col in df_feat.columns for col in required_cols_for_all_ta):\n",
    "            print(\"Warning: Not all OHLCV columns present, some TIs might fail or be inaccurate.\")\n",
    "\n",
    "        if c in df_feat:\n",
    "            df_feat.ta.rsi(close=df_feat[c], length=14, append=True)\n",
    "            df_feat.ta.rsi(close=df_feat[c], length=9, append=True, col_names='RSI_9')\n",
    "            df_feat.ta.rsi(close=df_feat[c], length=25, append=True, col_names='RSI_25')\n",
    "            df_feat.ta.macd(close=df_feat[c], fast=12, slow=26, signal=9, append=True)\n",
    "            df_feat.ta.macd(close=df_feat[c], fast=5, slow=15, signal=9, append=True, col_names=('MACD_5_15_9', 'MACDh_5_15_9', 'MACDs_5_15_9'))\n",
    "            for p in [10, 20, 50, 100, 200]:\n",
    "                df_feat.ta.sma(close=df_feat[c], length=p, append=True)\n",
    "                df_feat.ta.ema(close=df_feat[c], length=p, append=True)\n",
    "            df_feat.ta.bbands(close=df_feat[c], length=20, std=2, append=True)\n",
    "        else:\n",
    "            print(f\"Column '{c}' not found, skipping some TIs.\")\n",
    "\n",
    "        if all(x in df_feat.columns for x in [h,l,c]):\n",
    "            df_feat.ta.atr(high=df_feat[h], low=df_feat[l], close=df_feat[c], length=14, append=True)\n",
    "            df_feat.ta.adx(high=df_feat[h], low=df_feat[l], close=df_feat[c], length=14, append=True)\n",
    "            df_feat.ta.stoch(high=df_feat[h], low=df_feat[l], close=df_feat[c], append=True)\n",
    "            df_feat.ta.willr(high=df_feat[h], low=df_feat[l], close=df_feat[c], append=True)\n",
    "            df_feat.ta.cci(high=df_feat[h], low=df_feat[l], close=df_feat[c], append=True)\n",
    "        else:\n",
    "            print(f\"One or more of '{h}', '{l}', '{c}' not found, skipping some TIs.\")\n",
    "\n",
    "        if all(x in df_feat.columns for x in [h,l,c,v]):\n",
    "            try:\n",
    "                # Calculate MFI separately, do not append directly\n",
    "                mfi_series = df_feat.ta.mfi(high=df_feat[h], low=df_feat[l], close=df_feat[c], volume=df_feat[v], append=False)\n",
    "                if mfi_series is not None:\n",
    "                    # Ensure the series is float and assign it\n",
    "                    df_feat[mfi_series.name] = mfi_series.astype('float64')\n",
    "                else:\n",
    "                    print(f\"MFI calculation returned None for {symbol if 'symbol' in locals() else 'current symbol'}.\") # Added a bit more context\n",
    "            except Exception as e_mfi:\n",
    "                print(f\"Error calculating MFI: {e_mfi}\")\n",
    "        else:\n",
    "            print(f\"One or more of '{h}', '{l}', '{c}', '{v}' not found, skipping MFI.\")\n",
    "\n",
    "        df_feat.columns = df_feat.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
    "    except Exception as e: print(f\"Error adding TIs: {e}\\n{traceback.format_exc()}\"); return df\n",
    "    return df_feat\n",
    "\n",
    "def add_optimized_features(df: pd.DataFrame, price_col='close', volume_col='volume') -> pd.DataFrame:\n",
    "    df_new = df.copy()\n",
    "    if price_col not in df_new.columns:\n",
    "        print(f\"Price column '{price_col}' not in DataFrame. Skipping optimized features.\"); return df_new\n",
    "\n",
    "    df_new['returns'] = df_new[price_col].pct_change()\n",
    "    safe_price = df_new[price_col].replace(0, np.nan)\n",
    "    safe_price_shifted = df_new[price_col].shift(1).replace(0, np.nan)\n",
    "    df_new['log_returns'] = np.log(safe_price / safe_price_shifted)\n",
    "\n",
    "    for window in [5, 10, 20, 50]:\n",
    "        df_new[f'volatility_{window}'] = df_new['log_returns'].rolling(window).std()\n",
    "        df_new[f'skew_{window}'] = df_new['log_returns'].rolling(window).skew()\n",
    "        df_new[f'kurtosis_{window}'] = df_new['log_returns'].rolling(window).kurt()\n",
    "\n",
    "    if volume_col in df_new.columns and df_new[volume_col].isnull().sum() < len(df_new):\n",
    "        rolling_mean_volume = df_new[volume_col].rolling(20).mean().replace(0, np.nan)\n",
    "        df_new['volume_ratio'] = df_new[volume_col] / rolling_mean_volume\n",
    "        df_new['price_volume'] = df_new[price_col] * df_new[volume_col]\n",
    "        df_new['volume_change'] = df_new[volume_col].pct_change()\n",
    "\n",
    "    if all(col in df_new.columns for col in ['high', 'low', 'close']):\n",
    "        safe_low = df_new['low'].replace(0, np.nan)\n",
    "        safe_high = df_new['high'].replace(0, np.nan)\n",
    "        df_new['high_low_ratio'] = df_new['high'] / safe_low\n",
    "        df_new['close_to_high_ratio'] = safe_price / safe_high\n",
    "        df_new['close_to_low_ratio'] = safe_price / safe_low\n",
    "        df_new['intraday_range_norm'] = (df_new['high'] - df_new['low']) / safe_price\n",
    "    else:\n",
    "        print(\"High, Low, or Close columns missing for some ratio calculations.\")\n",
    "\n",
    "    if 'RSI_14' in df_new.columns:\n",
    "        df_new['RSI_signal'] = 0\n",
    "        df_new.loc[df_new['RSI_14'] < 30, 'RSI_signal'] = 1\n",
    "        df_new.loc[df_new['RSI_14'] > 70, 'RSI_signal'] = -1\n",
    "\n",
    "    macd_col_name = 'MACD_12_26_9'\n",
    "    macds_col_name = 'MACDs_12_26_9'\n",
    "    if macd_col_name in df_new.columns and macds_col_name in df_new.columns:\n",
    "        df_new['MACD_signal_line_cross'] = (df_new[macd_col_name] > df_new[macds_col_name]).astype(int)\n",
    "    return df_new\n",
    "\n",
    "def add_wavelet_features(df: pd.DataFrame, column='close', wavelet='mexh', scales_range=(1, 32), num_scales_to_extract=5) -> pd.DataFrame:\n",
    "    df_feat = df.copy()\n",
    "    if pywt is None: print(\"PyWavelets not available.\"); return df_feat\n",
    "    if column not in df_feat.columns: print(f\"Column '{column}' not found for wavelet. Skipping.\"); return df_feat\n",
    "\n",
    "    signal = df_feat[column].values\n",
    "    if len(signal) < scales_range[1] + 5:\n",
    "        print(f\"Signal length {len(signal)} too short for CWT with max scale {scales_range[1]}. Skipping.\"); return df_feat\n",
    "\n",
    "    actual_max_scale = min(scales_range[1], len(signal) // 2 - 1)\n",
    "    if actual_max_scale < scales_range[0]:\n",
    "        print(f\"Max scale {actual_max_scale} too small after constraint (min_scale {scales_range[0]}). Skipping CWT.\"); return df_feat\n",
    "\n",
    "    scales = np.arange(scales_range[0], actual_max_scale + 1)\n",
    "    if len(scales) == 0: print(\"No valid scales for CWT. Skipping.\"); return df_feat\n",
    "\n",
    "    try:\n",
    "        coefficients, _ = pywt.cwt(signal, scales, wavelet)\n",
    "        coeffs_df = pd.DataFrame(coefficients.T, index=df_feat.index, columns=[f\"cwt_scale_{s}\" for s in scales])\n",
    "        df_feat[f'{column}_cwt_mean'] = coeffs_df.mean(axis=1)\n",
    "        df_feat[f'{column}_cwt_std'] = coeffs_df.std(axis=1)\n",
    "        s_indices_to_extract = np.linspace(0, len(scales)-1, min(num_scales_to_extract, len(scales)), dtype=int)\n",
    "        for s_idx in s_indices_to_extract:\n",
    "            actual_scale_val = scales[s_idx]\n",
    "            col_name_for_scale = f\"cwt_scale_{actual_scale_val}\"\n",
    "            if col_name_for_scale in coeffs_df.columns:\n",
    "                df_feat[f'{column}_cwt_energy_s{actual_scale_val}'] = coeffs_df[col_name_for_scale]**2\n",
    "    except Exception as e: print(f\"Error adding wavelet features: {e}\\n{traceback.format_exc()}\"); return df\n",
    "    return df_feat\n",
    "\n",
    "def add_entropy_features(df: pd.DataFrame, column='close', window=40) -> pd.DataFrame:\n",
    "    df_feat = df.copy()\n",
    "    if ant is None: print(\"Antropy not available.\"); return df_feat\n",
    "    if column not in df_feat.columns: print(f\"Column '{column}' not found for entropy. Skipping.\"); return df_feat\n",
    "\n",
    "    if len(df_feat) < window + 15:\n",
    "        print(f\"Data length {len(df_feat)} too short for entropy features with window {window}. Skipping.\"); return df_feat\n",
    "    try:\n",
    "        sig = df_feat[column].astype(float)\n",
    "        df_feat[f'{column}_entropy_sample'] = sig.rolling(window=window, min_periods=window).apply(\n",
    "            lambda x: ant.sample_entropy(x.dropna()) if x.dropna().shape[0] >= window//2 and x.dropna().std() > 1e-6 else np.nan, raw=False\n",
    "        )\n",
    "        df_feat[f'{column}_entropy_spectral'] = sig.rolling(window=window, min_periods=window).apply(\n",
    "            lambda x: ant.spectral_entropy(x.dropna(), sf=1.0, method='welch',\n",
    "                                           nperseg=min(x.dropna().shape[0], window // 2 if window // 2 > 0 else 1) if x.dropna().shape[0] > 1 else None\n",
    "                                          ) if x.dropna().shape[0] == window and x.dropna().std() > 1e-6 else np.nan, raw=False\n",
    "        )\n",
    "    except Exception as e: print(f\"Error adding entropy features: {e}\\n{traceback.format_exc()}\"); return df\n",
    "    return df_feat\n",
    "\n",
    "def add_advanced_technical_features(df: pd.DataFrame, price_col='close', high_col='high', low_col='low', volume_col='volume') -> pd.DataFrame:\n",
    "    df_new = df.copy()\n",
    "    if not hasattr(df_new.ta, 'mom'): print(\"pandas_ta not registered. Skipping advanced TIs.\"); return df_new\n",
    "    try:\n",
    "        if not all(c in df_new.columns for c in [price_col, high_col, low_col]):\n",
    "            print(f\"Missing one or more of {price_col}, {high_col}, {low_col} for adv TIs. Skipping.\"); return df_new\n",
    "        for col in [price_col, high_col, low_col]: df_new[col] = df_new[col].astype(float)\n",
    "        if volume_col in df_new.columns: df_new[volume_col] = df_new[volume_col].astype(float)\n",
    "\n",
    "        df_new.ta.mom(close=df_new[price_col], append=True)\n",
    "        df_new.ta.roc(close=df_new[price_col], append=True)\n",
    "        df_new.ta.natr(high=df_new[high_col], low=df_new[low_col], close=df_new[price_col], append=True)\n",
    "        df_new.ta.aroon(high=df_new[high_col], low=df_new[low_col], append=True)\n",
    "        df_new.ta.stc(close=df_new[price_col], tclength=23, fast=50, slow=100, factor=0.5, append=True, col_names=('STC_23_50_05', 'STCD_23_50_05', 'STCK_23_50_05'))\n",
    "\n",
    "        if volume_col in df_new.columns and df_new[volume_col].isnull().sum() < len(df_new):\n",
    "            df_new.ta.pvol(close=df_new[price_col], volume=df_new[volume_col], append=True)\n",
    "            df_new.ta.cmf(high=df_new[high_col], low=df_new[low_col], close=df_new[price_col], volume=df_new[volume_col], append=True)\n",
    "\n",
    "        df_new.columns = df_new.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
    "    except Exception as e: print(f\"Error adding advanced TIs: {e}\\n{traceback.format_exc()}\"); return df\n",
    "    return df_new\n",
    "\n",
    "def add_transformer_features_conceptual(df: pd.DataFrame, column='close', sequence_length=20) -> pd.DataFrame:\n",
    "    df_feat = df.copy()\n",
    "    if column not in df_feat.columns: print(f\"Column '{column}' not found for conceptual Transformer features. Skipping.\"); return df_feat\n",
    "    if len(df_feat) < sequence_length + 5: print(f\"Data too short for conceptual Transformer features. Skipping.\"); return df_feat\n",
    "\n",
    "    feature_col_base = f\"{column}_trans_seq\"\n",
    "    for col_suffix in ['mean', 'std', 'trend', 'volatility', 'autocorr1']:\n",
    "        df_feat[f'{feature_col_base}_{col_suffix}'] = np.nan\n",
    "    try:\n",
    "        data_series = df_feat[column].values\n",
    "        windows = np.lib.stride_tricks.sliding_window_view(data_series, sequence_length)\n",
    "        results = {key: [np.nan] * (sequence_length -1) for key in ['mean', 'std', 'trend', 'volatility', 'autocorr1']}\n",
    "\n",
    "        for seq in windows:\n",
    "            if np.isnan(seq).any():\n",
    "                for key in results: results[key].append(np.nan)\n",
    "                continue\n",
    "            mean_val, std_val = np.mean(seq), np.std(seq)\n",
    "            norm_seq = (seq - mean_val) / std_val if std_val > 1e-8 else np.zeros_like(seq)\n",
    "\n",
    "            results['mean'].append(np.mean(norm_seq))\n",
    "            results['std'].append(np.std(norm_seq))\n",
    "            current_trend, current_vol, current_ac = 0.0, 0.0, 0.0\n",
    "\n",
    "            if len(norm_seq) > 1:\n",
    "                try:\n",
    "                    fit_params = np.polyfit(np.arange(len(norm_seq)), norm_seq, 1)\n",
    "                    current_trend = fit_params[0] if not np.isnan(fit_params[0]) else 0.0\n",
    "                except (np.linalg.LinAlgError, ValueError): pass\n",
    "\n",
    "                diff_norm_seq = np.diff(norm_seq)\n",
    "                current_vol = np.std(diff_norm_seq) if len(diff_norm_seq) > 0 else 0.0\n",
    "\n",
    "                if len(norm_seq) >= 2:\n",
    "                    s1, s2 = norm_seq[:-1], norm_seq[1:]\n",
    "                    if len(s1) >= 1 and np.std(s1) > 1e-8 and np.std(s2) > 1e-8:\n",
    "                        try:\n",
    "                            corr_matrix = np.corrcoef(s1, s2)\n",
    "                            current_ac = corr_matrix[0, 1] if not np.isnan(corr_matrix[0, 1]) else 0.0\n",
    "                        except (ValueError, IndexError): pass\n",
    "            results['trend'].append(current_trend)\n",
    "            results['volatility'].append(current_vol)\n",
    "            results['autocorr1'].append(current_ac)\n",
    "\n",
    "        for key, values in results.items():\n",
    "            if len(values) == len(df_feat):\n",
    "                df_feat[f'{feature_col_base}_{key}'] = values\n",
    "            else:\n",
    "                padded_values = np.full(len(df_feat), np.nan)\n",
    "                if len(values) > 0 : padded_values[-len(values):] = values\n",
    "                df_feat[f'{feature_col_base}_{key}'] = padded_values\n",
    "    except Exception as e: print(f\"Error in conceptual Transformer features: {e}\\n{traceback.format_exc()}\"); return df\n",
    "    return df_feat\n",
    "\n",
    "def detect_regimes_simple(df: pd.DataFrame, column='close', window=20) -> pd.DataFrame:\n",
    "    df_reg = df.copy()\n",
    "    # print(f\"Detecting regimes (simplified volatility-based) for {column}...\") # Less verbose\n",
    "    if column not in df_reg.columns:\n",
    "        print(f\"'{column}' not found. Skipping simple regimes.\")\n",
    "        df_reg['regime_simple'] = 0\n",
    "        return df_reg\n",
    "\n",
    "    if 'log_returns' not in df_reg.columns:\n",
    "        safe_price = df_reg[column].replace(0, np.nan)\n",
    "        safe_price_shifted = df_reg[column].shift(1).replace(0, np.nan)\n",
    "        df_reg['log_returns_temp_for_regime'] = np.log(safe_price / safe_price_shifted)\n",
    "        returns_col_for_regime = 'log_returns_temp_for_regime'\n",
    "    else:\n",
    "        returns_col_for_regime = 'log_returns'\n",
    "\n",
    "    returns = df_reg[returns_col_for_regime].dropna()\n",
    "    if returns.empty:\n",
    "        print(\"No valid returns for simple regime detection. Skipping.\")\n",
    "        df_reg['regime_simple'] = 0\n",
    "        if 'log_returns_temp_for_regime' in df_reg.columns: df_reg.drop(columns=['log_returns_temp_for_regime'], inplace=True, errors='ignore')\n",
    "        return df_reg\n",
    "\n",
    "    rolling_vol = returns.rolling(window=window, min_periods=window//2 if window//2 > 0 else 1).std()\n",
    "    df_reg['regime_simple'] = 0 # Default: medium volatility (class 0)\n",
    "\n",
    "    if not rolling_vol.dropna().empty:\n",
    "        vol_low_thresh = rolling_vol.quantile(0.33)\n",
    "        vol_high_thresh = rolling_vol.quantile(0.67)\n",
    "        df_reg.loc[rolling_vol.index[rolling_vol <= vol_low_thresh], 'regime_simple'] = 1  # Low vol (class 1)\n",
    "        df_reg.loc[rolling_vol.index[rolling_vol > vol_high_thresh], 'regime_simple'] = 2   # High vol (class 2)\n",
    "    else:\n",
    "        print(\"Not enough data for rolling volatility percentile calculation. Defaulting simple regimes to 0.\")\n",
    "\n",
    "    df_reg['regime_simple'] = df_reg['regime_simple'].bfill().ffill().fillna(0) # CORRECTED LINE\n",
    "    \n",
    "    if 'log_returns_temp_for_regime' in df_reg.columns: df_reg.drop(columns=['log_returns_temp_for_regime'], inplace=True, errors='ignore')\n",
    "\n",
    "    print(f\"Simple Regimes (0:Med,1:Low,2:High):\\n{df_reg['regime_simple'].value_counts(normalize=True, dropna=False).sort_index()*100} %\")\n",
    "    return df_reg\n",
    "\n",
    "def balanced_target_definition(df: pd.DataFrame, column='close', periods=5, lower_q_thresh=0.45, upper_q_thresh=0.55) -> pd.DataFrame:\n",
    "    df_t = df.copy()\n",
    "    if column not in df_t.columns:\n",
    "        print(f\"'{column}' not found for target. Defaulting target.\")\n",
    "        df_t['target'] = 0\n",
    "        return df_t\n",
    "\n",
    "    df_t[column] = pd.to_numeric(df_t[column], errors='coerce').replace(0, np.nan)\n",
    "    df_t['future_log_return_target'] = np.log(df_t[column].shift(-periods) / df_t[column])\n",
    "    valid_returns = df_t['future_log_return_target'].dropna()\n",
    "    df_t['target'] = 0\n",
    "\n",
    "    if len(valid_returns) > 20:\n",
    "        lower_q_val = valid_returns.quantile(lower_q_thresh)\n",
    "        upper_q_val = valid_returns.quantile(upper_q_thresh)\n",
    "        if lower_q_val >= upper_q_val and upper_q_val > 0 : lower_q_val = upper_q_val * 0.99\n",
    "        elif lower_q_val >= upper_q_val and upper_q_val < 0 : upper_q_val = lower_q_val * 0.99\n",
    "        df_t.loc[df_t['future_log_return_target'] < lower_q_val, 'target'] = 0\n",
    "        df_t.loc[df_t['future_log_return_target'] > upper_q_val, 'target'] = 1\n",
    "    else:\n",
    "        print(\"Not enough valid returns for quantile-based target balancing. Default target (all 0s) used or target may be skewed.\")\n",
    "\n",
    "    df_t.drop(columns=['future_log_return_target'], inplace=True, errors='ignore')\n",
    "    print(f\"Target distribution:\\n{df_t['target'].value_counts(normalize=True, dropna=False)*100}\")\n",
    "    return df_t\n",
    "\n",
    "def discover_causal_structure(df_features: pd.DataFrame, target_col='target', price_c='close', max_feats=10, symbol=\"\") -> tuple[CausalModel | None, list]:\n",
    "    print(f\"\\nDiscovering causal structure for {symbol} using DoWhy...\")\n",
    "    graph_feats = [] # Initialize graph_feats\n",
    "    if not dowhy_available or CausalModel is None:\n",
    "        print(\"DoWhy not available.\")\n",
    "        return None, graph_feats\n",
    "\n",
    "    df_c = df_features.copy()\n",
    "    if target_col not in df_c.columns or df_c[target_col].isnull().all():\n",
    "        print(f\"Target '{target_col}' missing for causal discovery.\")\n",
    "        return None, graph_feats\n",
    "    df_c[target_col] = pd.to_numeric(df_c[target_col], errors='coerce')\n",
    "    cand_cols = [c for c in df_c.columns if pd.api.types.is_numeric_dtype(df_c[c]) and c != target_col and df_c[c].notnull().any() and df_c[c].var() > 1e-6]\n",
    "    if not cand_cols:\n",
    "        print(\"No numeric candidate columns with variance for causal discovery.\")\n",
    "        return None, graph_feats\n",
    "\n",
    "    df_subset_for_causal = df_c[cand_cols + [target_col]].copy()\n",
    "    df_subset_for_causal.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    if cand_cols:\n",
    "        scaler_causal = StandardScaler()\n",
    "        df_subset_for_causal[cand_cols] = scaler_causal.fit_transform(df_subset_for_causal[cand_cols])\n",
    "    df_subset_for_causal.dropna(inplace=True)\n",
    "    if df_subset_for_causal.empty or target_col not in df_subset_for_causal.columns or df_subset_for_causal[target_col].nunique() < 1:\n",
    "        print(\"Not enough data post-cleaning/scaling for causal discovery.\")\n",
    "        return None, graph_feats\n",
    "\n",
    "    cwt_mean_col = f\"{price_c}_cwt_mean\" if f\"{price_c}_cwt_mean\" in df_subset_for_causal.columns else 'close_cwt_mean'\n",
    "    cwt_std_col = f\"{price_c}_cwt_std\" if f\"{price_c}_cwt_std\" in df_subset_for_causal.columns else 'close_cwt_std'\n",
    "    entropy_sample_col = f\"{price_c}_entropy_sample\" if f\"{price_c}_entropy_sample\" in df_subset_for_causal.columns else 'close_entropy_sample'\n",
    "    potential_causes = ['RSI_14', 'MACDh_12_26_9', 'ADX_14', 'ATR_14', cwt_mean_col, cwt_std_col, entropy_sample_col, 'regime_simple', 'volatility_20', 'log_returns', 'BBP_2020', 'BBB_2020']\n",
    "    \n",
    "    # graph_feats is defined here\n",
    "    graph_feats = [c for c in potential_causes if c in df_subset_for_causal.columns and c != target_col and df_subset_for_causal[c].nunique() > 1]\n",
    "    \n",
    "    if not graph_feats:\n",
    "        print(\"Predefined causal graph_feats not suitable or not found, selecting top varying features (after scaling).\")\n",
    "        num_to_select = min(max_feats, len(cand_cols))\n",
    "        if num_to_select > 0:\n",
    "            graph_feats = df_subset_for_causal[cand_cols].var().nlargest(num_to_select).index.tolist()\n",
    "        else:\n",
    "            print(\"No candidate columns for graph_feats fallback.\")\n",
    "            return None, [] # Return empty list for graph_feats\n",
    "    if not graph_feats:\n",
    "        print(\"No suitable graph features for causal discovery.\")\n",
    "        return None, [] # Return empty list for graph_feats\n",
    "\n",
    "    final_df_for_causal_model = df_subset_for_causal[graph_feats + [target_col]].copy()\n",
    "    if final_df_for_causal_model.empty or final_df_for_causal_model.shape[0] < 20 or final_df_for_causal_model[target_col].nunique() < 1:\n",
    "        print(\"Final DF for causal model too small or target has no variation.\")\n",
    "        return None, graph_feats # Return potentially non-empty graph_feats even if model fails\n",
    "        \n",
    "    print(f\"DoWhy using graph features: {graph_feats} for Outcome: {target_col}\")\n",
    "    treatment_var = graph_feats[0] # Simplistic: pick the first as treatment\n",
    "    graph_str = \"digraph { \" + \"; \".join([f'\"{f}\" -> \"{target_col}\"' for f in graph_feats]) + \" }\"\n",
    "    # print(f\"Generated Causal Graph:\\n{graph_str}\") # Less verbose\n",
    "    try:\n",
    "        model = CausalModel(data=final_df_for_causal_model, treatment=treatment_var, outcome=target_col, graph=graph_str)\n",
    "        print(\"DoWhy CausalModel created.\")\n",
    "        return model, graph_feats # MODIFIED: Return model and graph_feats\n",
    "    except Exception as e:\n",
    "        print(f\"DoWhy CausalModel error: {e}\\n{traceback.format_exc()}\")\n",
    "        return None, graph_feats # Return graph_feats even if model creation fails\n",
    "\n",
    "\n",
    "\n",
    "def causal_feature_ranking_from_graph_feats(discovered_graph_features: list, X_train_columns: pd.Index) -> list:\n",
    "    \"\"\"\n",
    "    Takes the list of features identified for the causal graph and returns them.\n",
    "    Ensures they are present in the training data columns.\n",
    "    This is a simple first step; more advanced methods would rank by causal effect strength.\n",
    "    \"\"\"\n",
    "    if not discovered_graph_features:\n",
    "        print(\"No graph features provided from causal discovery.\")\n",
    "        return []\n",
    "    \n",
    "    # Filter to ensure features are actually in X_train (should be, but good check)\n",
    "    ranked_causal_features = [(feat, 1.0) for feat in discovered_graph_features if feat in X_train_columns] # Assign dummy score 1.0\n",
    "    \n",
    "    if ranked_causal_features:\n",
    "        print(f\"Causal feature ranking (from graph features): {[feat for feat, score in ranked_causal_features]}\")\n",
    "    else:\n",
    "        print(\"No causal features from graph were found in X_train columns or none were discovered.\")\n",
    "    return ranked_causal_features\n",
    "\n",
    "\n",
    "\n",
    "def prepare_ml_data(df: pd.DataFrame, target_col='target', test_split_size=0.15, min_test_samples=50):\n",
    "    if target_col not in df.columns:\n",
    "        print(f\"Target '{target_col}' missing.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    cols_to_drop_base = ['open', 'high', 'low', 'close', 'volume', 'returns']\n",
    "    cols_to_drop_dynamic = [c for c in df.columns if 'target_' in c and c != target_col] + \\\n",
    "                           [c for c in df.columns if 'future_return' in c]\n",
    "\n",
    "    all_cols_to_drop = list(set(cols_to_drop_base + cols_to_drop_dynamic))\n",
    "    if target_col in all_cols_to_drop:\n",
    "        all_cols_to_drop.remove(target_col)\n",
    "\n",
    "    X = df.drop(columns=[c for c in all_cols_to_drop if c in df.columns] + [target_col], errors='ignore')\n",
    "    y = df[target_col].copy()\n",
    "\n",
    "    if y.isnull().all():\n",
    "        print(\"Target is all NaN.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    valid_target_mask = y.notna()\n",
    "    X = X.loc[valid_target_mask]\n",
    "    y = y.loc[valid_target_mask]\n",
    "\n",
    "    if X.empty or y.empty:\n",
    "        print(\"X or y empty after target NaN filter.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    if X.isnull().any().any():\n",
    "        for col in X.columns:\n",
    "            if X[col].isnull().any():\n",
    "                if pd.api.types.is_numeric_dtype(X[col]):\n",
    "                    X[col] = X[col].fillna(X[col].median())\n",
    "                else:\n",
    "                    X[col] = X[col].fillna(X[col].mode()[0] if not X[col].mode().empty else \"Unknown\")\n",
    "\n",
    "    if X.isnull().any().any():\n",
    "        print(f\"Warning: NaNs still present after imputation. Dropping rows with NaNs in X. Nulls per col:\\n{X.isnull().sum()[X.isnull().sum()>0]}\")\n",
    "        X.dropna(axis=0, how='any', inplace=True)\n",
    "        y = y.loc[X.index]\n",
    "\n",
    "    if X.empty or y.empty:\n",
    "        print(\"X or y empty after internal NaN handling.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    if len(X) < min_test_samples * 2:\n",
    "        print(f\"Not enough data ({len(X)} rows) for robust train/test split. Min required for split: {min_test_samples*2}.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    n_samples = len(X)\n",
    "    test_size_abs = max(min_test_samples, int(n_samples * test_split_size))\n",
    "\n",
    "    if n_samples - test_size_abs < min_test_samples:\n",
    "        test_size_abs = n_samples - min_test_samples\n",
    "\n",
    "    if test_size_abs < 1 and n_samples > 0:\n",
    "        test_size_abs = 1\n",
    "    elif test_size_abs < 1:\n",
    "        print(f\"Cannot make meaningful split (test_size_abs < 1).\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    train_size = n_samples - test_size_abs\n",
    "    if train_size < 1:\n",
    "        print(f\"Train size too small ({train_size}). Cannot split.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "    y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "\n",
    "    if X_train.empty or y_train.empty or X_test.empty or y_test.empty:\n",
    "        print(\"Train/Test set empty post-split.\")\n",
    "        return None, None, None, None, None\n",
    "    print(f\"Train shapes: X_train={X_train.shape}, y_train={y_train.shape}; Test shapes: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
    "\n",
    "    numeric_cols_xtrain = X_train.select_dtypes(include=np.number).columns\n",
    "    scaler = None\n",
    "\n",
    "    if not numeric_cols_xtrain.empty:\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled_np = scaler.fit_transform(X_train[numeric_cols_xtrain])\n",
    "        X_train_scaled_df = pd.DataFrame(X_train_scaled_np, columns=numeric_cols_xtrain, index=X_train.index)\n",
    "\n",
    "        X_train_final = X_train.copy()\n",
    "        X_train_final[numeric_cols_xtrain] = X_train_scaled_df\n",
    "\n",
    "        numeric_cols_xtest = X_test.select_dtypes(include=np.number).columns\n",
    "        common_numeric_cols = [col for col in numeric_cols_xtrain if col in numeric_cols_xtest]\n",
    "\n",
    "        X_test_final = X_test.copy()\n",
    "        if common_numeric_cols:\n",
    "            X_test_scaled_np = scaler.transform(X_test[common_numeric_cols])\n",
    "            X_test_scaled_df = pd.DataFrame(X_test_scaled_np, columns=common_numeric_cols, index=X_test[common_numeric_cols].index)\n",
    "            X_test_final[common_numeric_cols] = X_test_scaled_df\n",
    "        else:\n",
    "            print(\"No common numeric columns to scale in X_test, or X_test has no numeric columns that were scaled in train.\")\n",
    "\n",
    "        return X_train_final, X_test_final, y_train, y_test, scaler\n",
    "    else:\n",
    "        print(\"No numeric columns in X_train for scaling.\")\n",
    "        return X_train, X_test, y_train, y_test, None\n",
    "\n",
    "def lgbm_objective(trial, X_train, y_train, X_val, y_val, base_params):\n",
    "    params = {\n",
    "        **base_params,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=50),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 50),\n",
    "        'max_depth': trial.suggest_int('max_depth', -1, 10),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 0, 7),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 1.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 1.0, log=True),\n",
    "        'min_gain_to_split': trial.suggest_float('min_gain_to_split', 0.0, 0.1)\n",
    "    }\n",
    "    if params.get('num_class') is None and 'num_class' in params:\n",
    "        del params['num_class']\n",
    "\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X_train, y_train,\n",
    "              eval_set=[(X_val, y_val)],\n",
    "              eval_metric=base_params.get('metric', 'logloss'),\n",
    "              callbacks=[lgb.early_stopping(30, verbose=False)])\n",
    "    y_proba_val = model.predict_proba(X_val)\n",
    "    return log_loss(y_val, y_proba_val)\n",
    "\n",
    "def optimize_lgbm_hyperparameters(X_train: pd.DataFrame, y_train: pd.Series, base_params: dict, n_trials=30, validation_ratio=0.2) -> dict:\n",
    "    if not optuna_available:\n",
    "        print(\"Optuna not available. Using default HPs.\")\n",
    "        return optimized_lightgbm_params()\n",
    "\n",
    "    print(f\"Optimizing LightGBM HPs with Optuna ({n_trials} trials)...\")\n",
    "    if len(X_train) * validation_ratio < 1 or len(X_train) * (1-validation_ratio) < 1:\n",
    "        print(\"Too few samples for Optuna validation split. Using default HPs.\")\n",
    "        return optimized_lightgbm_params()\n",
    "\n",
    "    n_val_samples = int(len(X_train) * validation_ratio)\n",
    "    if n_val_samples == 0 and len(X_train) > 1: n_val_samples = 1\n",
    "    elif n_val_samples == 0 :\n",
    "        print(\"Cannot create validation set for Optuna (0 samples). Using default HPs.\")\n",
    "        return optimized_lightgbm_params()\n",
    "\n",
    "    n_train_opt_samples = len(X_train) - n_val_samples\n",
    "    if n_train_opt_samples == 0:\n",
    "        print(\"Train set for Optuna is empty after split. Using default HPs.\")\n",
    "        return optimized_lightgbm_params()\n",
    "\n",
    "    X_train_opt, X_val_opt = X_train.iloc[:n_train_opt_samples], X_train.iloc[n_train_opt_samples:]\n",
    "    y_train_opt, y_val_opt = y_train.iloc[:n_train_opt_samples], y_train.iloc[n_train_opt_samples:]\n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: lgbm_objective(trial, X_train_opt, y_train_opt, X_val_opt, y_val_opt, base_params),\n",
    "                   n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "    print(f\"Best Optuna trial for LightGBM: Value={study.best_value:.4f}, Params={study.best_params}\")\n",
    "    return study.best_params\n",
    "\n",
    "def train_lightgbm_model(X_train, y_train, X_test, y_test, optimized_params=None):\n",
    "    print(\"Training LightGBM model...\")\n",
    "    if X_train is None or X_train.empty or y_train is None or y_train.empty:\n",
    "        print(\"X_train or y_train is empty. Skipping LightGBM training.\")\n",
    "        return None, None\n",
    "\n",
    "    y_train_squeezed = y_train.squeeze()\n",
    "    y_test_squeezed = y_test.squeeze() if y_test is not None else pd.Series()\n",
    "    unique_labels_train = sorted(y_train_squeezed.unique())\n",
    "    num_classes = len(unique_labels_train)\n",
    "\n",
    "    if num_classes <= 1:\n",
    "        print(f\"Only {num_classes} class(es) in y_train. Skipping LightGBM training.\")\n",
    "        return None, None\n",
    "\n",
    "    current_params = optimized_lightgbm_params()\n",
    "    current_params['objective'] = 'multiclass' if num_classes > 2 else 'binary'\n",
    "    current_params['metric'] = 'multi_logloss' if num_classes > 2 else 'binary_logloss'\n",
    "    if num_classes > 2:\n",
    "        current_params['num_class'] = num_classes\n",
    "    elif 'num_class' in current_params:\n",
    "        del current_params['num_class']\n",
    "\n",
    "    if optimized_params and isinstance(optimized_params, dict):\n",
    "        print(\"Using Optuna-optimized parameters.\")\n",
    "        current_params.update(optimized_params)\n",
    "    else:\n",
    "        print(\"Using default (or non-Optuna optimized) LightGBM parameters.\")\n",
    "\n",
    "    label_map = {label: i for i, label in enumerate(unique_labels_train)}\n",
    "    y_train_mapped = y_train_squeezed.map(label_map)\n",
    "    model = lgb.LGBMClassifier(**current_params)\n",
    "    eval_set_data = None\n",
    "    valid_eval_indices = None\n",
    "    y_test_mapped_for_eval = None\n",
    "\n",
    "    if X_test is not None and not X_test.empty and not y_test_squeezed.empty:\n",
    "        y_test_mapped = y_test_squeezed.map(label_map).fillna(-1).astype(int)\n",
    "        valid_eval_indices = (y_test_mapped != -1)\n",
    "        if valid_eval_indices.any():\n",
    "            y_test_mapped_for_eval = y_test_mapped[valid_eval_indices]\n",
    "            X_test_eval = X_test[valid_eval_indices][X_train.columns] if all(c in X_test.columns for c in X_train.columns) else X_test[valid_eval_indices]\n",
    "            eval_set_data = (X_test_eval, y_test_mapped_for_eval)\n",
    "\n",
    "    if eval_set_data:\n",
    "        model.fit(X_train, y_train_mapped, eval_set=[eval_set_data],\n",
    "                  eval_metric=current_params.get('metric'),\n",
    "                  callbacks=[lgb.early_stopping(30, verbose=False)])\n",
    "    else:\n",
    "        print(\"Warning: No valid eval set. Fitting on full training data without early stopping based on eval set.\")\n",
    "        model.fit(X_train, y_train_mapped)\n",
    "\n",
    "    feat_imp_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': model.feature_importances_}).sort_values(by='Importance', ascending=False)\n",
    "    print(\"\\nTop 10 features:\\n\", feat_imp_df.head(10))\n",
    "\n",
    "    if eval_set_data and y_test_mapped_for_eval is not None and not y_test_mapped_for_eval.empty:\n",
    "        X_test_predict = X_test[valid_eval_indices][X_train.columns] if all(c in X_test.columns for c in X_train.columns) else X_test[valid_eval_indices]\n",
    "        y_pred_mapped_on_valid = model.predict(X_test_predict)\n",
    "        y_proba_on_valid = model.predict_proba(X_test_predict)\n",
    "        acc = accuracy_score(y_test_mapped_for_eval, y_pred_mapped_on_valid)\n",
    "        print(f\"\\nðŸŽ¯ Accuracy on mapped test data: {acc:.4f}\")\n",
    "\n",
    "        if current_params['objective'] == 'binary' and y_proba_on_valid.shape[1] == 2:\n",
    "            try:\n",
    "                auc = roc_auc_score(y_test_mapped_for_eval, y_proba_on_valid[:, 1])\n",
    "                print(f\"ðŸ“Š AUC: {auc:.4f}\")\n",
    "            except ValueError as e_auc:\n",
    "                print(f\"AUC Calculation Error: {e_auc}\")\n",
    "        print(\"\\nClassification Report (on mapped and valid test labels):\")\n",
    "        try:\n",
    "            report_labels = sorted(np.unique(np.concatenate((y_test_mapped_for_eval.unique(), pd.Series(y_pred_mapped_on_valid).unique()))))\n",
    "            print(classification_report(y_test_mapped_for_eval, y_pred_mapped_on_valid, labels=report_labels, zero_division=0))\n",
    "        except Exception as e_cr:\n",
    "            print(f\"Classification Report Error: {e_cr}\")\n",
    "    else:\n",
    "        print(\"No valid test samples for evaluation after mapping, or X_test/y_test was not provided.\")\n",
    "    return model, feat_imp_df\n",
    "\n",
    "def plot_feature_importance(feature_importance_df, top_n=20, symbol_for_plot=\"\", min_bar_height=0.05):\n",
    "    if feature_importance_df is None or feature_importance_df.empty:\n",
    "        print(\"No feature importance to plot.\")\n",
    "        return\n",
    "\n",
    "    plot_data = feature_importance_df.head(top_n).copy()\n",
    "    if plot_data.empty:\n",
    "        print(\"No features in plot_data after head(top_n).\")\n",
    "        return\n",
    "\n",
    "    max_importance = plot_data['Importance'].max()\n",
    "    min_threshold = max(max_importance * 0.02, 1e-6)\n",
    "    plot_data['Plot_Importance'] = np.maximum(plot_data['Importance'], min_threshold)\n",
    "\n",
    "    plt.figure(figsize=(14, max(8, min(top_n, len(plot_data)) * 0.5)))\n",
    "    ax = sns.barplot(x='Plot_Importance', y='Feature', hue='Feature', data=plot_data, palette=\"viridis\", orient='h', legend=False) # Added hue and legend=False\n",
    "    for i, row_data in enumerate(plot_data.itertuples()):\n",
    "        original_val = row_data.Importance\n",
    "        plot_val = row_data.Plot_Importance\n",
    "        ax.text(plot_val + max_importance * 0.01, i, f'{original_val:.0f}', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "    plt.title(f'Top {top_n} Feature Importances for {symbol_for_plot} (LightGBM)', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Importance', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Feature', fontsize=12, fontweight='bold')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plot_filename = f\"feature_importance_{symbol_for_plot}.png\"\n",
    "    plt.savefig(plot_filename)\n",
    "    print(f\"Feature importance plot saved to {plot_filename}\")\n",
    "    plt.close()\n",
    "\n",
    "def export_lgbm_to_onnx(lgbm_model, X_sample_df, file_path=\"lgbm_model.onnx\", target_opset=12):\n",
    "    print(f\"\\nExporting LGBM model to ONNX: {file_path} (opset={target_opset})\")\n",
    "    if not all([onnx_available, skl2onnx_available, onnxmltools_available, (FloatTensorType is not None)]):\n",
    "        print(\"One or more ONNX libraries missing or FloatTensorType not imported. Skipping ONNX export.\")\n",
    "        return None\n",
    "    if lgbm_model is None or X_sample_df is None or X_sample_df.empty:\n",
    "        print(\"Model or sample data empty for ONNX. Skipping.\")\n",
    "        return None\n",
    "    try:\n",
    "        initial_type = [('float_input', FloatTensorType([None, X_sample_df.shape[1]]))]\n",
    "        converted_model = onnxmltools.convert_lightgbm(lgbm_model, initial_types=initial_type, target_opset=target_opset)\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(converted_model.SerializeToString())\n",
    "        print(f\"Model exported to ONNX: {file_path}\")\n",
    "        onnx.checker.check_model(file_path)\n",
    "        print(\"ONNX model check OK.\")\n",
    "        return file_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting LGBM to ONNX: {e}. Fallback to pickle.\")\n",
    "        try:\n",
    "            import pickle\n",
    "            pkl_path = file_path.replace('.onnx', '.pkl')\n",
    "            with open(pkl_path, 'wb') as pf:\n",
    "                pickle.dump(lgbm_model, pf)\n",
    "            print(f\"Model saved as pickle: {pkl_path}\")\n",
    "            return pkl_path\n",
    "        except Exception as ep:\n",
    "            print(f\"Pickle save error: {ep}\")\n",
    "            return None\n",
    "\n",
    "def simple_feature_selection_fallback(X_train, y_train, max_features=20):\n",
    "    print(\"Using simple variance-based feature selection fallback...\")\n",
    "    if X_train.empty: return pd.DataFrame()\n",
    "    X_train_numeric = X_train.select_dtypes(include=np.number)\n",
    "    if X_train_numeric.empty:\n",
    "        print(\"No numeric features for variance selection. Returning first few columns if available.\")\n",
    "        return pd.DataFrame({'Feature': X_train.columns[:max_features].tolist()})\n",
    "    variance_scores = X_train_numeric.var().sort_values(ascending=False)\n",
    "    num_features_to_select = min(max_features, len(variance_scores))\n",
    "    selected_features = variance_scores.head(num_features_to_select).index.tolist()\n",
    "    return pd.DataFrame({'Feature': selected_features, 'Score': variance_scores.head(num_features_to_select).values})\n",
    "\n",
    "def prioritized_feature_selection(X_train, y_train, causal_ranking, max_features=25):\n",
    "    # print(\"Prioritized feature selection: Causal Ranking + Mutual Information...\") # Less verbose\n",
    "    if X_train.empty or y_train.empty:\n",
    "        print(\"X_train or y_train is empty in prioritized_feature_selection. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    top_causal_features = []\n",
    "    num_causal_to_select = 0\n",
    "    if causal_ranking and isinstance(causal_ranking, list) and all(isinstance(item, tuple) and len(item)==2 for item in causal_ranking):\n",
    "        num_causal_to_select = min(len(causal_ranking), max_features // 2)\n",
    "        if num_causal_to_select > 0:\n",
    "            # print(f\"Selecting up to {num_causal_to_select} features from causal ranking.\") # Less verbose\n",
    "            for feat, score in causal_ranking[:num_causal_to_select]:\n",
    "                if feat in X_train.columns:\n",
    "                    top_causal_features.append(feat)\n",
    "                # else: print(f\"Causal feature '{feat}' not in X_train.columns. Skipping.\") # Less verbose\n",
    "    # else: print(\"No valid causal ranking provided or num_causal_to_select is 0.\") # Less verbose\n",
    "\n",
    "    remaining_slots = max_features - len(top_causal_features)\n",
    "    stat_selected_features = []\n",
    "\n",
    "    if remaining_slots > 0:\n",
    "        features_for_stat_selection = [f for f in X_train.columns if f not in top_causal_features]\n",
    "        if features_for_stat_selection:\n",
    "            X_remaining_for_stat = X_train[features_for_stat_selection]\n",
    "            y_train_squeezed = y_train.squeeze()\n",
    "\n",
    "            if y_train_squeezed.nunique() > 1 and not X_remaining_for_stat.empty:\n",
    "                X_remaining_numeric = X_remaining_for_stat.select_dtypes(include=np.number)\n",
    "                if not X_remaining_numeric.empty:\n",
    "                    num_stat_to_select = min(remaining_slots, X_remaining_numeric.shape[1])\n",
    "                    if num_stat_to_select > 0:\n",
    "                        try:\n",
    "                            selector_mi = SelectKBest(mutual_info_classif, k=num_stat_to_select)\n",
    "                            selector_mi.fit(X_remaining_numeric, y_train_squeezed)\n",
    "                            stat_selected_features = X_remaining_numeric.columns[selector_mi.get_support()].tolist()\n",
    "                        except Exception as e_mi:\n",
    "                            print(f\"Error in MI based feature selection: {e_mi}. Proceeding without these stat features.\")\n",
    "    final_selected_features = list(dict.fromkeys(top_causal_features + stat_selected_features))\n",
    "    if not final_selected_features and not X_train.empty:\n",
    "        print(\"No features from prioritized selection, falling back to simple variance-based selection.\")\n",
    "        simple_fallback_df = simple_feature_selection_fallback(X_train, y_train, max_features)\n",
    "        if simple_fallback_df is not None and 'Feature' in simple_fallback_df.columns:\n",
    "            final_selected_features = simple_fallback_df['Feature'].tolist()\n",
    "        else:\n",
    "            final_selected_features = X_train.columns[:max_features].tolist()\n",
    "    return pd.DataFrame({'Feature': final_selected_features})\n",
    "\n",
    "def configure_extended_context(base_context=512, data_length=750):\n",
    "    max_possible_context = int(data_length * 0.7)\n",
    "    extended_contexts = {\n",
    "        'short_term': min(256, max_possible_context, data_length - 60),\n",
    "        'medium_term': min(512, max_possible_context, data_length - 60),\n",
    "        'long_term': min(1024, max_possible_context, data_length - 60),\n",
    "        'adaptive': min(base_context * 2, max_possible_context, data_length - 60)\n",
    "    }\n",
    "    extended_contexts['adaptive'] = max(extended_contexts['adaptive'], 64)\n",
    "    return extended_contexts\n",
    "\n",
    "def add_multitimeframe_features(df, price_col='close'):\n",
    "    df_mtf = df.copy()\n",
    "    if price_col not in df_mtf.columns:\n",
    "        print(f\"Price column '{price_col}' not in DataFrame. Skipping multi-timeframe features.\")\n",
    "        return df_mtf\n",
    "\n",
    "    df_mtf[f'{price_col}_weekly_mean'] = df_mtf[price_col].rolling(5, min_periods=1).mean()\n",
    "    df_mtf[f'{price_col}_weekly_std'] = df_mtf[price_col].rolling(5, min_periods=1).std()\n",
    "    df_mtf[f'{price_col}_weekly_max'] = df_mtf[price_col].rolling(5, min_periods=1).max()\n",
    "    df_mtf[f'{price_col}_weekly_min'] = df_mtf[price_col].rolling(5, min_periods=1).min()\n",
    "\n",
    "    df_mtf[f'{price_col}_monthly_mean'] = df_mtf[price_col].rolling(21, min_periods=1).mean()\n",
    "    df_mtf[f'{price_col}_monthly_std'] = df_mtf[price_col].rolling(21, min_periods=1).std()\n",
    "    df_mtf[f'{price_col}_monthly_trend'] = df_mtf[price_col].rolling(21, min_periods=2).apply(\n",
    "        lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) >= 2 else np.nan, raw=False\n",
    "    )\n",
    "    df_mtf[f'{price_col}_quarterly_mean'] = df_mtf[price_col].rolling(63, min_periods=1).mean()\n",
    "    df_mtf[f'{price_col}_quarterly_volatility'] = df_mtf[price_col].rolling(63, min_periods=1).std() / df_mtf[f'{price_col}_quarterly_mean'].replace(0, np.nan)\n",
    "\n",
    "    df_mtf[f'{price_col}_weekly_monthly_ratio'] = df_mtf[f'{price_col}_weekly_mean'] / df_mtf[f'{price_col}_monthly_mean'].replace(0, np.nan)\n",
    "    df_mtf[f'{price_col}_monthly_quarterly_ratio'] = df_mtf[f'{price_col}_monthly_mean'] / df_mtf[f'{price_col}_quarterly_mean'].replace(0, np.nan)\n",
    "    return df_mtf\n",
    "\n",
    "def detect_volatility_regimes(returns, window=21, threshold_multiplier=1.5):\n",
    "    if returns.empty or len(returns) < window:\n",
    "        return pd.Series(1, index=returns.index)\n",
    "\n",
    "    rolling_vol = returns.rolling(window, min_periods=window // 2 if window // 2 > 0 else 1).std()\n",
    "    if rolling_vol.dropna().empty:\n",
    "        return pd.Series(1, index=returns.index)\n",
    "\n",
    "    vol_median = rolling_vol.median()\n",
    "    if pd.isna(vol_median) or vol_median == 0:\n",
    "        vol_median = rolling_vol.mean()\n",
    "        if pd.isna(vol_median) or vol_median == 0:\n",
    "            return pd.Series(1, index=returns.index)\n",
    "\n",
    "    high_vol_threshold = vol_median * threshold_multiplier\n",
    "    low_vol_threshold = vol_median / threshold_multiplier\n",
    "    regimes = pd.Series(1, index=returns.index, dtype=int)\n",
    "    regimes[rolling_vol >= high_vol_threshold] = 2\n",
    "    regimes[rolling_vol <= low_vol_threshold] = 0\n",
    "    regimes = regimes.ffill().fillna(1) # Updated fillna\n",
    "    return regimes\n",
    "\n",
    "def add_regime_features(df, returns_col='log_returns', price_col='close'):\n",
    "    df_rf = df.copy()\n",
    "    if returns_col not in df_rf.columns:\n",
    "        print(f\"Returns column '{returns_col}' not found. Skipping regime features.\")\n",
    "        return df_rf\n",
    "    if price_col not in df_rf.columns:\n",
    "        print(f\"Price column '{price_col}' not found for regime-adjusted MAs. Skipping those.\")\n",
    "\n",
    "    regimes = detect_volatility_regimes(df_rf[returns_col])\n",
    "    df_rf['volatility_regime'] = regimes\n",
    "    df_rf['regime_0'] = (regimes == 0).astype(int)\n",
    "    df_rf['regime_1'] = (regimes == 1).astype(int)\n",
    "    df_rf['regime_2'] = (regimes == 2).astype(int)\n",
    "    if price_col in df_rf.columns:\n",
    "        for window in [10, 20, 50]:\n",
    "            df_rf[f'sma_{window}_regime_adj'] = df_rf[price_col].rolling(window, min_periods=1).mean() * (1 + 0.1 * regimes)\n",
    "    return df_rf\n",
    "\n",
    "def optimized_lightgbm_params():\n",
    "    return {\n",
    "        'boosting_type': 'gbdt', 'num_leaves': 31, 'learning_rate': 0.05,\n",
    "        'n_estimators': 200, 'feature_fraction': 0.8, 'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5, 'min_child_samples': 20, 'reg_alpha': 0.01,\n",
    "        'reg_lambda': 0.01, 'random_state': 42, 'verbose': -1, 'n_jobs': -1,\n",
    "        'class_weight': 'balanced', 'min_gain_to_split': 0.0\n",
    "    }\n",
    "\n",
    "def enhanced_patchtst_finetune(log_returns, context_length=512, prediction_length=5, min_sequences=20):\n",
    "    if not isinstance(log_returns, np.ndarray): log_returns = np.array(log_returns)\n",
    "    if log_returns.ndim > 1 : log_returns = log_returns.squeeze()\n",
    "\n",
    "    sequence_length_needed = context_length + prediction_length\n",
    "    padded_returns = log_returns.copy()\n",
    "\n",
    "    if len(log_returns) < sequence_length_needed:\n",
    "        mean_return = np.mean(log_returns) if len(log_returns) > 0 else 0\n",
    "        std_return = np.std(log_returns) if len(log_returns) > 1 else 0.01\n",
    "        std_return = max(std_return, 1e-6)\n",
    "        padding_length = sequence_length_needed - len(log_returns)\n",
    "        synthetic_padding = np.random.normal(mean_return, std_return, padding_length)\n",
    "        padded_returns = np.concatenate([synthetic_padding, log_returns])\n",
    "\n",
    "    X_sequences, y_sequences = [], []\n",
    "    if len(padded_returns) >= sequence_length_needed:\n",
    "        for i in range(len(padded_returns) - sequence_length_needed + 1):\n",
    "            seq = padded_returns[i : i + context_length]\n",
    "            target = padded_returns[i + context_length : i + sequence_length_needed]\n",
    "            X_sequences.append(seq)\n",
    "            y_sequences.append(target)\n",
    "\n",
    "    if len(X_sequences) >= min_sequences:\n",
    "        return np.array(X_sequences), np.array(y_sequences), True\n",
    "    else:\n",
    "        return None, None, False\n",
    "\n",
    "def run_patchtst_foundation_forecast(\n",
    "    symbol_name: str,\n",
    "    historical_data_df: pd.DataFrame,\n",
    "    prediction_length: int = 5,\n",
    "    model_checkpoint: str = \"ibm-research/patchtst-etth1-pretrain\",\n",
    "    fine_tune_epochs: int = 10,\n",
    "    enable_fine_tuning: bool = True,\n",
    "    configured_context_length: int | None = None\n",
    "):\n",
    "    print(f\"\\n--- Forecasting for {symbol_name} using PatchTST ({model_checkpoint}) ---\")\n",
    "    if not torch_available or 'PatchTSTForPrediction' not in globals():\n",
    "        print(\"Torch or PatchTST not available. Skipping.\")\n",
    "        return {\"status\": \"failed\", \"reason\": \"Dependencies missing\", \"forecast\": None}\n",
    "    if 'close' not in historical_data_df.columns or historical_data_df['close'].isnull().all():\n",
    "        print(f\"'close' column missing or all NaN for {symbol_name}. Skipping.\")\n",
    "        return {\"status\": \"failed\", \"reason\": \"'close' missing or all NaN\", \"forecast\": None}\n",
    "\n",
    "    df_clean = historical_data_df.dropna(subset=['close'])\n",
    "    if len(df_clean) < 2:\n",
    "        print(f\"Not enough non-NaN close prices ({len(df_clean)}) for {symbol_name}. Skipping.\")\n",
    "        return {\"status\": \"failed\", \"reason\": \"Insufficient non-NaN close prices\", \"forecast\": None}\n",
    "\n",
    "    try:\n",
    "        base_config = PatchTSTConfig.from_pretrained(model_checkpoint)\n",
    "        effective_context_length = configured_context_length if configured_context_length is not None else base_config.context_length\n",
    "        min_data_for_one_sequence = effective_context_length + prediction_length + 1\n",
    "        if len(df_clean) < min_data_for_one_sequence:\n",
    "            print(f\"Data length {len(df_clean)} too short for context {effective_context_length} + pred {prediction_length}. Adjusting context or skipping.\")\n",
    "            effective_context_length = max(10, len(df_clean) - prediction_length - 5)\n",
    "            if effective_context_length < 10 :\n",
    "                return {\"status\": \"failed\", \"reason\": f\"Cannot determine valid context with data {len(df_clean)}\", \"forecast\": None}\n",
    "\n",
    "        financial_config = PatchTSTConfig(\n",
    "            context_length=effective_context_length, prediction_length=prediction_length,\n",
    "            patch_length=min(16, effective_context_length // 2 if effective_context_length > 32 else 8),\n",
    "            patch_stride=min(8, effective_context_length // 4 if effective_context_length > 32 else 4),\n",
    "            num_input_channels=1, d_model=base_config.d_model,\n",
    "            num_attention_heads=base_config.num_attention_heads, num_hidden_layers=base_config.num_hidden_layers,\n",
    "            ffn_dim=base_config.ffn_dim, dropout=0.1, head_dropout=0.1, scaling=\"std\", loss=\"mse\"\n",
    "        )\n",
    "        model = PatchTSTForPrediction(financial_config)\n",
    "\n",
    "        try:\n",
    "            pretrained_model = PatchTSTForPrediction.from_pretrained(model_checkpoint, local_files_only=False, trust_remote_code=True)\n",
    "            pretrained_dict = pretrained_model.state_dict()\n",
    "            model_dict = model.state_dict()\n",
    "            compatible_weights = {}\n",
    "            for k, v in pretrained_dict.items():\n",
    "                if k in model_dict and v.size() == model_dict[k].size():\n",
    "                    if not any(skip_layer in k for skip_layer in ['input_embedding', 'projection', 'head', 'value_embedding', 'patch_embedding.weight', 'patch_embedding.bias']):\n",
    "                        compatible_weights[k] = v\n",
    "            model.load_state_dict(compatible_weights, strict=False)\n",
    "        except Exception as e_load:\n",
    "            print(f\"Weight transfer warning/error: {e_load}. Model may use more random init for some layers.\")\n",
    "\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"; model.to(device)\n",
    "        close_prices = df_clean['close'].values.astype(np.float32)\n",
    "        log_prices = np.log(np.maximum(close_prices, 1e-6))\n",
    "        log_returns = np.diff(log_prices)\n",
    "        if len(log_returns) < financial_config.context_length + financial_config.prediction_length:\n",
    "            print(f\"Insufficient log_returns ({len(log_returns)}) after diff for context/pred. Skipping fine-tune/forecast.\")\n",
    "            return {\"status\": \"failed\", \"reason\": \"insufficient log_returns data\", \"forecast\": None}\n",
    "\n",
    "        finetuned_this_run = False\n",
    "        if enable_fine_tuning and fine_tune_epochs > 0:\n",
    "            X_seqs, y_seqs, finetune_data_ok = enhanced_patchtst_finetune(\n",
    "                log_returns, financial_config.context_length, financial_config.prediction_length)\n",
    "            if finetune_data_ok and X_seqs is not None and len(X_seqs) > 0:\n",
    "                finetuned_this_run = True\n",
    "                train_inputs = torch.tensor(X_seqs, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "                train_targets = torch.tensor(y_seqs, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "                dataset = torch.utils.data.TensorDataset(train_inputs, train_targets)\n",
    "                batch_size = min(16, len(X_seqs) // 2 if len(X_seqs) >= 4 else 1)\n",
    "                if batch_size == 0 and len(X_seqs) > 0: batch_size = 1\n",
    "\n",
    "                if batch_size > 0:\n",
    "                    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True,\n",
    "                                                               drop_last=True if len(X_seqs) > batch_size else False)\n",
    "                    model.train()\n",
    "                    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01) # Using AdamW from torch.optim\n",
    "                    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=fine_tune_epochs) # Using CosineAnnealingLR from torch.optim\n",
    "                    best_loss = float('inf'); patience, patience_counter = 3, 0\n",
    "\n",
    "                    for epoch in range(fine_tune_epochs):\n",
    "                        epoch_loss, num_batches = 0, 0\n",
    "                        if not train_loader: break\n",
    "                        for batch_inputs_data, batch_targets_data in train_loader:\n",
    "                            optimizer.zero_grad()\n",
    "                            outputs = model(past_values=batch_inputs_data, future_values=batch_targets_data)\n",
    "                            loss = outputs.loss\n",
    "                            loss.backward(); torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0); optimizer.step()\n",
    "                            epoch_loss += loss.item(); num_batches += 1\n",
    "                        if num_batches > 0:\n",
    "                            avg_loss = epoch_loss / num_batches; scheduler.step()\n",
    "                            print(f\"Epoch {epoch+1}/{fine_tune_epochs} | Loss: {avg_loss:.6f} | LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "                            if avg_loss < best_loss: best_loss = avg_loss; patience_counter = 0\n",
    "                            else:\n",
    "                                patience_counter += 1\n",
    "                                if patience_counter >= patience: print(f\"Early stopping at epoch {epoch+1}\"); break\n",
    "                        else: print(f\"Epoch {epoch+1}/{fine_tune_epochs} | No batches. Stopping fine-tuning.\"); break\n",
    "                    loss_display = f\"{best_loss:.6f}\" if best_loss != float('inf') else \"N/A\"\n",
    "                    if num_batches > 0: print(f\"Fine-tuning completed. Best loss: {loss_display}\")\n",
    "                else: print(f\"Fine-tuning skipped for {symbol_name}: Not enough sequences or batch_size issue.\"); finetuned_this_run = False\n",
    "\n",
    "        model.eval()\n",
    "        current_model_context_length = model.config.context_length\n",
    "        if len(log_returns) < current_model_context_length:\n",
    "            mean_lr = np.mean(log_returns) if len(log_returns) > 0 else 0\n",
    "            padding_needed = current_model_context_length - len(log_returns)\n",
    "            past_returns_for_forecast = np.concatenate([np.full(padding_needed, mean_lr), log_returns])\n",
    "        else:\n",
    "            past_returns_for_forecast = log_returns[-current_model_context_length:]\n",
    "\n",
    "        returns_mean = np.mean(past_returns_for_forecast); returns_std = max(np.std(past_returns_for_forecast), 1e-8)\n",
    "        norm_returns_forecast_input = (past_returns_for_forecast - returns_mean) / returns_std\n",
    "        past_tensor = torch.tensor(norm_returns_forecast_input, dtype=torch.float32).view(1, current_model_context_length, 1).to(device)\n",
    "\n",
    "        with torch.no_grad(): outputs = model(past_values=past_tensor)\n",
    "        fc_returns_norm = outputs.prediction_outputs.cpu().numpy().squeeze()\n",
    "        if fc_returns_norm.ndim == 0: fc_returns_norm = np.array([fc_returns_norm])\n",
    "        elif fc_returns_norm.ndim > 1: fc_returns_norm = fc_returns_norm.flatten()\n",
    "\n",
    "        target_pred_len = model.config.prediction_length\n",
    "        if len(fc_returns_norm) < target_pred_len:\n",
    "            last_val = fc_returns_norm[-1] if len(fc_returns_norm) > 0 else 0\n",
    "            fc_returns_norm = np.concatenate([fc_returns_norm, np.full(target_pred_len - len(fc_returns_norm), last_val)])\n",
    "        forecast_log_returns = (fc_returns_norm[:target_pred_len] * returns_std) + returns_mean\n",
    "\n",
    "        last_log_price = log_prices[-1]\n",
    "        forecast_log_prices = last_log_price + np.cumsum(forecast_log_returns)\n",
    "        forecast_prices = np.exp(forecast_log_prices)\n",
    "        forecast_prices = np.maximum(forecast_prices, 0.01).tolist()\n",
    "\n",
    "        last_actual_price = close_prices[-1]\n",
    "        price_change = forecast_prices[-1] - last_actual_price\n",
    "        magnitude_pct = (price_change / last_actual_price) * 100 if last_actual_price != 0 else 0\n",
    "        direction = \"ðŸ“ˆ UP\" if price_change > 0.001 * last_actual_price else \"ðŸ“‰ DOWN\" if price_change < -0.001 * last_actual_price else \"íš¡ë³´ HOLD\"\n",
    "        atr_val = np.nan\n",
    "        if 'ATR_14' in df_clean.columns and not df_clean['ATR_14'].empty: atr_val = df_clean['ATR_14'].iloc[-1]\n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\", \"forecast\": forecast_prices, \"last_price\": float(last_actual_price),\n",
    "            \"direction\": direction, \"magnitude\": float(magnitude_pct), \"confidence\": \"ðŸŸ¡ Medium\",\n",
    "            \"atr_threshold\": float(atr_val) if pd.notna(atr_val) else None,\n",
    "            \"method\": f\"PatchTST {'Fine-tuned' if finetuned_this_run else 'Pre-trained'} ({fine_tune_epochs} epochs attempted)\",\n",
    "            \"model_info\": {\"context_length\": model.config.context_length, \"prediction_length\": model.config.prediction_length, \"fine_tuned_actually\": finetuned_this_run}\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in PatchTST forecasting for {symbol_name}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return {\"status\": \"failed\", \"reason\": f\"PatchTST error: {str(e)}\", \"forecast\": None}\n",
    "\n",
    "def simple_ma_fallback(prices, length=5):\n",
    "    if not isinstance(prices, np.ndarray): prices = np.array(prices)\n",
    "    if len(prices) == 0: return np.full(length, np.nan)\n",
    "    if len(prices) < 5: return np.full(length, prices[-1])\n",
    "    ma_val = np.mean(prices[-5:])\n",
    "    return np.full(length, ma_val)\n",
    "\n",
    "\n",
    "\n",
    "def discover_and_rank_causal_features(\n",
    "    df_features: pd.DataFrame, \n",
    "    target_col='target', \n",
    "    price_c='close', # Used to help select initial potential causes\n",
    "    max_features_to_analyze=15, # Max features to select for iterative causal analysis\n",
    "    symbol=\"\"\n",
    ") -> list:\n",
    "    print(f\"\\n--- Causal Feature Discovery and Effect Estimation for {symbol} (Subset Analysis) ---\")\n",
    "    if not dowhy_available or CausalModel is None:\n",
    "        print(\"DoWhy not available. Skipping causal analysis.\")\n",
    "        return []\n",
    "\n",
    "    df_c = df_features.copy()\n",
    "    if target_col not in df_c.columns or df_c[target_col].isnull().all():\n",
    "        print(f\"Target '{target_col}' missing or all NaN. Skipping causal analysis.\")\n",
    "        return []\n",
    "    \n",
    "    df_c[target_col] = pd.to_numeric(df_c[target_col], errors='coerce')\n",
    "\n",
    "    # Identify candidate numeric features\n",
    "    all_numeric_cols = [\n",
    "        col for col in df_c.columns \n",
    "        if pd.api.types.is_numeric_dtype(df_c[col]) and \n",
    "           col != target_col and \n",
    "           df_c[col].notnull().any() and\n",
    "           df_c[col].nunique() > 1 # Ensure feature has some variance\n",
    "    ]\n",
    "    if not all_numeric_cols:\n",
    "        print(\"No suitable numeric features for causal analysis.\")\n",
    "        return []\n",
    "\n",
    "    # Prepare data: Scale numeric features and handle NaNs\n",
    "    df_subset_for_analysis = df_c[all_numeric_cols + [target_col]].copy()\n",
    "    df_subset_for_analysis.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    scaler_causal = StandardScaler()\n",
    "    df_subset_for_analysis[all_numeric_cols] = scaler_causal.fit_transform(df_subset_for_analysis[all_numeric_cols])\n",
    "    df_subset_for_analysis.dropna(inplace=True) # Crucial for DoWhy\n",
    "\n",
    "    if df_subset_for_analysis.empty or df_subset_for_analysis.shape[0] < 20 or df_subset_for_analysis[target_col].nunique() < 1:\n",
    "        print(\"Not enough data or target variation after cleaning for causal analysis.\")\n",
    "        return []\n",
    "\n",
    "    # 1. Select a subset of features for deeper causal analysis (graph_feats)\n",
    "    #    Using a predefined list and a fallback, similar to the old discover_causal_structure\n",
    "    cwt_mean_col = f\"{price_c}_cwt_mean\" if f\"{price_c}_cwt_mean\" in df_subset_for_analysis.columns else f\"close_cwt_mean\"\n",
    "    cwt_std_col = f\"{price_c}_cwt_std\" if f\"{price_c}_cwt_std\" in df_subset_for_analysis.columns else f\"close_cwt_std\"\n",
    "    entropy_sample_col = f\"{price_c}_entropy_sample\" if f\"{price_c}_entropy_sample\" in df_subset_for_analysis.columns else f\"close_entropy_sample\"\n",
    "    \n",
    "    # Curated list of potentially interesting features\n",
    "    potential_causes_list = [\n",
    "        'RSI_14', 'MACDh_12_26_9', 'ADX_14', 'ATR_14', \n",
    "        cwt_mean_col, cwt_std_col, entropy_sample_col, \n",
    "        'regime_simple', 'volatility_20', 'log_returns', \n",
    "        'BBP_2020', 'BBB_2020', # Check actual sanitized names\n",
    "        'close_trans_seq_volatility', 'close_trans_seq_autocorr1' # From conceptual transformer\n",
    "    ]\n",
    "    \n",
    "    # Filter this list to only include features present in the (cleaned, scaled) df_subset_for_analysis\n",
    "    graph_feats_subset = [\n",
    "        c for c in potential_causes_list \n",
    "        if c in df_subset_for_analysis.columns and \n",
    "           c != target_col and \n",
    "           df_subset_for_analysis[c].nunique() > 1\n",
    "    ]\n",
    "\n",
    "    if not graph_feats_subset:\n",
    "        print(\"Predefined potential causes not found or lack variance. Falling back to top varying features.\")\n",
    "        # Fallback: select top N varying features from all_numeric_cols present in df_subset_for_analysis\n",
    "        available_features_for_fallback = [c for c in all_numeric_cols if c in df_subset_for_analysis.columns]\n",
    "        if available_features_for_fallback:\n",
    "            num_to_select = min(max_features_to_analyze, len(available_features_for_fallback))\n",
    "            graph_feats_subset = df_subset_for_analysis[available_features_for_fallback].var().nlargest(num_to_select).index.tolist()\n",
    "        else:\n",
    "            print(\"No features available for fallback selection.\")\n",
    "            return []\n",
    "    else:\n",
    "        # If predefined list yields too many, cap it\n",
    "        if len(graph_feats_subset) > max_features_to_analyze:\n",
    "            print(f\"Capping predefined graph features from {len(graph_feats_subset)} to {max_features_to_analyze}.\")\n",
    "            # This selection could be smarter (e.g. based on some preliminary importance)\n",
    "            # For now, just take the first `max_features_to_analyze`\n",
    "            graph_feats_subset = graph_feats_subset[:max_features_to_analyze]\n",
    "\n",
    "\n",
    "    if not graph_feats_subset:\n",
    "        print(\"No features selected for iterative causal effect estimation.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"Iteratively estimating causal effects for {len(graph_feats_subset)} selected features: {graph_feats_subset}\")\n",
    "\n",
    "    feature_effects = {}\n",
    "    # Data to be used for all models will be the subset containing only the graph_feats_subset and the target\n",
    "    data_for_iterative_models = df_subset_for_analysis[graph_feats_subset + [target_col]].copy()\n",
    "\n",
    "\n",
    "    for treatment_var in graph_feats_subset:\n",
    "        # Define common causes: all other features in graph_feats_subset *excluding* the current treatment_var\n",
    "        common_causes = [f for f in graph_feats_subset if f != treatment_var]\n",
    "        \n",
    "        # Construct a graph string for this specific treatment\n",
    "        # Assumes common causes can affect treatment, and both common causes and treatment can affect outcome\n",
    "        graph_dot_str = \"digraph { \"\n",
    "        graph_dot_str += f'\"{treatment_var}\" -> \"{target_col}\"; ' # Treatment -> Outcome\n",
    "        for cc in common_causes:\n",
    "            graph_dot_str += f'\"{cc}\" -> \"{treatment_var}\"; '    # Common Cause -> Treatment\n",
    "            graph_dot_str += f'\"{cc}\" -> \"{target_col}\"; '       # Common Cause -> Outcome\n",
    "        graph_dot_str += \"}\"\n",
    "        \n",
    "        # print(f\"  Testing {treatment_var} -> {target_col} with graph: {graph_dot_str}\") # Can be verbose\n",
    "\n",
    "        try:\n",
    "            model = CausalModel(\n",
    "                data=data_for_iterative_models, # Use the consistent, cleaned, scaled subset\n",
    "                treatment=treatment_var,\n",
    "                outcome=target_col,\n",
    "                graph=graph_dot_str\n",
    "            )\n",
    "            identified_estimand = model.identify_effect(proceed_when_unidentifiable=True)\n",
    "            estimate = model.estimate_effect(\n",
    "                identified_estimand,\n",
    "                method_name=\"backdoor.linear_regression\", # Simple and relatively fast\n",
    "                test_significance=False,\n",
    "                # No need for force_univariate if graph is well-defined for the backdoor\n",
    "            )\n",
    "            if estimate is not None and hasattr(estimate, 'value') and not np.isnan(estimate.value):\n",
    "                feature_effects[treatment_var] = abs(estimate.value)\n",
    "                # print(f\"    Estimated effect for {treatment_var}: {estimate.value:.4f}\") # Verbose\n",
    "            else:\n",
    "                # print(f\"    Could not estimate effect for {treatment_var} or effect was NaN.\") # Verbose\n",
    "                pass\n",
    "        except Exception as e:\n",
    "            # print(f\"    Error estimating effect for {treatment_var}: {e}\") # Verbose\n",
    "            pass # Continue to the next feature\n",
    "\n",
    "    if not feature_effects:\n",
    "        print(\"Causal effect estimation did not yield any valid effects for the selected feature subset.\")\n",
    "        return []\n",
    "    \n",
    "    # Sort features by the absolute estimated effect strength\n",
    "    sorted_features_by_effect = sorted(feature_effects.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"Causal features ranked by absolute effect strength (top {min(5, len(sorted_features_by_effect))} of {len(sorted_features_by_effect)}):\")\n",
    "    for feat, val in sorted_features_by_effect[:min(5, len(sorted_features_by_effect))]:\n",
    "        print(f\"  {feat}: {val:.4f}\")\n",
    "        \n",
    "    return sorted_features_by_effect # Returns list of (feature_name, abs_effect_value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- MAIN WORKFLOW FUNCTION ---\n",
    "def run_full_workflow(symbol=DEFAULT_SYMBOL, start_date_str=START_DATE, end_date_str=END_DATE,\n",
    "                      api_key_val=API_KEY, # force_train_autoformer is removed\n",
    "                      run_optuna_lgbm=False, use_foundation_model=True,\n",
    "                      enable_smote=True): # Added enable_smote flag\n",
    "    print(f\"\\n{'='*40}\\nðŸš€ ENHANCED WORKFLOW FOR: {symbol}\\n{'='*40}\")\n",
    "    default_return = {\n",
    "        \"symbol\": symbol, \"status\": \"Workflow Started\", \"raw_data_shape\": (0,0),\n",
    "        \"featured_data_shape\": (0,0), \"X_train_shape\": (0,0), \"X_test_shape\": (0,0),\n",
    "        \"selected_features_count\": 0, \"selected_feature_names\": [], \"scaler_object\": None,\n",
    "        \"ml_model_object\": None, \"lgbm_feature_importance\": None,\n",
    "        \"causal_model_object\": None, \"causal_feature_ranking_list\": [], # Changed name for clarity\n",
    "        \"onnx_model_path\": None,\n",
    "        \"forecasting_results\": {\n",
    "            \"configured_context_length\": 512,\n",
    "            \"patchtst_forecast\": None,\n",
    "        },\n",
    "        \"lgbm_optimized_params\": None\n",
    "    }\n",
    "    price_c, target_col = 'close', 'target'\n",
    "\n",
    "    df_raw = fetch_twelve_data(symbol, api_key_val, start_date_str=start_date_str, end_date_str=end_date_str)\n",
    "    if df_raw is None or df_raw.empty:\n",
    "        default_return[\"status\"] = \"Data Fetching Failed\"\n",
    "        print(f\"Workflow aborted for {symbol}: Data Fetching Failed.\")\n",
    "        return default_return\n",
    "    default_return[\"raw_data_shape\"] = df_raw.shape\n",
    "\n",
    "    dynamic_context_length = 512\n",
    "    if not df_raw.empty:\n",
    "        context_configs = configure_extended_context(data_length=len(df_raw))\n",
    "        dynamic_context_length = context_configs.get('adaptive', 512)\n",
    "    default_return[\"forecasting_results\"][\"configured_context_length\"] = dynamic_context_length\n",
    "\n",
    "    print(f\"\\n--- ðŸ”§ Feature Engineering: {symbol} ---\")\n",
    "    df_f = df_raw.copy()\n",
    "    # ... (all your add_feature functions calls)\n",
    "    df_f = add_technical_indicators(df_f)\n",
    "    df_f = add_optimized_features(df_f, price_col=price_c, volume_col='volume')\n",
    "    df_f = add_wavelet_features(df_f, column=price_c)\n",
    "    df_f = add_entropy_features(df_f, column=price_c, window=40)\n",
    "    df_f = add_advanced_technical_features(df_f, price_col=price_c, high_col='high', low_col='low', volume_col='volume')\n",
    "    df_f = add_transformer_features_conceptual(df_f, column=price_c, sequence_length=20)\n",
    "    df_f = add_multitimeframe_features(df_f, price_col=price_c)\n",
    "    if 'log_returns' in df_f.columns:\n",
    "        df_f = add_regime_features(df_f, returns_col='log_returns', price_col=price_c)\n",
    "    else:\n",
    "        print(f\"Skipping regime features for {symbol} as 'log_returns' column is missing.\")\n",
    "    if 'RSI_14' in df_f.columns and 'ADX_14' in df_f.columns:\n",
    "        df_f['RSI_ADX_interaction'] = df_f['RSI_14'] * df_f['ADX_14'] / 100.0\n",
    "    if 'ATR_14' in df_f.columns and 'volatility_20' in df_f.columns:\n",
    "        volatility_safe = df_f['volatility_20'].replace(0, np.nan)\n",
    "        df_f['ATR_vol_ratio'] = df_f['ATR_14'] / volatility_safe\n",
    "    default_return[\"featured_data_shape\"] = df_f.shape\n",
    "\n",
    "    print(f\"\\n--- Data Cleaning (Inf/NaN Handling & Imputation): {symbol} ---\")\n",
    "    df_f.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    numeric_cols_to_impute = df_f.select_dtypes(include=np.number).columns\n",
    "    if not numeric_cols_to_impute.empty:\n",
    "        nan_counts_before = df_f[numeric_cols_to_impute].isnull().sum().sum()\n",
    "        if nan_counts_before > 0:\n",
    "            print(f\"NaNs before imputation: {nan_counts_before}\")\n",
    "            df_f[numeric_cols_to_impute] = df_f[numeric_cols_to_impute].interpolate(method='linear', limit_direction='both', axis=0)\n",
    "            df_f[numeric_cols_to_impute] = df_f[numeric_cols_to_impute].bfill().ffill().fillna(0)\n",
    "            print(f\"NaNs after imputation: {df_f[numeric_cols_to_impute].isnull().sum().sum()}\")\n",
    "    \n",
    "    print(f\"\\n--- ðŸ“Š Regime Detection (Simplified): {symbol} ---\")\n",
    "    df_f = detect_regimes_simple(df_f, column=price_c)\n",
    "    print(f\"\\n--- ðŸŽ¯ Target Definition: {symbol} ---\")\n",
    "    df_f = balanced_target_definition(df_f, column=price_c, periods=5)\n",
    "\n",
    "    # --- Causal Discovery (Conditional) ---\n",
    "    #causal_model_obj = None\n",
    "    # This will store the list of (feature, score) tuples from causal analysis\n",
    "    # For now, score will be a dummy 1.0 if feature is in the causal graph\n",
    "    causal_feature_ranking_for_selection = [] \n",
    "    # discovered_causal_graph_features = []\n",
    "\n",
    "    if not SKIP_CAUSAL_ANALYSIS_FOR_DEBUGGING:\n",
    "        if df_f is not None and not df_f.empty and target_col in df_f.columns and df_f[target_col].nunique(dropna=True) > 1:\n",
    "            # Call the new unified function\n",
    "            causal_feature_ranking_for_selection = discover_and_rank_causal_features(\n",
    "                df_features=df_f.copy(), # Pass the full featured dataframe\n",
    "                target_col=target_col,\n",
    "                price_c=price_c,\n",
    "                max_features_to_analyze=15, # Adjust this to control runtime vs. depth\n",
    "                symbol=symbol\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Skipping Causal Discovery for {symbol} due to data/target issues.\")\n",
    "    else:\n",
    "        print(f\"\\n--- SKIPPING Causal Discovery & Ranking for {symbol} (DEBUG MODE) ---\")\n",
    "    \n",
    "    # Store the ranked list (which might be empty if causal analysis was skipped or failed)\n",
    "    default_return[\"causal_feature_ranking_list\"] = causal_feature_ranking_for_selection\n",
    "\n",
    "\n",
    "    print(f\"\\n--- ML Preparation & Feature Selection: {symbol} ---\")\n",
    "    X_tr, X_te, y_tr, y_te, scaler_obj = None, None, None, None, None\n",
    "    sel_feat_names = []\n",
    "    ml_model = None\n",
    "    lgbm_feat_imp_df = None\n",
    "    onnx_file_path = None\n",
    "    current_status_ml = \"ML Prep Incomplete\"\n",
    "\n",
    "    if df_f is None or df_f.empty or target_col not in df_f.columns or df_f[target_col].isnull().all():\n",
    "        current_status_ml = \"ML Prep Failed - DataFrame empty, target missing, or target all NaN\"\n",
    "        print(f\"{current_status_ml} for {symbol}.\")\n",
    "    elif df_f[target_col].nunique(dropna=True) <= 1:\n",
    "        unique_vals_count = df_f[target_col].nunique(dropna=True)\n",
    "        current_status_ml = f\"ML Prep Skipped - Target has {unique_vals_count} unique non-NaN value(s). Training not meaningful.\"\n",
    "        print(f\"{current_status_ml} for {symbol}.\")\n",
    "    else:\n",
    "        ml_data_prep_output = prepare_ml_data(df_f.copy(), target_col=target_col, test_split_size=0.15, min_test_samples=30)\n",
    "\n",
    "        if ml_data_prep_output is None or not all(item is not None for item in ml_data_prep_output[:4]): # Check X/y sets\n",
    "            current_status_ml = \"ML Data Preparation Failed or returned insufficient data.\"\n",
    "            print(f\"{current_status_ml} for {symbol}. Skipping subsequent ML steps.\")\n",
    "        else:\n",
    "            X_tr, X_te, y_tr, y_te, scaler_obj = ml_data_prep_output\n",
    "            default_return[\"scaler_object\"] = scaler_obj\n",
    "            default_return[\"X_train_shape\"] = X_tr.shape if X_tr is not None else (0,0)\n",
    "            default_return[\"X_test_shape\"] = X_te.shape if X_te is not None else (0,0)\n",
    "\n",
    "            if X_tr is None or X_tr.empty or y_tr is None or y_tr.empty:\n",
    "                current_status_ml = \"ML Training Data (X_tr or y_tr) is empty after preparation. Skipping training.\"\n",
    "                print(f\"{current_status_ml} for {symbol}.\")\n",
    "            else:\n",
    "                # Pass the causal_feature_ranking_for_selection to prioritized_feature_selection\n",
    "                # If causal discovery was skipped, this list will be empty, and MI will be used.\n",
    "                selected_features_df = prioritized_feature_selection(\n",
    "                    X_tr.copy(), \n",
    "                    y_tr.copy(), \n",
    "                    causal_feature_ranking_for_selection, # MODIFIED: Pass the causal ranking\n",
    "                    max_features=35\n",
    "                )\n",
    "                # ... (rest of feature selection fallback logic remains the same) ...\n",
    "                if selected_features_df is not None and 'Feature' in selected_features_df.columns and not selected_features_df.empty:\n",
    "                    sel_feat_names = selected_features_df['Feature'].tolist()\n",
    "                    # ... (fallback logic as before) ...\n",
    "                else: # Fallback if prioritized selection itself fails\n",
    "                    sel_feat_df_fallback = simple_feature_selection_fallback(X_tr.copy(), y_tr.copy(), max_features=20)\n",
    "                    if sel_feat_df_fallback is not None and 'Feature' in sel_feat_df_fallback.columns:\n",
    "                        sel_feat_names = sel_feat_df_fallback['Feature'].tolist()\n",
    "                    else: sel_feat_names = X_tr.columns[:20].tolist() if not X_tr.empty else []\n",
    "                \n",
    "                default_return[\"selected_feature_names\"] = sel_feat_names\n",
    "                default_return[\"selected_features_count\"] = len(sel_feat_names)\n",
    "                print(f\"Selected {len(sel_feat_names)} features: {sel_feat_names[:10]}...\")\n",
    "\n",
    "                if not sel_feat_names:\n",
    "                    current_status_ml = \"No features selected. Skipping LightGBM training.\"\n",
    "                    print(current_status_ml)\n",
    "                else:\n",
    "                    X_tr_selected = X_tr[sel_feat_names].copy()\n",
    "                    X_te_selected = X_te[sel_feat_names].copy() if X_te is not None and not X_te.empty and all(f in X_te.columns for f in sel_feat_names) else pd.DataFrame()\n",
    "\n",
    "                    # --- SMOTE Integration ---\n",
    "                    X_train_for_model = X_tr_selected\n",
    "                    y_train_for_model = y_tr\n",
    "\n",
    "                    if enable_smote and imblearn_available and y_tr.nunique() == 2: # SMOTE for binary classification\n",
    "                        print(f\"Class distribution before SMOTE: \\n{y_tr.value_counts(normalize=True)}\")\n",
    "                        try:\n",
    "                            smote = SMOTE(random_state=42)\n",
    "                            X_train_for_model, y_train_for_model = smote.fit_resample(X_tr_selected, y_tr)\n",
    "                            print(f\"Class distribution after SMOTE: \\n{pd.Series(y_train_for_model).value_counts(normalize=True)}\")\n",
    "                            print(f\"Shape of X_train after SMOTE: {X_train_for_model.shape}\")\n",
    "                        except Exception as e_smote:\n",
    "                            print(f\"Error during SMOTE: {e_smote}. Using original data.\")\n",
    "                            # Fallback to original data if SMOTE fails\n",
    "                            X_train_for_model = X_tr_selected\n",
    "                            y_train_for_model = y_tr\n",
    "                    elif enable_smote and not imblearn_available:\n",
    "                        print(\"SMOTE enabled but imblearn not available. Using original data.\")\n",
    "                    elif enable_smote and y_tr.nunique() != 2:\n",
    "                        print(\"SMOTE enabled but target is not binary. Using original data.\")\n",
    "                    # --- End SMOTE Integration ---\n",
    "\n",
    "                    num_classes_for_optuna = y_train_for_model.nunique() # Use y_train_for_model\n",
    "                    optuna_base_params = optimized_lightgbm_params() \n",
    "                    optuna_base_params['objective'] = 'multiclass' if num_classes_for_optuna > 2 else 'binary'\n",
    "                    optuna_base_params['metric'] = 'multi_logloss' if num_classes_for_optuna > 2 else 'binary_logloss'\n",
    "                    if num_classes_for_optuna > 2:\n",
    "                        optuna_base_params['num_class'] = num_classes_for_optuna\n",
    "                    elif 'num_class' in optuna_base_params:\n",
    "                        del optuna_base_params['num_class']\n",
    "                    \n",
    "                    lgbm_final_params = optuna_base_params.copy()\n",
    "\n",
    "                    if run_optuna_lgbm and optuna_available:\n",
    "                        print(f\"\\n--- Hyperparameter Optimization (Optuna for LightGBM): {symbol} ---\")\n",
    "                        tuned_params_from_optuna = optimize_lgbm_hyperparameters(\n",
    "                            X_train_for_model.copy(), # Use SMOTE'd or original data\n",
    "                            y_train_for_model.copy(), \n",
    "                            optuna_base_params, \n",
    "                            n_trials=50\n",
    "                        )\n",
    "                        if tuned_params_from_optuna:\n",
    "                           lgbm_final_params.update(tuned_params_from_optuna)\n",
    "                        default_return[\"lgbm_optimized_params\"] = lgbm_final_params\n",
    "                    else:\n",
    "                         print(\"Optuna HPO for LightGBM skipped or Optuna not available.\")\n",
    "                         default_return[\"lgbm_optimized_params\"] = lgbm_final_params\n",
    "\n",
    "                    print(f\"\\n--- LightGBM Model Training: {symbol} ---\")\n",
    "                    ml_model, lgbm_feat_imp_df = train_lightgbm_model(\n",
    "                        X_train_for_model, # Use SMOTE'd or original data\n",
    "                        y_train_for_model, \n",
    "                        X_te_selected, \n",
    "                        y_te, \n",
    "                        optimized_params=lgbm_final_params\n",
    "                    )\n",
    "                    # ... (rest of ML model handling)\n",
    "                    default_return[\"ml_model_object\"] = ml_model\n",
    "                    default_return[\"lgbm_feature_importance\"] = lgbm_feat_imp_df\n",
    "                    if ml_model:\n",
    "                        current_status_ml = \"LightGBM Model Trained\"\n",
    "                        plot_feature_importance(lgbm_feat_imp_df, top_n=20, symbol_for_plot=symbol)\n",
    "                        if not X_train_for_model.empty: # Use X_train_for_model for sample\n",
    "                             onnx_file_path = export_lgbm_to_onnx(ml_model, X_train_for_model.head(1), file_path=f\"lgbm_model_{symbol}.onnx\")\n",
    "                             default_return[\"onnx_model_path\"] = onnx_file_path\n",
    "                        else:\n",
    "                            print(\"X_train_for_model is empty. Skipping ONNX export.\")\n",
    "                    else:\n",
    "                        current_status_ml = \"LightGBM Model Training Failed.\"\n",
    "                        print(current_status_ml)\n",
    "    default_return[\"status\"] = current_status_ml\n",
    "    \n",
    "    # ... (PatchTST forecasting section remains the same) ...\n",
    "    if use_foundation_model and torch_available and 'PatchTSTForPrediction' in globals():\n",
    "        print(f\"\\n--- Foundation Model Forecasting (PatchTST): {symbol} ---\")\n",
    "        # ... (PatchTST call) ...\n",
    "    else:\n",
    "        default_return[\"forecasting_results\"][\"patchtst_forecast\"] = {\"status\": \"skipped\", \"reason\": \"Disabled or dependencies missing\"}\n",
    "\n",
    "    print(f\"\\nðŸ Workflow completed for {symbol}. Final Status: {default_return['status']}\")\n",
    "    return default_return\n",
    "\n",
    "# --- Example Usage and Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"DEBUG: Script execution started, entering __main__ block.\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    current_api_key = API_KEY \n",
    "\n",
    "    if current_api_key == \"YOUR_API_KEY_HERE\" or not current_api_key:\n",
    "        print(\"ðŸ›‘ CRITICAL: TWELVE_DATA_API_KEY is not set. Update 'YOUR_API_KEY_HERE' in the script constants or set API_KEY directly.\")\n",
    "        print(\"ðŸ›‘ CRITICAL: Workflow cannot proceed without a valid API key.\")\n",
    "    else:\n",
    "        print(f\"DEBUG: API_KEY appears to be set. Proceeding. Key ends with '...{current_api_key[-4:] if len(current_api_key)>4 else current_api_key}'\")\n",
    "\n",
    "    symbols_to_run = [\"AAPL\", \"GOOGL\"]\n",
    "    print(f\"DEBUG: Symbols to process: {symbols_to_run}\")\n",
    "    all_results_dict = {}\n",
    "\n",
    "    if current_api_key == \"YOUR_API_KEY_HERE\" or not current_api_key:\n",
    "        print(\"DEBUG: Halting before loop due to missing API Key.\")\n",
    "    else:\n",
    "        for sym_item in symbols_to_run:\n",
    "            print(f\"DEBUG: Starting main loop for symbol: {sym_item}\")\n",
    "            try:\n",
    "                workflow_output = run_full_workflow(\n",
    "                    symbol=sym_item,\n",
    "                    api_key_val=current_api_key, \n",
    "                    run_optuna_lgbm=True, \n",
    "                    use_foundation_model=True,\n",
    "                    enable_smote=True # SMOTE is now enabled by default\n",
    "                )\n",
    "                # ... (rest of the main loop for printing results) ...\n",
    "                all_results_dict[sym_item] = workflow_output\n",
    "                print(f\"\\n--- Results Summary for {sym_item} ---\")\n",
    "                if workflow_output:\n",
    "                    print(f\"  Overall Status: {workflow_output.get('status')}\")\n",
    "                    # ... (other print statements for summary) ...\n",
    "                    lgbm_feat_imp = workflow_output.get(\"lgbm_feature_importance\")\n",
    "                    if lgbm_feat_imp is not None and not lgbm_feat_imp.empty:\n",
    "                        print(f\"  Top LGBM Features: {lgbm_feat_imp['Feature'].head(3).tolist()}\")\n",
    "                    forecast_summary = workflow_output.get(\"forecasting_results\", {})\n",
    "                    patchtst_info = forecast_summary.get(\"patchtst_forecast\")\n",
    "                    if patchtst_info and isinstance(patchtst_info, dict) and patchtst_info.get(\"status\") == \"success\":\n",
    "                        print(f\"  PatchTST Forecast ({patchtst_info.get('method', 'N/A')}):\")\n",
    "                        print(f\"    Values: {patchtst_info.get('forecast')}\")\n",
    "                        print(f\"    Direction: {patchtst_info.get('direction')}, Magnitude: {patchtst_info.get('magnitude', 0):.2f}%\")\n",
    "                    elif patchtst_info and isinstance(patchtst_info, dict):\n",
    "                        print(f\"  PatchTST Status: {patchtst_info.get('status')}, Reason: {patchtst_info.get('reason')}\")\n",
    "                    print(f\"  ONNX Model Path: {workflow_output.get('onnx_model_path')}\")\n",
    "                print(\"-\" * 40)\n",
    "\n",
    "            except Exception as e_main_loop:\n",
    "                print(f\"ðŸ›‘ ERROR: Unhandled exception in main loop for symbol {sym_item}: {e_main_loop}\")\n",
    "                traceback.print_exc()\n",
    "                all_results_dict[sym_item] = {\"status\": f\"Error: {e_main_loop}\", \"symbol\": sym_item}\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time for {len(symbols_to_run)} symbol(s): {(end_time - start_time):.2f} seconds.\")\n",
    "    print(\"DEBUG: Script __main__ block finished.\")\n",
    "\n",
    "\n",
    "# --- Example Usage and Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"DEBUG: Script execution started, entering __main__ block.\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    current_api_key = API_KEY \n",
    "\n",
    "    if current_api_key == \"YOUR_API_KEY_HERE\" or not current_api_key:\n",
    "        print(\"ðŸ›‘ CRITICAL: TWELVE_DATA_API_KEY is not set. Update 'YOUR_API_KEY_HERE' in the script constants or set API_KEY directly.\")\n",
    "        print(\"ðŸ›‘ CRITICAL: Workflow cannot proceed without a valid API key.\")\n",
    "    else:\n",
    "        print(f\"DEBUG: API_KEY appears to be set. Proceeding. Key ends with '...{current_api_key[-4:] if len(current_api_key)>4 else current_api_key}'\")\n",
    "\n",
    "    symbols_to_run = [\"AAPL\", \"GOOGL\"]\n",
    "    print(f\"DEBUG: Symbols to process: {symbols_to_run}\")\n",
    "    all_results_dict = {}\n",
    "\n",
    "    if current_api_key == \"YOUR_API_KEY_HERE\" or not current_api_key:\n",
    "        print(\"DEBUG: Halting before loop due to missing API Key.\")\n",
    "    else:\n",
    "        for sym_item in symbols_to_run:\n",
    "            print(f\"DEBUG: Starting main loop for symbol: {sym_item}\")\n",
    "            try:\n",
    "                workflow_output = run_full_workflow(\n",
    "                    symbol=sym_item,\n",
    "                    api_key_val=current_api_key, \n",
    "                    run_optuna_lgbm=True, \n",
    "                    # force_train_autoformer flag removed\n",
    "                    use_foundation_model=True \n",
    "                )\n",
    "                all_results_dict[sym_item] = workflow_output\n",
    "                print(f\"\\n--- Results Summary for {sym_item} ---\")\n",
    "                if workflow_output:\n",
    "                    print(f\"  Overall Status: {workflow_output.get('status')}\")\n",
    "                    print(f\"  Raw Data Shape: {workflow_output.get('raw_data_shape')}\")\n",
    "                    print(f\"  Featured Data Shape: {workflow_output.get('featured_data_shape')}\")\n",
    "                    print(f\"  Selected Features Count: {workflow_output.get('selected_features_count')}\")\n",
    "                    \n",
    "                    lgbm_feat_imp = workflow_output.get(\"lgbm_feature_importance\")\n",
    "                    if lgbm_feat_imp is not None and not lgbm_feat_imp.empty:\n",
    "                        print(f\"  Top LGBM Features: {lgbm_feat_imp['Feature'].head(3).tolist()}\")\n",
    "                    \n",
    "                    forecast_summary = workflow_output.get(\"forecasting_results\", {})\n",
    "                    patchtst_info = forecast_summary.get(\"patchtst_forecast\")\n",
    "                    # autoformer_info retrieval removed\n",
    "                    \n",
    "                    if patchtst_info and isinstance(patchtst_info, dict) and patchtst_info.get(\"status\") == \"success\":\n",
    "                        print(f\"  PatchTST Forecast ({patchtst_info.get('method', 'N/A')}):\")\n",
    "                        print(f\"    Values: {patchtst_info.get('forecast')}\")\n",
    "                        print(f\"    Direction: {patchtst_info.get('direction')}, Magnitude: {patchtst_info.get('magnitude', 0):.2f}%\")\n",
    "                    elif patchtst_info and isinstance(patchtst_info, dict):\n",
    "                        print(f\"  PatchTST Status: {patchtst_info.get('status')}, Reason: {patchtst_info.get('reason')}\")\n",
    "                    \n",
    "                    # Autoformer forecast printing removed\n",
    "                    \n",
    "                    print(f\"  ONNX Model Path: {workflow_output.get('onnx_model_path')}\")\n",
    "                print(\"-\" * 40)\n",
    "\n",
    "            except Exception as e_main_loop:\n",
    "                print(f\"ðŸ›‘ ERROR: Unhandled exception in main loop for symbol {sym_item}: {e_main_loop}\")\n",
    "                traceback.print_exc()\n",
    "                all_results_dict[sym_item] = {\"status\": f\"Error: {e_main_loop}\", \"symbol\": sym_item}\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time for {len(symbols_to_run)} symbol(s): {(end_time - start_time):.2f} seconds.\")\n",
    "    print(\"DEBUG: Script __main__ block finished.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b270eb9-6739-4a75-a30c-0784aebfc56b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Stock Analysis Py310",
   "language": "python",
   "name": "stock_py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
