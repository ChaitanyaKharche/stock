{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e782b400-b23d-49d7-a4fd-00e502a14b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna imported successfully.\n",
      "PyTorch imported successfully.\n",
      "PyTorch CUDA available: True, Version: 12.1\n",
      "Using PyTorch on GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "DoWhy 0.10 and NetworkX 3.1 imported successfully.\n",
      "ONNX, ONNXRuntime, skl2onnx, and onnxmltools imported successfully.\n",
      "Onnxmltools version: 1.11.1\n",
      "\n",
      "All libraries and modules conditional imports attempted.\n",
      "DEBUG: Script execution started, entering __main__ block.\n",
      "DEBUG: API_KEY appears to be set. Proceeding. Key ends with '...ef0a'\n",
      "DEBUG: Symbols to process: ['AAPL', 'GOOGL']\n",
      "DEBUG: Starting main loop for symbol: AAPL\n",
      "\n",
      "========================================\n",
      "🚀 ENHANCED WORKFLOW FOR: AAPL\n",
      "========================================\n",
      "Fetching data for AAPL from Twelve Data (interval=1day, from 2022-06-04 to 2025-06-03)...\n",
      "Successfully fetched/processed 750 data points for AAPL.\n",
      "\n",
      "--- 🔧 Feature Engineering: AAPL ---\n",
      "Adding technical indicators...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35088/1678251532.py:355: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[9.98611407e+09 8.00782855e+09 1.23548189e+10 1.76871775e+10\n",
      " 1.09688179e+10 9.95967086e+09 9.95940605e+09 1.25755687e+10\n",
      " 9.97233035e+09 9.79635737e+09 1.02719769e+10 1.05694694e+10\n",
      " 9.63284651e+09 9.45818898e+09 1.13631385e+10 1.14780874e+10\n",
      " 1.14194116e+10 1.24235717e+10 9.87757346e+09 1.00419950e+10\n",
      " 1.03075722e+10 1.22197531e+10 1.27330542e+10 1.64772630e+10\n",
      " 1.09881365e+10 1.35716587e+10 9.19816034e+09 9.98346202e+09\n",
      " 1.18232614e+10 9.67097850e+09 1.16499329e+10 9.33857157e+09\n",
      " 9.74196305e+09 1.38739808e+10 8.68182651e+09 1.20859660e+10\n",
      " 1.35908341e+10 1.06564850e+10 1.70375068e+10 1.24421969e+10\n",
      " 1.68017624e+10 1.41309629e+10 1.28473936e+10 1.61259518e+10\n",
      " 1.27816217e+10 1.15707123e+10 9.99105288e+09 1.58872146e+10\n",
      " 1.24359106e+10 1.20936924e+10 1.42446076e+10 9.28966227e+09\n",
      " 1.26294602e+10 1.12878711e+10 1.13142214e+10 2.53217851e+10\n",
      " 1.25398382e+10 1.71641023e+10 1.39115016e+10 1.09080380e+10\n",
      " 1.35458486e+10 1.20141956e+10 1.13232148e+10 7.72796261e+09\n",
      " 8.78874185e+09 1.62356225e+10 1.05462467e+10 1.01707065e+10\n",
      " 8.84851958e+09 1.08724857e+10 1.01008282e+10 1.37599618e+10\n",
      " 1.16005664e+10 9.78646964e+09 1.12560026e+10 9.28421318e+09\n",
      " 9.20237836e+09 9.49610381e+09 7.73378147e+09 8.64235945e+09\n",
      " 9.49524164e+09 1.09668336e+10 1.15099425e+10 9.43356275e+09\n",
      " 7.75704819e+09 1.02597539e+10 1.12187659e+10 1.77575559e+10\n",
      " 2.36531981e+10 1.27905743e+10 9.51706899e+09 1.01357423e+10\n",
      " 7.21041317e+09 6.66787773e+09 1.05973489e+10 1.35303380e+10\n",
      " 7.20927441e+09 1.27054446e+10 1.12039877e+10 1.17336367e+10\n",
      " 1.17783146e+10 1.53700545e+10 1.15216877e+10 1.17123191e+10\n",
      " 1.20562687e+10 1.07780607e+10 9.44065802e+09 8.22892145e+09\n",
      " 8.02092546e+09 1.12706128e+10 9.43297948e+09 7.67571884e+09\n",
      " 7.43822714e+09 1.12428743e+10 8.15453475e+09 8.31267856e+09\n",
      " 7.97452246e+09 1.08637797e+10 9.33790006e+09 8.89730492e+09\n",
      " 1.95897615e+10 9.69014555e+09 9.30311330e+09 8.59096550e+09\n",
      " 7.25714883e+09 1.14154201e+10 9.78787029e+09 9.69023774e+09\n",
      " 9.58351277e+09 9.94039710e+09 1.77120500e+10 1.23366373e+10\n",
      " 1.11913526e+10 2.20548372e+10 9.01976806e+09 8.85895485e+09\n",
      " 9.92597445e+09 1.00685038e+10 1.05417606e+10 1.21337144e+10\n",
      " 9.52684296e+09 9.89486590e+09 9.50559906e+09 9.67570713e+09\n",
      " 8.78435837e+09 1.64385657e+10 8.94379767e+09 1.15413230e+10\n",
      " 7.87584582e+09 9.76925981e+09 9.35613794e+09 1.57252226e+10\n",
      " 8.77168000e+09 7.22026451e+09 9.23265175e+09 9.44257841e+09\n",
      " 7.61211187e+09 9.77655958e+09 7.81007094e+09 8.11554314e+09\n",
      " 7.45114775e+09 9.50767827e+09 9.15082586e+09 7.87793938e+09\n",
      " 9.69123290e+09 1.13567933e+10 1.14388905e+10 8.65352272e+09\n",
      " 1.17164206e+10 1.05526049e+10 1.19610916e+10 9.25680273e+09\n",
      " 9.94434069e+09 8.11408236e+09 8.88820816e+09 9.02411056e+09\n",
      " 9.16968433e+09 8.46067744e+09 1.00840311e+10 7.54330174e+09\n",
      " 7.80824996e+09 8.51560981e+09 1.02532891e+10 7.57384537e+09\n",
      " 9.82804399e+09 8.69789424e+09 9.83683246e+09 1.36833622e+10\n",
      " 1.13822654e+10 1.27713375e+10 9.01167686e+09 9.82689253e+09\n",
      " 1.22677457e+10 1.12573015e+10 1.01357438e+10 1.03261176e+10\n",
      " 8.88619451e+09 7.59431678e+09 7.31019429e+09 8.70997681e+09\n",
      " 1.28370713e+10 7.93291445e+09 9.22428407e+09 1.04151571e+10\n",
      " 1.02082680e+10 1.38651689e+10 1.32302063e+10 8.00412063e+09\n",
      " 6.59898412e+09 1.08956000e+10 7.89745727e+09 8.68023877e+09\n",
      " 7.52069210e+09 1.46546896e+10 1.31141791e+10 1.16555532e+10\n",
      " 8.25595341e+09 1.04620365e+10 1.20606821e+10 1.30691700e+10\n",
      " 8.19627617e+09 1.01368440e+10 8.53192482e+09 7.55660592e+09\n",
      " 9.61810469e+09 9.88803149e+09 1.30249841e+10 1.04077419e+10\n",
      " 1.26227852e+10 1.32294752e+10 9.67591167e+09 9.48135364e+09\n",
      " 1.03875007e+10 8.09026070e+09 9.12187171e+09 7.18981841e+09\n",
      " 1.57448407e+10 1.79183493e+10 7.98523720e+09 8.23764065e+09\n",
      " 8.11482711e+09 8.57215960e+09 7.61983755e+09 1.18757914e+10\n",
      " 1.62470454e+10 3.00898188e+10 1.41371691e+10 9.00209590e+09\n",
      " 1.34003330e+10 9.81541667e+09 1.33229651e+10 1.00511572e+10\n",
      " 8.45777740e+09 8.12340494e+09 6.87903644e+09 9.96964115e+09\n",
      " 9.55836280e+09 1.43985768e+10 9.70793814e+09 9.22052856e+09\n",
      " 1.06082338e+10 1.04078334e+10 3.49308741e+10 4.22798781e+10\n",
      " 2.09618648e+10 2.02559353e+10 1.68835139e+10 1.18921488e+10\n",
      " 1.40978938e+10 1.06545106e+10 1.30103818e+10 1.26879321e+10\n",
      " 8.24788855e+09 1.35815128e+10 1.33715335e+10 1.09685619e+10\n",
      " 1.45152221e+10 1.22335835e+10 1.47125096e+10 1.08426737e+10\n",
      " 9.06159537e+09 9.09569998e+09 1.11168985e+10 2.33366435e+10\n",
      " 1.33352684e+10 1.00034981e+10 9.07320347e+09 8.27288639e+09\n",
      " 9.74610492e+09 9.29357354e+09 1.04095772e+10 9.99915840e+09\n",
      " 6.85898652e+09 7.87357447e+09 8.75899028e+09 8.16637752e+09\n",
      " 8.65091399e+09 1.19659592e+10 8.16987215e+09 9.86326096e+09\n",
      " 8.32635044e+09 8.19221117e+09 9.83505139e+09 1.31956451e+10\n",
      " 1.52109995e+10 7.31805895e+10 9.90828696e+09 8.32178216e+09\n",
      " 7.76484651e+09 1.26473574e+10 8.42893858e+09 7.16753250e+09\n",
      " 7.66011433e+09 6.44190253e+09 9.19455241e+09 1.51887363e+10\n",
      " 7.66028053e+09 1.09143429e+10 8.55340346e+09 8.97885235e+09\n",
      " 8.42850296e+09 6.26450950e+09 1.21849487e+10 9.55022893e+09\n",
      " 8.71379728e+09 9.06510920e+09 1.09197860e+10 1.02140477e+10\n",
      " 1.01724702e+10 8.26956806e+09 8.76153324e+09 2.09112428e+10\n",
      " 1.07908152e+10 7.86575919e+09 6.73214852e+09 1.15134314e+10\n",
      " 9.38246991e+09 1.07759670e+10 9.73771216e+09 1.09500676e+10\n",
      " 9.12189330e+09 1.12042575e+10 8.21875680e+09 1.29249059e+10\n",
      " 1.29604723e+10 3.71285007e+10 1.04104135e+10 5.97673925e+09\n",
      " 7.05213913e+09 1.10441301e+10 9.22615134e+09 9.44363611e+09\n",
      " 1.42732762e+10 1.35124727e+10 2.16917809e+10 1.78987905e+10\n",
      " 1.08140170e+10 1.32761743e+10 2.41444830e+10 1.04042367e+10\n",
      " 9.15770407e+09 6.95713340e+09 1.24623107e+10 1.06247858e+10\n",
      " 1.28573762e+10 9.96694950e+09 1.19043900e+10 7.87972360e+09\n",
      " 7.93810776e+09 1.31124498e+10 1.26670135e+10 1.18744892e+10\n",
      " 1.12953777e+10 1.06353707e+10 1.10316262e+10 1.27632480e+10\n",
      " 1.02426788e+10 9.04276876e+09 1.17427143e+10 2.03438414e+10\n",
      " 9.75755554e+09 7.67977022e+09 8.27708662e+09 1.44524764e+10\n",
      " 8.08082752e+09 8.02018124e+09 3.51181998e+10 1.70135682e+10\n",
      " 2.08320751e+10 1.00996813e+10 1.05476014e+10 1.08575434e+10\n",
      " 9.77981451e+09 7.96590954e+09 8.12442820e+09 7.75613832e+09\n",
      " 1.10280943e+10 1.21764435e+10 9.96487144e+09 7.24958685e+09\n",
      " 1.33676201e+10 1.09931782e+10 1.04710452e+10 1.12273044e+10\n",
      " 9.11403521e+09 7.11515508e+09]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df_feat.ta.mfi(high=df_feat[h], low=df_feat[l], close=df_feat[c], volume=df_feat[v], append=True)\n",
      "/tmp/ipykernel_35088/1678251532.py:355: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[1.00300486e+10 1.26473852e+10 1.62339912e+10 1.12517242e+10\n",
      " 1.41098082e+10 9.35097896e+09 9.19819886e+09 1.34875418e+10\n",
      " 9.16163569e+09 1.02996074e+10 1.20866617e+10 8.22715736e+09\n",
      " 8.37170893e+09 9.62605634e+09 9.33988618e+09 1.03961021e+10\n",
      " 1.08420068e+10 1.21129296e+10 1.16096177e+10 9.07126366e+09\n",
      " 9.00750348e+09 1.31135798e+10 1.18304358e+10 1.24439573e+10\n",
      " 1.39301643e+10 1.16553317e+10 1.14334111e+10 1.31207271e+10\n",
      " 1.91242078e+10 1.36645455e+10 1.38433070e+10 2.43645396e+10\n",
      " 1.57989375e+10 1.32324147e+10 1.44192093e+10 2.17748714e+10\n",
      " 1.83613494e+10 1.74603932e+10 1.21051987e+10 1.05078306e+10\n",
      " 1.07474853e+10 9.78699739e+09 8.85840469e+09 1.32109186e+10\n",
      " 1.59395814e+10 1.50016338e+10 1.21970226e+10 1.37973180e+10\n",
      " 1.37226250e+10 1.94995641e+10 1.14984687e+10 1.01890705e+10\n",
      " 9.54604987e+09 8.73227356e+09 5.21028372e+09 1.00231277e+10\n",
      " 1.19036739e+10 9.63080285e+09 9.32352165e+09 9.86111998e+09\n",
      " 1.18233256e+10 1.36634847e+10 2.16686936e+10 1.05828977e+10\n",
      " 1.02066755e+10 1.03047635e+10 8.37932180e+09 8.97469439e+09\n",
      " 1.09059270e+10 9.94537174e+09 1.42067818e+10 1.12906932e+10\n",
      " 1.01891557e+10 8.30884865e+09 7.87348777e+09 9.27989439e+09\n",
      " 9.20540035e+09 9.46199338e+09 1.06093803e+10 9.78194872e+09\n",
      " 8.50598497e+09 8.64767076e+09 9.41719684e+09 1.05294415e+10\n",
      " 8.99778928e+09 8.79456346e+09 7.58402001e+09 8.12851537e+09\n",
      " 7.46949871e+09 8.09161205e+09 7.60099180e+09 8.55389694e+09\n",
      " 8.17122252e+09 1.02184305e+10 8.32866063e+09 7.23794582e+09\n",
      " 8.42405159e+09 7.70055332e+09 7.68135658e+09 8.05371554e+09\n",
      " 6.84532943e+09 8.74413200e+09 9.64432271e+09 6.91916716e+09\n",
      " 8.01850324e+09 7.45937645e+09 8.17474226e+09 1.09760674e+10\n",
      " 1.34617885e+10 7.80997596e+09 7.85034219e+09 6.41918929e+09\n",
      " 9.96844769e+09 7.58656525e+09 8.73225707e+09 7.74632364e+09\n",
      " 1.16026757e+10 1.10746627e+10 1.87684057e+10 9.22146542e+09\n",
      " 9.11022876e+09 8.95331451e+09 6.06199514e+09 8.99196269e+09\n",
      " 8.61346230e+09 1.12981311e+10 8.76765207e+09 7.91982781e+09\n",
      " 1.15607388e+10 1.38595190e+10 9.22262426e+09 6.88986409e+09\n",
      " 9.73538072e+09 1.17210296e+10 2.12797393e+10 1.75419067e+10\n",
      " 1.21550120e+10 1.07902414e+10 9.23585909e+09 7.76453203e+09\n",
      " 8.32233683e+09 1.15607871e+10 1.06247043e+10 9.77101170e+09\n",
      " 8.56229707e+09 1.50765874e+10 1.98471489e+10 1.60030566e+10\n",
      " 1.47606465e+10 1.06652908e+10 1.91229052e+10 1.03352211e+10\n",
      " 1.10142578e+10 1.11700046e+10 1.14329200e+10 9.57642801e+09\n",
      " 8.54432300e+09 9.23836424e+09 9.35345262e+09 1.01741582e+10\n",
      " 9.64820944e+09 1.04469967e+10 1.11462580e+10 9.64611734e+09\n",
      " 9.80833169e+09 1.18636379e+10 7.61739177e+09 1.40070377e+10\n",
      " 8.07166933e+09 9.65222821e+09 7.26970698e+09 4.57013885e+09\n",
      " 7.69634251e+09 8.17886331e+09 9.24588837e+09 8.19960298e+09\n",
      " 1.17436478e+10 2.53507918e+10 1.09071159e+10 1.02280952e+10\n",
      " 9.06811586e+09 7.20157584e+09 5.58884755e+09 9.26072904e+09\n",
      " 8.22252699e+09 1.53420733e+10 1.07786433e+10 1.30981358e+10\n",
      " 1.12999491e+10 9.10938798e+09 1.20014124e+10 8.61034696e+09\n",
      " 1.06638122e+10 8.60842561e+09 9.01282435e+09 1.05631986e+10\n",
      " 1.02772591e+10 1.88780066e+10 7.71737748e+09 7.83563797e+09\n",
      " 1.04534350e+10 1.00541970e+10 1.19897820e+10 9.09253995e+09\n",
      " 9.73118307e+09 8.26878652e+09 7.41838640e+09 8.88790690e+09\n",
      " 2.47327169e+10 1.31683147e+10 1.42857316e+10 1.62289530e+10\n",
      " 1.16379609e+10 1.21574866e+10 1.03352366e+10 9.01195969e+09\n",
      " 2.09072672e+10 1.83941409e+10 1.22158769e+10 9.26954644e+09\n",
      " 9.76961011e+09 1.12567384e+10 7.87259924e+09 8.32698403e+09\n",
      " 7.13424413e+09 6.31106219e+09 8.35083094e+09 1.27901213e+10\n",
      " 1.25655480e+10 8.59636416e+09 7.21920459e+09 1.11932399e+10\n",
      " 1.13258406e+10 8.58393686e+09 1.43083283e+10 8.21931644e+09\n",
      " 9.31047667e+09 7.84182433e+09 6.62929750e+09 9.59778020e+09\n",
      " 1.01191828e+10 8.03251009e+09 1.88873269e+10 1.49352004e+10\n",
      " 1.72123252e+10 1.81757912e+10 5.14601818e+10 1.75262917e+10\n",
      " 1.47913335e+10 1.01372906e+10 1.31318290e+10 1.48993866e+10\n",
      " 1.10490934e+10 8.98613886e+09 1.36007503e+10 1.11854663e+10\n",
      " 7.90737873e+09 1.37473034e+10 2.46576957e+10 1.43567972e+10\n",
      " 9.15367003e+09 9.85722258e+09 6.91966735e+09 1.21327734e+10\n",
      " 1.12584208e+10 9.64645162e+09 1.07471624e+10 1.47547436e+10\n",
      " 1.13211290e+10 1.28105143e+10 1.23043175e+10 9.55725319e+09\n",
      " 1.43361622e+10 7.42191868e+09 7.66902357e+09 8.80588534e+09\n",
      " 7.24427352e+09 7.88139514e+09 9.12480749e+09 1.20897998e+10\n",
      " 7.15311124e+09 8.26779129e+09 1.08752609e+10 1.46143729e+10\n",
      " 1.45464776e+10 9.95535648e+09 9.40127334e+09 1.08018422e+10\n",
      " 8.02828171e+09 9.60616662e+09 8.96631543e+09 8.11112026e+09\n",
      " 1.41948500e+10 1.51978453e+10 1.08337799e+10 8.96582362e+09\n",
      " 9.91136640e+09 1.36521986e+10 9.78508449e+09 9.92950633e+09\n",
      " 9.11196373e+09 1.46051189e+10 1.15604788e+10 1.66074473e+10\n",
      " 1.57694308e+10 2.17860125e+10 1.22125825e+10 1.66959381e+10\n",
      " 9.11796420e+09 7.56625218e+09 1.07302701e+10 9.83324129e+09\n",
      " 1.35288119e+10 1.27444110e+10 1.10437021e+10 1.65249652e+10\n",
      " 1.68550438e+10 1.36278493e+10 1.29883959e+10 1.04861415e+10\n",
      " 7.66319272e+09 8.75199529e+09 2.10950632e+10 2.41581047e+10\n",
      " 2.94311241e+10 2.14312098e+10 2.30840720e+10 1.03620546e+10\n",
      " 1.16944093e+10 8.98655724e+09 2.06903386e+10 1.38313079e+10\n",
      " 1.01780841e+10 1.34546680e+10 9.51548384e+09 1.15606445e+10\n",
      " 9.57446120e+09 8.78773024e+09 1.20365747e+10 9.40784285e+09\n",
      " 1.53318021e+10 1.03182501e+10 1.41543018e+10]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df_feat.ta.mfi(high=df_feat[h], low=df_feat[l], close=df_feat[c], volume=df_feat[v], append=True)\n",
      "/tmp/ipykernel_35088/1678251532.py:477: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  df_new.ta.stc(close=df_new[price_col], tclength=23, fast=50, slow=100, factor=0.5, append=True, col_names=('STC_23_50_05', 'STCD_23_50_05', 'STCK_23_50_05'))\n",
      "/tmp/ipykernel_35088/1678251532.py:1121: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  regimes = regimes.fillna(method='ffill').fillna(1)\n",
      "/tmp/ipykernel_35088/1678251532.py:1432: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_f[numeric_cols_to_impute] = df_f[numeric_cols_to_impute].fillna(method='bfill').fillna(method='ffill').fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Cleaning (Inf/NaN Handling & Imputation): AAPL ---\n",
      "NaNs before imputation: 1858\n",
      "NaNs after imputation: 0\n",
      "\n",
      "--- 📊 Regime Detection (Simplified): AAPL ---\n",
      "Simple Regimes (0:Med,1:Low,2:High):\n",
      "regime_simple\n",
      "0    34.666667\n",
      "1    32.666667\n",
      "2    32.666667\n",
      "Name: proportion, dtype: float64 %\n",
      "\n",
      "--- 🎯 Target Definition: AAPL ---\n",
      "Target distribution:\n",
      "target\n",
      "0    55.333333\n",
      "1    44.666667\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- SKIPPING Causal Discovery & Ranking for AAPL (DEBUG MODE) ---\n",
      "\n",
      "--- ML Preparation & Feature Selection: AAPL ---\n",
      "Train shapes: X_train=(638, 100), y_train=(638,); Test shapes: X_test=(112, 100), y_test=(112,)\n",
      "Prioritized feature selection: Causal Ranking + Mutual Information...\n",
      "No valid causal ranking provided or num_causal_to_select is 0.\n",
      "Selected 35 features: ['MACDh_12_26_9', 'SMA_10', 'SMA_20', 'EMA_20', 'SMA_50', 'EMA_50', 'SMA_100', 'EMA_100', 'SMA_200', 'EMA_200']...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35088/1678251532.py:579: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_reg['regime_simple'] = df_reg['regime_simple'].fillna(method='bfill').fillna(method='ffill').fillna(0)\n",
      "[I 2025-06-03 12:12:31,775] A new study created in memory with name: no-name-5351b1f5-4972-41a4-9615-2ec8cadf8628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameter Optimization (Optuna for LightGBM): AAPL ---\n",
      "Optimizing LightGBM HPs with Optuna (50 trials)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e401576f58745dc849a27022df9166f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.8404274529851141, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8404274529851141\n",
      "[LightGBM] [Warning] feature_fraction is set=0.660758091101159, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.660758091101159\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.03779467804797152, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.03779467804797152\n",
      "[I 2025-06-03 12:12:31,918] Trial 0 finished with value: 0.6897750834958044 and parameters: {'n_estimators': 650, 'learning_rate': 0.016373035192659546, 'num_leaves': 36, 'max_depth': 8, 'min_child_samples': 12, 'feature_fraction': 0.660758091101159, 'bagging_fraction': 0.8404274529851141, 'bagging_freq': 0, 'reg_alpha': 0.023527090620285265, 'reg_lambda': 0.0018648832453604764, 'min_gain_to_split': 0.03779467804797152}. Best is trial 0 with value: 0.6897750834958044.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7750888355633201, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7750888355633201\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7538762938408647, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7538762938408647\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02680804799325477, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02680804799325477\n",
      "[I 2025-06-03 12:12:31,965] Trial 1 finished with value: 0.688572913734503 and parameters: {'n_estimators': 100, 'learning_rate': 0.049312759888169765, 'num_leaves': 25, 'max_depth': 9, 'min_child_samples': 45, 'feature_fraction': 0.7538762938408647, 'bagging_fraction': 0.7750888355633201, 'bagging_freq': 2, 'reg_alpha': 0.07116210456011514, 'reg_lambda': 0.15233681231495746, 'min_gain_to_split': 0.02680804799325477}. Best is trial 1 with value: 0.688572913734503.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5219525169112432, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5219525169112432\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7472127999466525, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7472127999466525\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.05277397902300677, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.05277397902300677\n",
      "[I 2025-06-03 12:12:32,017] Trial 2 finished with value: 0.6929249989099757 and parameters: {'n_estimators': 200, 'learning_rate': 0.04331172388106658, 'num_leaves': 21, 'max_depth': 7, 'min_child_samples': 23, 'feature_fraction': 0.7472127999466525, 'bagging_fraction': 0.5219525169112432, 'bagging_freq': 7, 'reg_alpha': 0.17835274020836098, 'reg_lambda': 0.014214450147604273, 'min_gain_to_split': 0.05277397902300677}. Best is trial 1 with value: 0.688572913734503.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9870950597840182, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9870950597840182\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6448264953252383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6448264953252383\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04605212654697147, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04605212654697147\n",
      "[I 2025-06-03 12:12:32,064] Trial 3 finished with value: 0.6904550787915193 and parameters: {'n_estimators': 300, 'learning_rate': 0.013257760750198928, 'num_leaves': 37, 'max_depth': 3, 'min_child_samples': 12, 'feature_fraction': 0.6448264953252383, 'bagging_fraction': 0.9870950597840182, 'bagging_freq': 3, 'reg_alpha': 0.009150631342418334, 'reg_lambda': 0.05581639350351704, 'min_gain_to_split': 0.04605212654697147}. Best is trial 1 with value: 0.688572913734503.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6241678704885922, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6241678704885922\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8415758865215872, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8415758865215872\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0214895183469357, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0214895183469357\n",
      "[I 2025-06-03 12:12:32,121] Trial 4 finished with value: 0.6881372446943943 and parameters: {'n_estimators': 550, 'learning_rate': 0.04665053222382827, 'num_leaves': 30, 'max_depth': 8, 'min_child_samples': 25, 'feature_fraction': 0.8415758865215872, 'bagging_fraction': 0.6241678704885922, 'bagging_freq': 5, 'reg_alpha': 0.024602261273731506, 'reg_lambda': 0.021913258711648977, 'min_gain_to_split': 0.0214895183469357}. Best is trial 4 with value: 0.6881372446943943.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5827518927096446, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5827518927096446\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9978096987364382, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9978096987364382\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04669845128130215, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04669845128130215\n",
      "[I 2025-06-03 12:12:32,173] Trial 5 finished with value: 0.6909475940654857 and parameters: {'n_estimators': 250, 'learning_rate': 0.05423867465142441, 'num_leaves': 41, 'max_depth': 8, 'min_child_samples': 24, 'feature_fraction': 0.9978096987364382, 'bagging_fraction': 0.5827518927096446, 'bagging_freq': 6, 'reg_alpha': 0.4893302454002948, 'reg_lambda': 0.001325063626104883, 'min_gain_to_split': 0.04669845128130215}. Best is trial 4 with value: 0.6881372446943943.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6761738804090509, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6761738804090509\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8680537009463224, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8680537009463224\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.07165729834667593, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.07165729834667593\n",
      "[I 2025-06-03 12:12:32,239] Trial 6 finished with value: 0.6910170574924278 and parameters: {'n_estimators': 450, 'learning_rate': 0.029236260495127522, 'num_leaves': 23, 'max_depth': 8, 'min_child_samples': 22, 'feature_fraction': 0.8680537009463224, 'bagging_fraction': 0.6761738804090509, 'bagging_freq': 7, 'reg_alpha': 0.05148860042786368, 'reg_lambda': 0.005216061095942454, 'min_gain_to_split': 0.07165729834667593}. Best is trial 4 with value: 0.6881372446943943.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9954276842622132, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9954276842622132\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9250629885469106, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9250629885469106\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0024481853610631822, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0024481853610631822\n",
      "[I 2025-06-03 12:12:32,296] Trial 7 finished with value: 0.6916689906140735 and parameters: {'n_estimators': 550, 'learning_rate': 0.02702229799388793, 'num_leaves': 46, 'max_depth': 8, 'min_child_samples': 35, 'feature_fraction': 0.9250629885469106, 'bagging_fraction': 0.9954276842622132, 'bagging_freq': 6, 'reg_alpha': 0.030215127757687706, 'reg_lambda': 0.028243399906590187, 'min_gain_to_split': 0.0024481853610631822}. Best is trial 4 with value: 0.6881372446943943.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9396136538874593, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9396136538874593\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7912293560354925, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7912293560354925\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.03356056390170094, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.03356056390170094\n",
      "[I 2025-06-03 12:12:32,372] Trial 8 finished with value: 0.6899616178015757 and parameters: {'n_estimators': 100, 'learning_rate': 0.02381050126064158, 'num_leaves': 14, 'max_depth': 5, 'min_child_samples': 19, 'feature_fraction': 0.7912293560354925, 'bagging_fraction': 0.9396136538874593, 'bagging_freq': 1, 'reg_alpha': 0.005360242868727042, 'reg_lambda': 0.6002362849008817, 'min_gain_to_split': 0.03356056390170094}. Best is trial 4 with value: 0.6881372446943943.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7480538077463729, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7480538077463729\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6639061697762993, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6639061697762993\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02277616954710171, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02277616954710171\n",
      "[I 2025-06-03 12:12:32,421] Trial 9 finished with value: 0.6918692130571428 and parameters: {'n_estimators': 250, 'learning_rate': 0.012544642372688947, 'num_leaves': 10, 'max_depth': 10, 'min_child_samples': 46, 'feature_fraction': 0.6639061697762993, 'bagging_fraction': 0.7480538077463729, 'bagging_freq': 1, 'reg_alpha': 0.16347192697963342, 'reg_lambda': 0.004128858040386683, 'min_gain_to_split': 0.02277616954710171}. Best is trial 4 with value: 0.6881372446943943.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6413467125554336, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6413467125554336\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5162961481790136, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5162961481790136\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.09410143829017298, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.09410143829017298\n",
      "[I 2025-06-03 12:12:32,475] Trial 10 finished with value: 0.6928862021297367 and parameters: {'n_estimators': 950, 'learning_rate': 0.09368612516300528, 'num_leaves': 30, 'max_depth': 1, 'min_child_samples': 34, 'feature_fraction': 0.5162961481790136, 'bagging_fraction': 0.6413467125554336, 'bagging_freq': 4, 'reg_alpha': 0.0013340308149510317, 'reg_lambda': 0.11246072042033453, 'min_gain_to_split': 0.09410143829017298}. Best is trial 4 with value: 0.6881372446943943.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7901933492486819, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7901933492486819\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8038830425448367, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8038830425448367\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.010443683712089143, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.010443683712089143\n",
      "[I 2025-06-03 12:12:32,534] Trial 11 finished with value: 0.6879595045816966 and parameters: {'n_estimators': 850, 'learning_rate': 0.059116404139591, 'num_leaves': 27, 'max_depth': 10, 'min_child_samples': 50, 'feature_fraction': 0.8038830425448367, 'bagging_fraction': 0.7901933492486819, 'bagging_freq': 3, 'reg_alpha': 0.06456306966003232, 'reg_lambda': 0.22998328439488447, 'min_gain_to_split': 0.010443683712089143}. Best is trial 11 with value: 0.6879595045816966.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8465473629367326, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8465473629367326\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8497020855907143, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8497020855907143\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0026341516743156428, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0026341516743156428\n",
      "[I 2025-06-03 12:12:32,605] Trial 12 finished with value: 0.6927465161818169 and parameters: {'n_estimators': 850, 'learning_rate': 0.07926190959945544, 'num_leaves': 29, 'max_depth': 5, 'min_child_samples': 33, 'feature_fraction': 0.8497020855907143, 'bagging_fraction': 0.8465473629367326, 'bagging_freq': 4, 'reg_alpha': 0.010782269726844818, 'reg_lambda': 0.9003050647593627, 'min_gain_to_split': 0.0026341516743156428}. Best is trial 11 with value: 0.6879595045816966.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6943826365286986, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6943826365286986\n",
      "[LightGBM] [Warning] feature_fraction is set=0.851635230904185, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.851635230904185\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.016186004317527147, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.016186004317527147\n",
      "[I 2025-06-03 12:12:32,660] Trial 13 finished with value: 0.6928313398939584 and parameters: {'n_estimators': 750, 'learning_rate': 0.06466913155760805, 'num_leaves': 18, 'max_depth': 10, 'min_child_samples': 49, 'feature_fraction': 0.851635230904185, 'bagging_fraction': 0.6943826365286986, 'bagging_freq': 5, 'reg_alpha': 0.7034439711232662, 'reg_lambda': 0.21940552682443054, 'min_gain_to_split': 0.016186004317527147}. Best is trial 11 with value: 0.6879595045816966.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5836797955823378, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5836797955823378\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9263536180196101, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9263536180196101\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.012968089311883169, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.012968089311883169\n",
      "[I 2025-06-03 12:12:32,725] Trial 14 finished with value: 0.6906588957198024 and parameters: {'n_estimators': 1000, 'learning_rate': 0.04197615384798893, 'num_leaves': 34, 'max_depth': 6, 'min_child_samples': 39, 'feature_fraction': 0.9263536180196101, 'bagging_fraction': 0.5836797955823378, 'bagging_freq': 3, 'reg_alpha': 0.00247456793366222, 'reg_lambda': 0.017292947951317826, 'min_gain_to_split': 0.012968089311883169}. Best is trial 11 with value: 0.6879595045816966.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7988230253234739, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7988230253234739\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7609393597175612, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7609393597175612\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.06479786423178946, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.06479786423178946\n",
      "[I 2025-06-03 12:12:32,905] Trial 15 finished with value: 0.6897939571367065 and parameters: {'n_estimators': 450, 'learning_rate': 0.06875085621390217, 'num_leaves': 50, 'max_depth': -1, 'min_child_samples': 6, 'feature_fraction': 0.7609393597175612, 'bagging_fraction': 0.7988230253234739, 'bagging_freq': 5, 'reg_alpha': 0.12769189092040165, 'reg_lambda': 0.35239087857599544, 'min_gain_to_split': 0.06479786423178946}. Best is trial 11 with value: 0.6879595045816966.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8897728542017029, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8897728542017029\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8101693694856218, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8101693694856218\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.011131945332430514, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.011131945332430514\n",
      "[I 2025-06-03 12:12:33,005] Trial 16 finished with value: 0.6907415406400008 and parameters: {'n_estimators': 700, 'learning_rate': 0.03744060533574671, 'num_leaves': 27, 'max_depth': 10, 'min_child_samples': 28, 'feature_fraction': 0.8101693694856218, 'bagging_fraction': 0.8897728542017029, 'bagging_freq': 2, 'reg_alpha': 0.01671576772409876, 'reg_lambda': 0.0622888514290624, 'min_gain_to_split': 0.011131945332430514}. Best is trial 11 with value: 0.6879595045816966.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7205517996466544, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7205517996466544\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7081978425475015, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7081978425475015\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.026181627829920555, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.026181627829920555\n",
      "[I 2025-06-03 12:12:33,075] Trial 17 finished with value: 0.6916982077775724 and parameters: {'n_estimators': 850, 'learning_rate': 0.09703637761762209, 'num_leaves': 32, 'max_depth': 4, 'min_child_samples': 41, 'feature_fraction': 0.7081978425475015, 'bagging_fraction': 0.7205517996466544, 'bagging_freq': 5, 'reg_alpha': 0.0565756638685778, 'reg_lambda': 0.00935618999718802, 'min_gain_to_split': 0.026181627829920555}. Best is trial 11 with value: 0.6879595045816966.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6259047346767856, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6259047346767856\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5902843412426022, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5902843412426022\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0007080248879301129, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0007080248879301129\n",
      "[I 2025-06-03 12:12:33,157] Trial 18 finished with value: 0.690174408129361 and parameters: {'n_estimators': 550, 'learning_rate': 0.018281440105806488, 'num_leaves': 42, 'max_depth': 6, 'min_child_samples': 30, 'feature_fraction': 0.5902843412426022, 'bagging_fraction': 0.6259047346767856, 'bagging_freq': 4, 'reg_alpha': 0.3170680814516602, 'reg_lambda': 0.05051561436081398, 'min_gain_to_split': 0.0007080248879301129}. Best is trial 11 with value: 0.6879595045816966.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5021407892024521, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5021407892024521\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9134138348521158, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9134138348521158\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.035330565649679485, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.035330565649679485\n",
      "[I 2025-06-03 12:12:33,240] Trial 19 finished with value: 0.689780148044401 and parameters: {'n_estimators': 400, 'learning_rate': 0.034918239738313386, 'num_leaves': 19, 'max_depth': 3, 'min_child_samples': 13, 'feature_fraction': 0.9134138348521158, 'bagging_fraction': 0.5021407892024521, 'bagging_freq': 2, 'reg_alpha': 0.004356674951829568, 'reg_lambda': 0.2935317746198534, 'min_gain_to_split': 0.035330565649679485}. Best is trial 11 with value: 0.6879595045816966.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8097060183875189, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8097060183875189\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9620304265012808, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9620304265012808\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.018091771233266964, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.018091771233266964\n",
      "[I 2025-06-03 12:12:33,384] Trial 20 finished with value: 0.6910762954410887 and parameters: {'n_estimators': 800, 'learning_rate': 0.058483340996502535, 'num_leaves': 39, 'max_depth': 9, 'min_child_samples': 17, 'feature_fraction': 0.9620304265012808, 'bagging_fraction': 0.8097060183875189, 'bagging_freq': 6, 'reg_alpha': 0.08137549115549259, 'reg_lambda': 0.10328491641116717, 'min_gain_to_split': 0.018091771233266964}. Best is trial 11 with value: 0.6879595045816966.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7651280794521267, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7651280794521267\n",
      "[LightGBM] [Warning] feature_fraction is set=0.806819870013947, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.806819870013947\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.028007895100660013, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.028007895100660013\n",
      "[I 2025-06-03 12:12:33,455] Trial 21 finished with value: 0.6887745943627498 and parameters: {'n_estimators': 100, 'learning_rate': 0.05007206025669035, 'num_leaves': 25, 'max_depth': 9, 'min_child_samples': 44, 'feature_fraction': 0.806819870013947, 'bagging_fraction': 0.7651280794521267, 'bagging_freq': 2, 'reg_alpha': 0.05330664251939391, 'reg_lambda': 0.14672170912607965, 'min_gain_to_split': 0.028007895100660013}. Best is trial 11 with value: 0.6879595045816966.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7607927258967679, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7607927258967679\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7131638456464279, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7131638456464279\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.009202738947964473, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.009202738947964473\n",
      "[I 2025-06-03 12:12:33,524] Trial 22 finished with value: 0.690803604702186 and parameters: {'n_estimators': 900, 'learning_rate': 0.048494019299625525, 'num_leaves': 26, 'max_depth': 9, 'min_child_samples': 50, 'feature_fraction': 0.7131638456464279, 'bagging_fraction': 0.7607927258967679, 'bagging_freq': 3, 'reg_alpha': 0.09872832164006548, 'reg_lambda': 0.4755766232784412, 'min_gain_to_split': 0.009202738947964473}. Best is trial 11 with value: 0.6879595045816966.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8933656408785959, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8933656408785959\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8744823479834518, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8744823479834518\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02116838960315396, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02116838960315396\n",
      "[I 2025-06-03 12:12:33,623] Trial 23 finished with value: 0.6904079795065978 and parameters: {'n_estimators': 650, 'learning_rate': 0.07447556615988622, 'num_leaves': 30, 'max_depth': 7, 'min_child_samples': 40, 'feature_fraction': 0.8744823479834518, 'bagging_fraction': 0.8933656408785959, 'bagging_freq': 1, 'reg_alpha': 0.020266120529169693, 'reg_lambda': 0.029389800241785717, 'min_gain_to_split': 0.02116838960315396}. Best is trial 11 with value: 0.6879595045816966.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6970793510162014, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6970793510162014\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7621280863558022, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7621280863558022\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.05670762413403044, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.05670762413403044\n",
      "[I 2025-06-03 12:12:33,712] Trial 24 finished with value: 0.6891689039928394 and parameters: {'n_estimators': 350, 'learning_rate': 0.042323856919875946, 'num_leaves': 23, 'max_depth': 10, 'min_child_samples': 44, 'feature_fraction': 0.7621280863558022, 'bagging_fraction': 0.6970793510162014, 'bagging_freq': 2, 'reg_alpha': 0.0431349767661134, 'reg_lambda': 0.16127925710390537, 'min_gain_to_split': 0.05670762413403044}. Best is trial 11 with value: 0.6879595045816966.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8020828751878475, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8020828751878475\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8210583255450532, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8210583255450532\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.039648521349844375, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.039648521349844375\n",
      "[I 2025-06-03 12:12:33,785] Trial 25 finished with value: 0.688939928995166 and parameters: {'n_estimators': 600, 'learning_rate': 0.05737466112340838, 'num_leaves': 16, 'max_depth': 7, 'min_child_samples': 47, 'feature_fraction': 0.8210583255450532, 'bagging_fraction': 0.8020828751878475, 'bagging_freq': 3, 'reg_alpha': 0.20349307227206465, 'reg_lambda': 0.0719966843675064, 'min_gain_to_split': 0.039648521349844375}. Best is trial 11 with value: 0.6879595045816966.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7275247097117152, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7275247097117152\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7182857756258144, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7182857756258144\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.030518749878158294, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.030518749878158294\n",
      "[I 2025-06-03 12:12:33,890] Trial 26 finished with value: 0.6916821194118624 and parameters: {'n_estimators': 450, 'learning_rate': 0.03277053045591873, 'num_leaves': 34, 'max_depth': 9, 'min_child_samples': 36, 'feature_fraction': 0.7182857756258144, 'bagging_fraction': 0.7275247097117152, 'bagging_freq': 0, 'reg_alpha': 0.09028374663514735, 'reg_lambda': 0.246440067898349, 'min_gain_to_split': 0.030518749878158294}. Best is trial 11 with value: 0.6879595045816966.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.648703530512426, subsample=1.0 will be ignored. Current value: bagging_fraction=0.648703530512426\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8989472273518628, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8989472273518628\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.008509481282318107, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.008509481282318107\n",
      "[I 2025-06-03 12:12:33,985] Trial 27 finished with value: 0.6931348530105952 and parameters: {'n_estimators': 150, 'learning_rate': 0.08189104715442576, 'num_leaves': 28, 'max_depth': 6, 'min_child_samples': 30, 'feature_fraction': 0.8989472273518628, 'bagging_fraction': 0.648703530512426, 'bagging_freq': 4, 'reg_alpha': 0.010439316257058804, 'reg_lambda': 0.017149796773953917, 'min_gain_to_split': 0.008509481282318107}. Best is trial 11 with value: 0.6879595045816966.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5947059972048445, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5947059972048445\n",
      "[LightGBM] [Warning] feature_fraction is set=0.781158982542707, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.781158982542707\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.07824236007670786, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.07824236007670786\n",
      "[I 2025-06-03 12:12:34,071] Trial 28 finished with value: 0.690342232938131 and parameters: {'n_estimators': 1000, 'learning_rate': 0.02320014811610084, 'num_leaves': 23, 'max_depth': 9, 'min_child_samples': 26, 'feature_fraction': 0.781158982542707, 'bagging_fraction': 0.5947059972048445, 'bagging_freq': 5, 'reg_alpha': 0.034916183011022305, 'reg_lambda': 0.03062434947683211, 'min_gain_to_split': 0.07824236007670786}. Best is trial 11 with value: 0.6879595045816966.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8520870824198333, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8520870824198333\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6697186060874661, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6697186060874661\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04168620182526405, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04168620182526405\n",
      "[I 2025-06-03 12:12:34,145] Trial 29 finished with value: 0.692263571656324 and parameters: {'n_estimators': 700, 'learning_rate': 0.06537577514673228, 'num_leaves': 33, 'max_depth': 7, 'min_child_samples': 43, 'feature_fraction': 0.6697186060874661, 'bagging_fraction': 0.8520870824198333, 'bagging_freq': 0, 'reg_alpha': 0.019160455651605426, 'reg_lambda': 0.0031621433782804986, 'min_gain_to_split': 0.04168620182526405}. Best is trial 11 with value: 0.6879595045816966.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5412911881308692, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5412911881308692\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6148312801579183, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6148312801579183\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.022603515464519625, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.022603515464519625\n",
      "[I 2025-06-03 12:12:34,226] Trial 30 finished with value: 0.6919170888210585 and parameters: {'n_estimators': 500, 'learning_rate': 0.047929139045328494, 'num_leaves': 36, 'max_depth': 8, 'min_child_samples': 38, 'feature_fraction': 0.6148312801579183, 'bagging_fraction': 0.5412911881308692, 'bagging_freq': 3, 'reg_alpha': 0.33073719302556925, 'reg_lambda': 0.007448910078201216, 'min_gain_to_split': 0.022603515464519625}. Best is trial 11 with value: 0.6879595045816966.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7674501763955963, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7674501763955963\n",
      "[LightGBM] [Warning] feature_fraction is set=0.82141259110359, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.82141259110359\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02836743836538393, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02836743836538393\n",
      "[I 2025-06-03 12:12:34,320] Trial 31 finished with value: 0.6890932973785814 and parameters: {'n_estimators': 100, 'learning_rate': 0.05442106051715198, 'num_leaves': 25, 'max_depth': 9, 'min_child_samples': 46, 'feature_fraction': 0.82141259110359, 'bagging_fraction': 0.7674501763955963, 'bagging_freq': 2, 'reg_alpha': 0.06530623373877545, 'reg_lambda': 0.1292382322627572, 'min_gain_to_split': 0.02836743836538393}. Best is trial 11 with value: 0.6879595045816966.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7768580887231128, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7768580887231128\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7311343803002752, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7311343803002752\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02960099340517688, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02960099340517688\n",
      "[I 2025-06-03 12:12:34,391] Trial 32 finished with value: 0.6891251298886758 and parameters: {'n_estimators': 200, 'learning_rate': 0.04799505061272877, 'num_leaves': 21, 'max_depth': 10, 'min_child_samples': 50, 'feature_fraction': 0.7311343803002752, 'bagging_fraction': 0.7768580887231128, 'bagging_freq': 2, 'reg_alpha': 0.029232570935586094, 'reg_lambda': 0.1593138823416883, 'min_gain_to_split': 0.02960099340517688}. Best is trial 11 with value: 0.6879595045816966.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8271947367895475, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8271947367895475\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8374339437704877, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8374339437704877\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.01822637463628532, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.01822637463628532\n",
      "[I 2025-06-03 12:12:34,463] Trial 33 finished with value: 0.6886870829048543 and parameters: {'n_estimators': 150, 'learning_rate': 0.03742193018833421, 'num_leaves': 25, 'max_depth': 8, 'min_child_samples': 44, 'feature_fraction': 0.8374339437704877, 'bagging_fraction': 0.8271947367895475, 'bagging_freq': 2, 'reg_alpha': 0.04006995184596003, 'reg_lambda': 0.07715474426425721, 'min_gain_to_split': 0.01822637463628532}. Best is trial 11 with value: 0.6879595045816966.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8230308122934713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8230308122934713\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8402151055015911, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8402151055015911\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.01708485362141359, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.01708485362141359\n",
      "[I 2025-06-03 12:12:34,559] Trial 34 finished with value: 0.6873201851644442 and parameters: {'n_estimators': 300, 'learning_rate': 0.038230219610713045, 'num_leaves': 31, 'max_depth': 8, 'min_child_samples': 47, 'feature_fraction': 0.8402151055015911, 'bagging_fraction': 0.8230308122934713, 'bagging_freq': 1, 'reg_alpha': 0.01564668905628908, 'reg_lambda': 0.04427169904910272, 'min_gain_to_split': 0.01708485362141359}. Best is trial 34 with value: 0.6873201851644442.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8730980220487401, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8730980220487401\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8781374280879748, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8781374280879748\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.006653386278521531, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.006653386278521531\n",
      "[I 2025-06-03 12:12:34,653] Trial 35 finished with value: 0.6926973655469708 and parameters: {'n_estimators': 300, 'learning_rate': 0.0413035266912137, 'num_leaves': 32, 'max_depth': 7, 'min_child_samples': 48, 'feature_fraction': 0.8781374280879748, 'bagging_fraction': 0.8730980220487401, 'bagging_freq': 1, 'reg_alpha': 0.005781349604058094, 'reg_lambda': 0.04872706427729295, 'min_gain_to_split': 0.006653386278521531}. Best is trial 34 with value: 0.6873201851644442.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7364985352960706, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7364985352960706\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7798584370455044, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7798584370455044\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.013740842638065576, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.013740842638065576\n",
      "[I 2025-06-03 12:12:34,788] Trial 36 finished with value: 0.6892710007700467 and parameters: {'n_estimators': 350, 'learning_rate': 0.010342500885538116, 'num_leaves': 36, 'max_depth': 8, 'min_child_samples': 20, 'feature_fraction': 0.7798584370455044, 'bagging_fraction': 0.7364985352960706, 'bagging_freq': 0, 'reg_alpha': 0.018011348235286526, 'reg_lambda': 0.011704014618310797, 'min_gain_to_split': 0.013740842638065576}. Best is trial 34 with value: 0.6873201851644442.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9632679193119761, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9632679193119761\n",
      "[LightGBM] [Warning] feature_fraction is set=0.949296867742067, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.949296867742067\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04916921724493148, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04916921724493148\n",
      "[I 2025-06-03 12:12:34,893] Trial 37 finished with value: 0.6909761148284097 and parameters: {'n_estimators': 250, 'learning_rate': 0.030917131887013005, 'num_leaves': 30, 'max_depth': 8, 'min_child_samples': 24, 'feature_fraction': 0.949296867742067, 'bagging_fraction': 0.9632679193119761, 'bagging_freq': 1, 'reg_alpha': 0.013834638346005351, 'reg_lambda': 0.020427603865024718, 'min_gain_to_split': 0.04916921724493148}. Best is trial 34 with value: 0.6873201851644442.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9242157771711008, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9242157771711008\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6897567098134617, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6897567098134617\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04259082869458966, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04259082869458966\n",
      "[I 2025-06-03 12:12:34,971] Trial 38 finished with value: 0.6918952140739686 and parameters: {'n_estimators': 600, 'learning_rate': 0.02601093724182898, 'num_leaves': 21, 'max_depth': 10, 'min_child_samples': 37, 'feature_fraction': 0.6897567098134617, 'bagging_fraction': 0.9242157771711008, 'bagging_freq': 6, 'reg_alpha': 0.02696708526436694, 'reg_lambda': 0.04297866687970321, 'min_gain_to_split': 0.04259082869458966}. Best is trial 34 with value: 0.6873201851644442.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6721944250044909, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6721944250044909\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7446286790605137, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7446286790605137\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.03454622112875744, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.03454622112875744\n",
      "[I 2025-06-03 12:12:35,040] Trial 39 finished with value: 0.6905599967055494 and parameters: {'n_estimators': 200, 'learning_rate': 0.0207967276705717, 'num_leaves': 40, 'max_depth': 4, 'min_child_samples': 42, 'feature_fraction': 0.7446286790605137, 'bagging_fraction': 0.6721944250044909, 'bagging_freq': 7, 'reg_alpha': 0.007198070470842124, 'reg_lambda': 0.5377731754207489, 'min_gain_to_split': 0.03454622112875744}. Best is trial 34 with value: 0.6873201851644442.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7860830664810515, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7860830664810515\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8846306399164736, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8846306399164736\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.05570518188568961, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.05570518188568961\n",
      "[I 2025-06-03 12:12:35,107] Trial 40 finished with value: 0.6889291502776245 and parameters: {'n_estimators': 350, 'learning_rate': 0.03888437557448737, 'num_leaves': 28, 'max_depth': 2, 'min_child_samples': 47, 'feature_fraction': 0.8846306399164736, 'bagging_fraction': 0.7860830664810515, 'bagging_freq': 1, 'reg_alpha': 0.13908716119996334, 'reg_lambda': 0.002313314318016213, 'min_gain_to_split': 0.05570518188568961}. Best is trial 34 with value: 0.6873201851644442.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8265894768890996, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8265894768890996\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8476068460611609, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8476068460611609\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.01936745152108533, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.01936745152108533\n",
      "[I 2025-06-03 12:12:35,190] Trial 41 finished with value: 0.6886819738939834 and parameters: {'n_estimators': 150, 'learning_rate': 0.03416650690792275, 'num_leaves': 24, 'max_depth': 8, 'min_child_samples': 45, 'feature_fraction': 0.8476068460611609, 'bagging_fraction': 0.8265894768890996, 'bagging_freq': 2, 'reg_alpha': 0.03877754319676228, 'reg_lambda': 0.0791629378727445, 'min_gain_to_split': 0.01936745152108533}. Best is trial 34 with value: 0.6873201851644442.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8291864579430973, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8291864579430973\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8414169778700442, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8414169778700442\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.020722486371166228, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.020722486371166228\n",
      "[I 2025-06-03 12:12:35,305] Trial 42 finished with value: 0.6899747021614954 and parameters: {'n_estimators': 150, 'learning_rate': 0.029515736270752546, 'num_leaves': 31, 'max_depth': 9, 'min_child_samples': 32, 'feature_fraction': 0.8414169778700442, 'bagging_fraction': 0.8291864579430973, 'bagging_freq': 3, 'reg_alpha': 0.025625145185487305, 'reg_lambda': 0.0232938221639182, 'min_gain_to_split': 0.020722486371166228}. Best is trial 34 with value: 0.6873201851644442.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8636149375187595, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8636149375187595\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7956729359097979, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7956729359097979\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0061552565610056495, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0061552565610056495\n",
      "[I 2025-06-03 12:12:35,403] Trial 43 finished with value: 0.6895586591492536 and parameters: {'n_estimators': 200, 'learning_rate': 0.0441590555130903, 'num_leaves': 27, 'max_depth': 7, 'min_child_samples': 45, 'feature_fraction': 0.7956729359097979, 'bagging_fraction': 0.8636149375187595, 'bagging_freq': 1, 'reg_alpha': 0.06626427675499633, 'reg_lambda': 0.0892366106857286, 'min_gain_to_split': 0.0061552565610056495}. Best is trial 34 with value: 0.6873201851644442.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8141852416040667, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8141852416040667\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8589780409475757, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8589780409475757\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.014920066806993463, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.014920066806993463\n",
      "[I 2025-06-03 12:12:35,482] Trial 44 finished with value: 0.6888463938050092 and parameters: {'n_estimators': 300, 'learning_rate': 0.06061268464426881, 'num_leaves': 24, 'max_depth': 6, 'min_child_samples': 48, 'feature_fraction': 0.8589780409475757, 'bagging_fraction': 0.8141852416040667, 'bagging_freq': 3, 'reg_alpha': 0.01350186945468633, 'reg_lambda': 0.045237507997200804, 'min_gain_to_split': 0.014920066806993463}. Best is trial 34 with value: 0.6873201851644442.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9147866346538924, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9147866346538924\n",
      "[LightGBM] [Warning] feature_fraction is set=0.834696953248757, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.834696953248757\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.024232241133396268, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.024232241133396268\n",
      "[I 2025-06-03 12:12:35,562] Trial 45 finished with value: 0.6919716614355134 and parameters: {'n_estimators': 250, 'learning_rate': 0.033793248386469764, 'num_leaves': 19, 'max_depth': 8, 'min_child_samples': 41, 'feature_fraction': 0.834696953248757, 'bagging_fraction': 0.9147866346538924, 'bagging_freq': 4, 'reg_alpha': 0.04019730824326809, 'reg_lambda': 0.19083614620459258, 'min_gain_to_split': 0.024232241133396268}. Best is trial 34 with value: 0.6873201851644442.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7883418594309783, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7883418594309783\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7656631464134606, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7656631464134606\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0033504883434078425, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0033504883434078425\n",
      "[I 2025-06-03 12:12:35,656] Trial 46 finished with value: 0.6893256122760215 and parameters: {'n_estimators': 150, 'learning_rate': 0.05304316599448632, 'num_leaves': 22, 'max_depth': 8, 'min_child_samples': 49, 'feature_fraction': 0.7656631464134606, 'bagging_fraction': 0.7883418594309783, 'bagging_freq': 2, 'reg_alpha': 0.007512296615279251, 'reg_lambda': 0.886240089893491, 'min_gain_to_split': 0.0033504883434078425}. Best is trial 34 with value: 0.6873201851644442.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8340231007560845, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8340231007560845\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7960283656294361, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7960283656294361\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.01234966800255637, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.01234966800255637\n",
      "[I 2025-06-03 12:12:35,797] Trial 47 finished with value: 0.6912304734593814 and parameters: {'n_estimators': 500, 'learning_rate': 0.07222726060627822, 'num_leaves': 29, 'max_depth': 10, 'min_child_samples': 15, 'feature_fraction': 0.7960283656294361, 'bagging_fraction': 0.8340231007560845, 'bagging_freq': 1, 'reg_alpha': 0.11324464856337464, 'reg_lambda': 0.3320587575865159, 'min_gain_to_split': 0.01234966800255637}. Best is trial 34 with value: 0.6873201851644442.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7123234285692409, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7123234285692409\n",
      "[LightGBM] [Warning] feature_fraction is set=0.908582925483178, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.908582925483178\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.016801787129406737, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.016801787129406737\n",
      "[I 2025-06-03 12:12:35,886] Trial 48 finished with value: 0.6922370832643148 and parameters: {'n_estimators': 300, 'learning_rate': 0.045238934506553294, 'num_leaves': 13, 'max_depth': 5, 'min_child_samples': 22, 'feature_fraction': 0.908582925483178, 'bagging_fraction': 0.7123234285692409, 'bagging_freq': 3, 'reg_alpha': 0.003208417868809235, 'reg_lambda': 0.037248094799238365, 'min_gain_to_split': 0.016801787129406737}. Best is trial 34 with value: 0.6873201851644442.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7466171207836022, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7466171207836022\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9456459526104466, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9456459526104466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.03394066296265362, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.03394066296265362\n",
      "[I 2025-06-03 12:12:35,977] Trial 49 finished with value: 0.69236266311869 and parameters: {'n_estimators': 100, 'learning_rate': 0.03634881085862415, 'num_leaves': 17, 'max_depth': -1, 'min_child_samples': 27, 'feature_fraction': 0.9456459526104466, 'bagging_fraction': 0.7466171207836022, 'bagging_freq': 4, 'reg_alpha': 0.22366799891334785, 'reg_lambda': 0.06536478254952854, 'min_gain_to_split': 0.03394066296265362}. Best is trial 34 with value: 0.6873201851644442.\n",
      "Best Optuna trial for LightGBM: Value=0.6873, Params={'n_estimators': 300, 'learning_rate': 0.038230219610713045, 'num_leaves': 31, 'max_depth': 8, 'min_child_samples': 47, 'feature_fraction': 0.8402151055015911, 'bagging_fraction': 0.8230308122934713, 'bagging_freq': 1, 'reg_alpha': 0.01564668905628908, 'reg_lambda': 0.04427169904910272, 'min_gain_to_split': 0.01708485362141359}\n",
      "\n",
      "--- LightGBM Model Training: AAPL ---\n",
      "Training LightGBM model...\n",
      "Using Optuna-optimized parameters.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8230308122934713, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8230308122934713\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8402151055015911, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8402151055015911\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.01708485362141359, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.01708485362141359\n",
      "\n",
      "Top 10 features:\n",
      "                        Feature  Importance\n",
      "25  close_trans_seq_volatility          23\n",
      "31        close_quarterly_mean          22\n",
      "22        close_entropy_sample          20\n",
      "26   close_trans_seq_autocorr1          19\n",
      "0                MACDh_12_26_9          17\n",
      "18                 kurtosis_20          14\n",
      "17               volatility_20          14\n",
      "20                     skew_50          13\n",
      "14               STOCHk_14_3_3          10\n",
      "21                 kurtosis_50           9\n",
      "\n",
      "🎯 Accuracy on mapped test data: 0.6071\n",
      "📊 AUC: 0.5895\n",
      "\n",
      "Classification Report (on mapped and valid test labels):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.76      0.72        75\n",
      "           1       0.38      0.30      0.33        37\n",
      "\n",
      "    accuracy                           0.61       112\n",
      "   macro avg       0.53      0.53      0.53       112\n",
      "weighted avg       0.59      0.61      0.59       112\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35088/1678251532.py:938: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  ax = sns.barplot(x='Plot_Importance', y='Feature', data=plot_data, palette=\"viridis\", orient='h')\n",
      "The maximum opset needed by this model is only 9.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance plot saved to feature_importance_AAPL.png\n",
      "\n",
      "Exporting LGBM model to ONNX: lgbm_model_AAPL.onnx (opset=12)\n",
      "Model exported to ONNX: lgbm_model_AAPL.onnx\n",
      "ONNX model check OK.\n",
      "Autoformer forecast for AAPL: [209.91397609 207.94556174 207.57115676 207.62369551 207.75268295]\n",
      "\n",
      "--- Foundation Model Forecasting (PatchTST): AAPL ---\n",
      "\n",
      "--- Forecasting for AAPL using PatchTST (ibm-research/patchtst-etth1-pretrain) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight transfer warning/error: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
      "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434. Model may use more random init for some layers.\n",
      "Epoch 1/10 | Loss: 0.000425 | LR: 0.000098\n",
      "Epoch 2/10 | Loss: 0.000446 | LR: 0.000090\n",
      "Epoch 3/10 | Loss: 0.000447 | LR: 0.000079\n",
      "Epoch 4/10 | Loss: 0.000446 | LR: 0.000065\n",
      "Early stopping at epoch 4\n",
      "Fine-tuning completed. Best loss: 0.000425\n",
      "PatchTST Forecast for AAPL: [201.49424743652344, 201.43862915039062, 201.374755859375, 201.34461975097656, 201.39971923828125]\n",
      "  Direction: 📉 DOWN, Magnitude: -0.15%\n",
      "\n",
      "🏁 Workflow completed for AAPL. Final Status: LightGBM Model Trained\n",
      "\n",
      "--- Results Summary for AAPL ---\n",
      "  Overall Status: LightGBM Model Trained\n",
      "  Raw Data Shape: (750, 5)\n",
      "  Featured Data Shape: (750, 105)\n",
      "  Selected Features Count: 35\n",
      "  Top LGBM Features: ['close_trans_seq_volatility', 'close_quarterly_mean', 'close_entropy_sample']\n",
      "  PatchTST Forecast (PatchTST Fine-tuned (10 epochs attempted)):\n",
      "    Values: [201.49424743652344, 201.43862915039062, 201.374755859375, 201.34461975097656, 201.39971923828125]\n",
      "    Direction: 📉 DOWN, Magnitude: -0.15%\n",
      "  Autoformer Forecast: [209.91397608931283, 207.94556174311245, 207.57115675684273, 207.6236955139305, 207.75268294951192]\n",
      "  ONNX Model Path: lgbm_model_AAPL.onnx\n",
      "----------------------------------------\n",
      "DEBUG: Starting main loop for symbol: GOOGL\n",
      "\n",
      "========================================\n",
      "🚀 ENHANCED WORKFLOW FOR: GOOGL\n",
      "========================================\n",
      "Fetching data for GOOGL from Twelve Data (interval=1day, from 2022-06-04 to 2025-06-03)...\n",
      "Successfully fetched/processed 750 data points for GOOGL.\n",
      "\n",
      "--- 🔧 Feature Engineering: GOOGL ---\n",
      "Adding technical indicators...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35088/1678251532.py:355: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[3.06651079e+09 4.35166168e+09 5.49533105e+09 5.25738335e+09\n",
      " 3.43865086e+09 3.17167519e+09 4.78883455e+09 4.24405295e+09\n",
      " 4.35871985e+09 4.26272497e+09 4.81415547e+09 4.53681124e+09\n",
      " 5.23540262e+09 4.10838675e+09 4.05583490e+09 6.68380011e+09\n",
      " 3.71943344e+09 4.29421548e+09 3.34559263e+09 2.54813406e+09\n",
      " 2.26042461e+09 3.70164717e+09 2.30950652e+09 2.37180236e+09\n",
      " 1.96404962e+09 3.09653760e+09 2.52002904e+09 2.55685047e+09\n",
      " 2.54479800e+09 2.36472752e+09 3.22347074e+09 2.74643330e+09\n",
      " 2.92220136e+09 2.26916019e+09 2.13695967e+09 3.37931161e+09\n",
      " 2.48702991e+09 2.83936227e+09 2.43983518e+09 2.54650099e+09\n",
      " 2.76519870e+09 4.22385634e+09 4.15080653e+09 4.37850685e+09\n",
      " 3.00166951e+09 2.70625383e+09 4.81684360e+09 3.16852568e+09\n",
      " 4.09870394e+09 2.87297429e+09 1.85102348e+09 4.31310047e+09\n",
      " 2.90208266e+09 3.92662859e+09 2.21159676e+09 2.03844918e+09\n",
      " 2.05264799e+09 2.51952407e+09 3.58487934e+09 2.57100238e+09\n",
      " 2.43665204e+09 2.75096559e+09 2.40859522e+09 2.66476038e+09\n",
      " 3.41994701e+09 6.13669823e+09 3.96490547e+09 2.91573786e+09\n",
      " 3.35410020e+09 2.93270533e+09 3.54363504e+09 7.48222552e+09\n",
      " 5.21043773e+09 4.81973583e+09 2.46943152e+09 2.71890885e+09\n",
      " 2.81100061e+09 2.93993968e+09 3.27961693e+09 3.10182148e+09\n",
      " 3.23083296e+09 3.37393822e+09 4.82057384e+09 6.48392631e+09\n",
      " 6.19561328e+09 4.36880323e+09 4.53512338e+09 4.30226400e+09\n",
      " 2.92141720e+09 3.79275236e+09 2.59383434e+09 2.55873809e+09\n",
      " 5.22706869e+09 2.65627771e+09 2.87618257e+09 2.92498618e+09\n",
      " 2.71924454e+09 2.49004348e+09 5.33449980e+09 3.85226170e+09\n",
      " 2.86392474e+09 2.30929528e+09 2.80060552e+09 2.83574772e+09\n",
      " 3.93525362e+09 7.00202797e+09 9.17607000e+09 4.81453391e+09\n",
      " 5.35458272e+09 4.01438301e+09 4.30996479e+09 5.10952722e+09\n",
      " 4.39745926e+09 5.22981697e+09 4.42265276e+09 3.80309542e+09\n",
      " 3.36661267e+09 4.06273970e+09 3.38270883e+09 2.91407458e+09\n",
      " 3.48498964e+09 2.76377164e+09 4.38170248e+09 3.28116083e+09\n",
      " 4.25821598e+09 3.22766940e+09 2.87051101e+09 3.54698877e+09\n",
      " 3.34318581e+09 2.60260586e+09 3.62491392e+09 5.46074979e+09\n",
      " 4.16600136e+09 3.22266845e+09 3.61685547e+09 6.41282236e+09\n",
      " 7.98927140e+09 5.86520002e+09 4.84089403e+09 3.72231022e+09\n",
      " 3.37538621e+09 2.97339637e+09 3.07879618e+09 2.71978761e+09\n",
      " 3.22409298e+09 4.36139752e+09 3.22897208e+09 2.85170955e+09\n",
      " 3.66895477e+09 2.69589715e+09 5.79194561e+09 3.83424254e+09\n",
      " 4.10695185e+09 3.20306507e+09 2.83897961e+09 2.82027197e+09\n",
      " 3.40771785e+09 3.01671373e+09 2.95712075e+09 2.96722650e+09\n",
      " 2.96995029e+09 3.59675774e+09 2.67085635e+09 3.76740535e+09\n",
      " 2.65375899e+09 3.84483182e+09 3.62587274e+09 3.95910411e+09\n",
      " 3.26712832e+09 6.20812498e+09 3.59445689e+09 3.77694559e+09\n",
      " 3.46652388e+09 3.39854837e+09 2.47215133e+09 3.89582572e+09\n",
      " 3.47789963e+09 3.54179028e+09 2.41844600e+09 4.34786788e+09\n",
      " 3.20714475e+09 3.81418393e+09 3.76927840e+09 3.09432096e+09\n",
      " 2.46651265e+09 3.20947243e+09 3.57262110e+09 3.07858023e+09\n",
      " 7.76176539e+09 3.99029109e+09 6.72945677e+09 4.35891580e+09\n",
      " 3.48352772e+09 6.84407058e+09 3.85101911e+09 3.74907632e+09\n",
      " 2.37939055e+09 3.35442506e+09 2.95453670e+09 3.47121339e+09\n",
      " 3.02881868e+09 3.42476104e+09 3.23651630e+09 3.68757122e+09\n",
      " 4.85313302e+09 4.72508680e+09 3.75684776e+09 4.41569993e+09\n",
      " 3.96729252e+09 4.24840451e+09 5.73288889e+09 5.54387901e+09\n",
      " 4.20077201e+09 3.65604660e+09 3.28935084e+09 3.97631067e+09\n",
      " 3.30383947e+09 3.31020757e+09 3.91463653e+09 2.80756199e+09\n",
      " 5.81042159e+09 5.04345884e+09 5.34410436e+09 4.46452265e+09\n",
      " 3.81211009e+09 3.26759626e+09 6.08737189e+09 1.03217808e+10\n",
      " 3.15651745e+09 3.67100792e+09 4.38928956e+09 3.34384955e+09\n",
      " 3.69345919e+09 4.88485110e+09 3.81058157e+09 3.19314556e+09\n",
      " 4.87799995e+09 4.30632717e+09 3.38873232e+09 3.09889807e+09\n",
      " 4.12543087e+09 3.33849797e+09 3.61329685e+09 1.11291137e+10\n",
      " 5.51599506e+09 4.02673239e+09 5.75684720e+09 3.66176602e+09\n",
      " 4.77983410e+09 2.60295379e+09 4.27487352e+09 4.63216021e+09\n",
      " 4.84892161e+09 4.29220045e+09 3.99791778e+09 3.01171094e+09\n",
      " 3.62098143e+09 4.74914288e+09 4.65491953e+09 3.86926996e+09\n",
      " 4.10480897e+09 3.78561685e+09 4.96279586e+09 3.17919347e+09\n",
      " 3.46861626e+09 3.55207522e+09 1.04880610e+10 3.28619881e+09\n",
      " 4.24235576e+09 3.64290324e+09 3.49006237e+09 3.19868272e+09\n",
      " 1.89789529e+09 3.96467475e+09 2.86832796e+09 3.04139724e+09\n",
      " 3.07344441e+09 4.37500779e+09 6.62758167e+09 3.43607857e+09\n",
      " 3.22289614e+09 4.42368979e+09 4.02544402e+09 4.14316884e+09\n",
      " 4.63853022e+09 2.59109275e+09 3.04161305e+09 5.07068366e+09\n",
      " 3.94721229e+09 3.71824974e+09 3.07305719e+09 2.30562115e+09\n",
      " 2.35641057e+09 3.60418826e+09 2.94452934e+09 4.44293146e+09\n",
      " 4.57461373e+09 4.64631178e+09 2.89733046e+09 3.30241197e+09\n",
      " 3.77976533e+09 4.31846277e+09 6.67129338e+09 3.31265549e+09\n",
      " 3.46258265e+09 3.37006264e+09 4.72991237e+09 3.17945549e+09\n",
      " 2.49794980e+09 3.13505763e+09 3.35948291e+09 2.72760630e+09\n",
      " 3.26700956e+09 5.35021987e+09 7.12866925e+09 1.21831086e+10\n",
      " 3.09574524e+09 5.91861446e+09 4.55524007e+09 3.13740747e+09\n",
      " 4.55360613e+09 3.52699461e+09 4.14446206e+09 5.54342513e+09\n",
      " 3.45897450e+09 3.25430454e+09 4.06111673e+09 3.81839606e+09\n",
      " 5.48353369e+09 3.70907361e+09 3.73195374e+09 4.44554953e+09\n",
      " 1.00961892e+10 1.30324988e+10 6.71803674e+09 8.81143432e+09\n",
      " 8.58217909e+09 4.96280633e+09 2.03211127e+09 2.35603391e+09\n",
      " 3.56414769e+09 5.80372896e+09 5.21877319e+09 3.26294475e+09\n",
      " 4.23739184e+09 5.42664967e+09 5.97637705e+09 4.76841634e+09\n",
      " 4.68228410e+09 3.55639422e+09 4.87045282e+09 6.52909158e+09\n",
      " 9.00868936e+09 5.59013943e+09 4.31712732e+09 3.96345763e+09\n",
      " 3.78942933e+09 3.61220787e+09 7.71611114e+09 5.32279826e+09\n",
      " 4.88360453e+09 4.73865626e+09 4.71246649e+09 5.27324449e+09\n",
      " 5.60703115e+09 5.97147068e+09 5.15825072e+09 4.10569960e+09\n",
      " 4.79304977e+09 3.92445169e+09 1.08465447e+10 5.24309885e+09\n",
      " 4.83659692e+09 4.06554923e+09 4.84249999e+09 7.26134853e+09\n",
      " 9.13541004e+09 4.85425830e+09 4.20757401e+09 3.50233173e+09\n",
      " 6.97113524e+09 6.72829979e+09 7.99564256e+09 5.44202543e+09\n",
      " 7.15771761e+09 1.23647227e+10 1.29353208e+10 6.53614684e+09\n",
      " 6.02389312e+09]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df_feat.ta.mfi(high=df_feat[h], low=df_feat[l], close=df_feat[c], volume=df_feat[v], append=True)\n",
      "/tmp/ipykernel_35088/1678251532.py:355: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[3.68436805e+09 2.99179836e+09 4.62613301e+09 5.06073645e+09\n",
      " 3.60025021e+09 5.50985898e+09 4.07032589e+09 2.76600463e+09\n",
      " 4.69978839e+09 3.83989859e+09 3.62918666e+09 3.44224921e+09\n",
      " 4.90780907e+09 4.06838630e+09 4.76364524e+09 3.70330267e+09\n",
      " 5.32599270e+09 3.74291684e+09 5.05969108e+09 3.00192835e+09\n",
      " 2.32848907e+09 2.24536567e+09 2.22459520e+09 2.90625533e+09\n",
      " 2.31521951e+09 2.74331545e+09 2.15678519e+09 2.56712355e+09\n",
      " 2.45835418e+09 1.81081265e+09 2.01784279e+09 4.16717307e+09\n",
      " 2.32516910e+09 3.00115076e+09 3.12225147e+09 2.62425470e+09\n",
      " 2.67913277e+09 2.62694695e+09 4.02991300e+09 2.76215923e+09\n",
      " 3.53883093e+09 4.37772559e+09 2.68733130e+09 3.16373895e+09\n",
      " 3.43180359e+09 3.11900237e+09 2.66981943e+09 2.95084338e+09\n",
      " 3.02213445e+09 3.18555286e+09 2.23365737e+09 2.72745268e+09\n",
      " 1.78587755e+09 2.58656282e+09 2.37769705e+09 3.24760113e+09\n",
      " 8.45689760e+09 5.65522034e+09 3.00405699e+09 4.35252559e+09\n",
      " 5.03895355e+09 5.17454584e+09 2.79610439e+09 2.88717217e+09\n",
      " 2.55688901e+09 2.76526374e+09 2.08796389e+09 2.25259615e+09\n",
      " 9.49120300e+08 2.52964486e+09 1.90867014e+09 2.14885763e+09\n",
      " 2.43937110e+09 2.43443968e+09 2.96524964e+09 3.03599615e+09\n",
      " 2.63286531e+09 2.73096053e+09 2.73486405e+09 3.66992422e+09\n",
      " 5.24252637e+09 2.62694151e+09 2.07675579e+09 2.42120758e+09\n",
      " 1.76414101e+09 1.69201065e+09 2.10359851e+09 3.09045896e+09\n",
      " 2.35385662e+09 2.67019799e+09 2.97323024e+09 3.24731634e+09\n",
      " 4.02332540e+09 2.64665890e+09 6.86117736e+09 3.29745211e+09\n",
      " 9.49171967e+09 1.14943583e+10 5.23361283e+09 4.73469526e+09\n",
      " 5.14042257e+09 4.07541881e+09 3.23550480e+09 3.10073354e+09\n",
      " 2.82454769e+09 3.74415086e+09 3.26339092e+09 2.62637221e+09\n",
      " 2.68685888e+09 3.28045487e+09 3.39878412e+09 3.32724251e+09\n",
      " 3.18896635e+09 3.20700874e+09 3.24019880e+09 3.32880258e+09\n",
      " 2.96324125e+09 2.87952024e+09 2.77817526e+09 2.56481161e+09\n",
      " 3.96564016e+09 2.79359720e+09 2.17579557e+09 4.89008659e+09\n",
      " 5.56042035e+09 3.26741485e+09 2.45327069e+09 4.24321562e+09\n",
      " 4.19223441e+09 4.13233451e+09 4.34167700e+09 4.59642341e+09\n",
      " 6.54390746e+09 3.59145820e+09 3.76458649e+09 5.65769363e+09\n",
      " 3.21703881e+09 3.67019393e+09 4.06064861e+09 4.65877867e+09\n",
      " 1.73089133e+09 2.95960840e+09 4.13070123e+09 2.69406525e+09\n",
      " 3.24682832e+09 4.57867607e+09 4.56789571e+09 8.74622664e+09\n",
      " 3.04921414e+09 3.38076063e+09 2.57989359e+09 3.24448995e+09\n",
      " 2.52873196e+09 2.57335431e+09 3.25161609e+09 3.89363737e+09\n",
      " 3.73929194e+09 3.46035367e+09 2.92713050e+09 2.63086633e+09\n",
      " 2.52028999e+09 2.53637990e+09 2.49946690e+09 5.34860792e+09\n",
      " 2.80250091e+09 4.04460199e+09 4.12229075e+09 3.44819139e+09\n",
      " 2.62060383e+09 3.31042542e+09 4.06444286e+09 3.05398837e+09\n",
      " 3.45437441e+09 3.23187286e+09 3.24180091e+09 3.60397205e+09\n",
      " 3.58369176e+09 3.58019939e+09 1.06579820e+10 6.99721249e+09\n",
      " 5.43250780e+09 3.26147219e+09 3.10985370e+09 5.02772912e+09\n",
      " 1.71366657e+09 2.55895503e+09 3.26049789e+09 5.19012250e+09\n",
      " 4.13466959e+09 4.73286175e+09 4.35608924e+09 4.13087077e+09\n",
      " 3.83987803e+09 5.10296695e+09 2.76331432e+09 2.25238839e+09\n",
      " 2.61450439e+09 3.27293597e+09 3.72600630e+09 3.06111600e+09\n",
      " 2.67558905e+09 2.95105080e+09 3.17127517e+09 5.52570358e+09\n",
      " 1.01625611e+10 8.77711930e+09 3.19321193e+09 4.04110612e+09\n",
      " 5.34727720e+09 4.44351624e+09 3.54280451e+09 7.46336262e+09\n",
      " 4.58114990e+09 5.09908669e+09 4.28339171e+09 7.48376000e+09\n",
      " 5.31348473e+09 4.66005381e+09 6.99686100e+09 3.54556985e+09\n",
      " 2.87330621e+09 3.44263315e+09 3.78140867e+09 5.27526068e+09\n",
      " 3.56054179e+09 3.55848219e+09 4.01121654e+09 4.23953549e+09\n",
      " 3.21029401e+09 4.97288113e+09 8.82078372e+09 7.61550170e+09\n",
      " 5.52087352e+09 3.31546215e+09 5.01323718e+09 5.24653760e+09\n",
      " 3.15129040e+09 3.67760542e+09 2.89692824e+09 4.11220116e+09\n",
      " 3.97277337e+09 6.46134447e+09 3.45142776e+09 4.16001237e+09\n",
      " 3.67223613e+09 3.83875069e+09 5.33799033e+09 2.92468800e+09\n",
      " 3.97564485e+09 4.79638109e+09 4.24895091e+09 3.38969343e+09\n",
      " 3.76303177e+09 4.52864119e+09 3.36777562e+09 8.60534634e+09\n",
      " 7.59329409e+09 6.87777113e+09 4.20051156e+09 4.85346296e+09\n",
      " 8.55606275e+09 7.76347208e+09 6.51216561e+09 3.79692273e+09\n",
      " 3.70914517e+09 1.95289907e+09 2.67976606e+09 3.20450080e+09\n",
      " 6.17529672e+09 3.04119168e+09 5.80434251e+09 5.88030682e+09\n",
      " 4.64987483e+09 3.93667736e+09 3.78135560e+09 3.05429457e+09\n",
      " 2.94884441e+09 2.49419610e+09 3.70049622e+09 3.78376898e+09\n",
      " 5.05602503e+09 2.28922258e+09 2.70534945e+09 3.51878652e+09\n",
      " 3.23506002e+09 3.42957049e+09 2.98904456e+09 3.63874404e+09\n",
      " 7.74399254e+09 5.43179610e+09 3.63928728e+09 3.94153050e+09\n",
      " 4.16060064e+09 5.46476201e+09 5.61083581e+09 3.33994341e+09\n",
      " 1.00644499e+10 6.37130306e+09 2.40252825e+09 4.79561984e+09\n",
      " 6.52731952e+09 6.12952748e+09 1.20477969e+10 3.64433991e+09\n",
      " 2.72412981e+09 3.31737187e+09 3.86318635e+09 4.82890984e+09\n",
      " 5.14522805e+09 4.14322933e+09 3.45143683e+09 5.20850461e+09\n",
      " 5.33170562e+09 8.05951975e+09 5.61479961e+09 1.34205373e+10\n",
      " 9.20152147e+09 3.94021051e+09 4.05068998e+09 5.49938772e+09\n",
      " 3.76525616e+09 6.38372694e+09 5.38629476e+09 7.38863400e+09\n",
      " 6.14603143e+09 6.81180033e+09 8.14466013e+09 6.88083707e+09\n",
      " 7.23855484e+09 6.49444484e+09 5.19743129e+09 5.13771727e+09\n",
      " 6.75496889e+09 4.58329277e+09 4.81221272e+09 4.00024481e+09\n",
      " 7.62128808e+09 8.38782546e+09 7.09330214e+09 9.17393768e+09\n",
      " 1.12664627e+10 7.65550749e+09 7.37089678e+09 4.32714675e+09\n",
      " 4.32892684e+09 4.98842592e+09 3.84415260e+09 4.78272803e+09\n",
      " 4.27532068e+09 5.52083369e+09 3.46974393e+09 1.97278181e+10\n",
      " 8.87607394e+09 4.97341270e+09 5.04464728e+09 7.69617399e+09\n",
      " 5.94274547e+09 5.06120365e+09 8.97317317e+09 6.50289001e+09]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df_feat.ta.mfi(high=df_feat[h], low=df_feat[l], close=df_feat[c], volume=df_feat[v], append=True)\n",
      "/tmp/ipykernel_35088/1678251532.py:477: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  df_new.ta.stc(close=df_new[price_col], tclength=23, fast=50, slow=100, factor=0.5, append=True, col_names=('STC_23_50_05', 'STCD_23_50_05', 'STCK_23_50_05'))\n",
      "/tmp/ipykernel_35088/1678251532.py:1121: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  regimes = regimes.fillna(method='ffill').fillna(1)\n",
      "/tmp/ipykernel_35088/1678251532.py:1432: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_f[numeric_cols_to_impute] = df_f[numeric_cols_to_impute].fillna(method='bfill').fillna(method='ffill').fillna(0)\n",
      "/tmp/ipykernel_35088/1678251532.py:579: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_reg['regime_simple'] = df_reg['regime_simple'].fillna(method='bfill').fillna(method='ffill').fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Cleaning (Inf/NaN Handling & Imputation): GOOGL ---\n",
      "NaNs before imputation: 1879\n",
      "NaNs after imputation: 0\n",
      "\n",
      "--- 📊 Regime Detection (Simplified): GOOGL ---\n",
      "Simple Regimes (0:Med,1:Low,2:High):\n",
      "regime_simple\n",
      "0    34.666667\n",
      "1    32.666667\n",
      "2    32.666667\n",
      "Name: proportion, dtype: float64 %\n",
      "\n",
      "--- 🎯 Target Definition: GOOGL ---\n",
      "Target distribution:\n",
      "target\n",
      "0    55.333333\n",
      "1    44.666667\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- SKIPPING Causal Discovery & Ranking for GOOGL (DEBUG MODE) ---\n",
      "\n",
      "--- ML Preparation & Feature Selection: GOOGL ---\n",
      "Train shapes: X_train=(638, 100), y_train=(638,); Test shapes: X_test=(112, 100), y_test=(112,)\n",
      "Prioritized feature selection: Causal Ranking + Mutual Information...\n",
      "No valid causal ranking provided or num_causal_to_select is 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-03 12:12:38,246] A new study created in memory with name: no-name-4849b0c6-90f5-4d2d-9e60-d121926bbd2f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 35 features: ['MACDs_12_26_9', 'SMA_10', 'EMA_10', 'SMA_20', 'EMA_20', 'SMA_50', 'EMA_50', 'SMA_100', 'EMA_100', 'SMA_200']...\n",
      "\n",
      "--- Hyperparameter Optimization (Optuna for LightGBM): GOOGL ---\n",
      "Optimizing LightGBM HPs with Optuna (50 trials)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69024de3ba7048c08603d69beb13c24f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.6617438762026049, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6617438762026049\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9217247282929125, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9217247282929125\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04819052063531404, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04819052063531404\n",
      "[I 2025-06-03 12:12:38,313] Trial 0 finished with value: 0.6621338281221715 and parameters: {'n_estimators': 500, 'learning_rate': 0.08049659454612014, 'num_leaves': 44, 'max_depth': 3, 'min_child_samples': 36, 'feature_fraction': 0.9217247282929125, 'bagging_fraction': 0.6617438762026049, 'bagging_freq': 4, 'reg_alpha': 0.026630791527572355, 'reg_lambda': 0.08346765548141377, 'min_gain_to_split': 0.04819052063531404}. Best is trial 0 with value: 0.6621338281221715.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6308705834446129, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6308705834446129\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6532205490382381, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6532205490382381\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.05476259103565463, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.05476259103565463\n",
      "[I 2025-06-03 12:12:38,443] Trial 1 finished with value: 0.6533971244209816 and parameters: {'n_estimators': 700, 'learning_rate': 0.049199248231725896, 'num_leaves': 34, 'max_depth': 7, 'min_child_samples': 33, 'feature_fraction': 0.6532205490382381, 'bagging_fraction': 0.6308705834446129, 'bagging_freq': 0, 'reg_alpha': 0.0014647301725780346, 'reg_lambda': 0.005760884875329684, 'min_gain_to_split': 0.05476259103565463}. Best is trial 1 with value: 0.6533971244209816.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9276866792730714, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9276866792730714\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5463868891167029, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5463868891167029\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.07761396553739346, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.07761396553739346\n",
      "[I 2025-06-03 12:12:38,524] Trial 2 finished with value: 0.65955358118233 and parameters: {'n_estimators': 600, 'learning_rate': 0.042727469907737056, 'num_leaves': 49, 'max_depth': 9, 'min_child_samples': 39, 'feature_fraction': 0.5463868891167029, 'bagging_fraction': 0.9276866792730714, 'bagging_freq': 1, 'reg_alpha': 0.019964069330744065, 'reg_lambda': 0.013071816241767656, 'min_gain_to_split': 0.07761396553739346}. Best is trial 1 with value: 0.6533971244209816.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6737372931793122, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6737372931793122\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6702513055560083, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6702513055560083\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.03713606767388774, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.03713606767388774\n",
      "[I 2025-06-03 12:12:38,564] Trial 3 finished with value: 0.6851318365534405 and parameters: {'n_estimators': 400, 'learning_rate': 0.015798307513929348, 'num_leaves': 36, 'max_depth': 1, 'min_child_samples': 7, 'feature_fraction': 0.6702513055560083, 'bagging_fraction': 0.6737372931793122, 'bagging_freq': 2, 'reg_alpha': 0.005041122598979371, 'reg_lambda': 0.37974542971677977, 'min_gain_to_split': 0.03713606767388774}. Best is trial 1 with value: 0.6533971244209816.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7815662884472836, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7815662884472836\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7948228068077784, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7948228068077784\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.08480934523484096, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.08480934523484096\n",
      "[I 2025-06-03 12:12:38,682] Trial 4 finished with value: 0.6901016654372352 and parameters: {'n_estimators': 650, 'learning_rate': 0.0570674819143289, 'num_leaves': 46, 'max_depth': 0, 'min_child_samples': 13, 'feature_fraction': 0.7948228068077784, 'bagging_fraction': 0.7815662884472836, 'bagging_freq': 0, 'reg_alpha': 0.00144743970308908, 'reg_lambda': 0.19538037297365232, 'min_gain_to_split': 0.08480934523484096}. Best is trial 1 with value: 0.6533971244209816.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8833972709637186, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8833972709637186\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6346900593237002, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6346900593237002\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.07343605676503195, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.07343605676503195\n",
      "[I 2025-06-03 12:12:38,749] Trial 5 finished with value: 0.661316901787623 and parameters: {'n_estimators': 250, 'learning_rate': 0.09452498665730939, 'num_leaves': 49, 'max_depth': 6, 'min_child_samples': 27, 'feature_fraction': 0.6346900593237002, 'bagging_fraction': 0.8833972709637186, 'bagging_freq': 5, 'reg_alpha': 0.01896006473172844, 'reg_lambda': 0.37233950404288324, 'min_gain_to_split': 0.07343605676503195}. Best is trial 1 with value: 0.6533971244209816.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8729709505043237, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8729709505043237\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5828956213144554, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5828956213144554\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.08315914340868809, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.08315914340868809\n",
      "[I 2025-06-03 12:12:38,972] Trial 6 finished with value: 0.6640753815220718 and parameters: {'n_estimators': 250, 'learning_rate': 0.010281891867822784, 'num_leaves': 30, 'max_depth': 8, 'min_child_samples': 26, 'feature_fraction': 0.5828956213144554, 'bagging_fraction': 0.8729709505043237, 'bagging_freq': 2, 'reg_alpha': 0.007688170168993637, 'reg_lambda': 0.008987580450861463, 'min_gain_to_split': 0.08315914340868809}. Best is trial 1 with value: 0.6533971244209816.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.559934055608326, subsample=1.0 will be ignored. Current value: bagging_fraction=0.559934055608326\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6436481399889491, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6436481399889491\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.03951720894209792, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.03951720894209792\n",
      "[I 2025-06-03 12:12:39,090] Trial 7 finished with value: 0.676885149450043 and parameters: {'n_estimators': 500, 'learning_rate': 0.02343019782688506, 'num_leaves': 48, 'max_depth': 9, 'min_child_samples': 17, 'feature_fraction': 0.6436481399889491, 'bagging_fraction': 0.559934055608326, 'bagging_freq': 0, 'reg_alpha': 0.47950830060269783, 'reg_lambda': 0.002178290182737802, 'min_gain_to_split': 0.03951720894209792}. Best is trial 1 with value: 0.6533971244209816.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.533158119827019, subsample=1.0 will be ignored. Current value: bagging_fraction=0.533158119827019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9543253407523493, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9543253407523493\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.08942543999641492, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.08942543999641492\n",
      "[I 2025-06-03 12:12:39,207] Trial 8 finished with value: 0.6621987172303733 and parameters: {'n_estimators': 300, 'learning_rate': 0.012082978254704165, 'num_leaves': 50, 'max_depth': 6, 'min_child_samples': 32, 'feature_fraction': 0.9543253407523493, 'bagging_fraction': 0.533158119827019, 'bagging_freq': 3, 'reg_alpha': 0.002424224365954349, 'reg_lambda': 0.00211813137032879, 'min_gain_to_split': 0.08942543999641492}. Best is trial 1 with value: 0.6533971244209816.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7488344483926068, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7488344483926068\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6826856308614166, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6826856308614166\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04617018538497774, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04617018538497774\n",
      "[I 2025-06-03 12:12:39,271] Trial 9 finished with value: 0.6698949586823483 and parameters: {'n_estimators': 450, 'learning_rate': 0.0881169884814827, 'num_leaves': 38, 'max_depth': 9, 'min_child_samples': 50, 'feature_fraction': 0.6826856308614166, 'bagging_fraction': 0.7488344483926068, 'bagging_freq': 2, 'reg_alpha': 0.030565004468552752, 'reg_lambda': 0.7938706262381702, 'min_gain_to_split': 0.04617018538497774}. Best is trial 1 with value: 0.6533971244209816.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6215466657518895, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6215466657518895\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7921381700804007, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7921381700804007\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.003461529991906423, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.003461529991906423\n",
      "[I 2025-06-03 12:12:39,361] Trial 10 finished with value: 0.6567818294009479 and parameters: {'n_estimators': 950, 'learning_rate': 0.029789514940194932, 'num_leaves': 18, 'max_depth': 4, 'min_child_samples': 46, 'feature_fraction': 0.7921381700804007, 'bagging_fraction': 0.6215466657518895, 'bagging_freq': 7, 'reg_alpha': 0.4073292798952479, 'reg_lambda': 0.02868491030840586, 'min_gain_to_split': 0.003461529991906423}. Best is trial 1 with value: 0.6533971244209816.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.624760901574191, subsample=1.0 will be ignored. Current value: bagging_fraction=0.624760901574191\n",
      "[LightGBM] [Warning] feature_fraction is set=0.788114239510424, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.788114239510424\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.0009854215824216944, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0009854215824216944\n",
      "[I 2025-06-03 12:12:39,460] Trial 11 finished with value: 0.6688183275686032 and parameters: {'n_estimators': 950, 'learning_rate': 0.0294801189495515, 'num_leaves': 17, 'max_depth': 4, 'min_child_samples': 47, 'feature_fraction': 0.788114239510424, 'bagging_fraction': 0.624760901574191, 'bagging_freq': 7, 'reg_alpha': 0.24001285367606037, 'reg_lambda': 0.019631223840299746, 'min_gain_to_split': 0.0009854215824216944}. Best is trial 1 with value: 0.6533971244209816.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6083703296383361, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6083703296383361\n",
      "[LightGBM] [Warning] feature_fraction is set=0.846399115414169, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.846399115414169\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.013423632840955161, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.013423632840955161\n",
      "[I 2025-06-03 12:12:39,563] Trial 12 finished with value: 0.6679084162552616 and parameters: {'n_estimators': 900, 'learning_rate': 0.043543326839827014, 'num_leaves': 19, 'max_depth': 6, 'min_child_samples': 43, 'feature_fraction': 0.846399115414169, 'bagging_fraction': 0.6083703296383361, 'bagging_freq': 7, 'reg_alpha': 0.12133117759257113, 'reg_lambda': 0.06544728976090235, 'min_gain_to_split': 0.013423632840955161}. Best is trial 1 with value: 0.6533971244209816.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7373185208625584, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7373185208625584\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7249732210922217, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7249732210922217\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.06462468630233485, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.06462468630233485\n",
      "[I 2025-06-03 12:12:39,642] Trial 13 finished with value: 0.6803046869043582 and parameters: {'n_estimators': 800, 'learning_rate': 0.01869006341655661, 'num_leaves': 25, 'max_depth': 2, 'min_child_samples': 41, 'feature_fraction': 0.7249732210922217, 'bagging_fraction': 0.7373185208625584, 'bagging_freq': 5, 'reg_alpha': 0.8018928895314086, 'reg_lambda': 0.008725534744815062, 'min_gain_to_split': 0.06462468630233485}. Best is trial 1 with value: 0.6533971244209816.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5260126813078552, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5260126813078552\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8627243931092475, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8627243931092475\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02924161275950242, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02924161275950242\n",
      "[I 2025-06-03 12:12:39,759] Trial 14 finished with value: 0.6744985519145243 and parameters: {'n_estimators': 800, 'learning_rate': 0.03669854085374799, 'num_leaves': 11, 'max_depth': 5, 'min_child_samples': 22, 'feature_fraction': 0.8627243931092475, 'bagging_fraction': 0.5260126813078552, 'bagging_freq': 6, 'reg_alpha': 0.1020156034269957, 'reg_lambda': 0.004003659932153393, 'min_gain_to_split': 0.02924161275950242}. Best is trial 1 with value: 0.6533971244209816.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7071239496277997, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7071239496277997\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5154721577027143, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5154721577027143\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.06253810719761188, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.06253810719761188\n",
      "[I 2025-06-03 12:12:39,861] Trial 15 finished with value: 0.6825863506374297 and parameters: {'n_estimators': 750, 'learning_rate': 0.05567263008097488, 'num_leaves': 26, 'max_depth': -1, 'min_child_samples': 32, 'feature_fraction': 0.5154721577027143, 'bagging_fraction': 0.7071239496277997, 'bagging_freq': 4, 'reg_alpha': 0.08563372888149284, 'reg_lambda': 0.03911193929133178, 'min_gain_to_split': 0.06253810719761188}. Best is trial 1 with value: 0.6533971244209816.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5986723877828829, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5986723877828829\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7481976112374455, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7481976112374455\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.09961449101088904, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.09961449101088904\n",
      "[I 2025-06-03 12:12:39,953] Trial 16 finished with value: 0.6651300042005517 and parameters: {'n_estimators': 100, 'learning_rate': 0.027128471676015634, 'num_leaves': 37, 'max_depth': 7, 'min_child_samples': 46, 'feature_fraction': 0.7481976112374455, 'bagging_fraction': 0.5986723877828829, 'bagging_freq': 6, 'reg_alpha': 0.0010316952889200474, 'reg_lambda': 0.004920002888095048, 'min_gain_to_split': 0.09961449101088904}. Best is trial 1 with value: 0.6533971244209816.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7939112091834697, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7939112091834697\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9968147839076451, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9968147839076451\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02367210240307023, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02367210240307023\n",
      "[I 2025-06-03 12:12:40,028] Trial 17 finished with value: 0.6758133193951604 and parameters: {'n_estimators': 950, 'learning_rate': 0.05826360655771134, 'num_leaves': 20, 'max_depth': 4, 'min_child_samples': 33, 'feature_fraction': 0.9968147839076451, 'bagging_fraction': 0.7939112091834697, 'bagging_freq': 3, 'reg_alpha': 0.007693947846644466, 'reg_lambda': 0.028425786942175974, 'min_gain_to_split': 0.02367210240307023}. Best is trial 1 with value: 0.6533971244209816.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9894146486884396, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9894146486884396\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8538709089686113, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8538709089686113\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.06100259055107654, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.06100259055107654\n",
      "[I 2025-06-03 12:12:40,261] Trial 18 finished with value: 0.6479554299598419 and parameters: {'n_estimators': 700, 'learning_rate': 0.02164969595363055, 'num_leaves': 31, 'max_depth': 10, 'min_child_samples': 38, 'feature_fraction': 0.8538709089686113, 'bagging_fraction': 0.9894146486884396, 'bagging_freq': 1, 'reg_alpha': 0.300473067069012, 'reg_lambda': 0.0014301263175626495, 'min_gain_to_split': 0.06100259055107654}. Best is trial 18 with value: 0.6479554299598419.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9731446548999685, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9731446548999685\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8708438349766738, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8708438349766738\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.06257392278989554, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.06257392278989554\n",
      "[I 2025-06-03 12:12:40,438] Trial 19 finished with value: 0.6735728980643961 and parameters: {'n_estimators': 700, 'learning_rate': 0.020460280169100516, 'num_leaves': 30, 'max_depth': 10, 'min_child_samples': 22, 'feature_fraction': 0.8708438349766738, 'bagging_fraction': 0.9731446548999685, 'bagging_freq': 1, 'reg_alpha': 0.06578651565684264, 'reg_lambda': 0.0011083376879461606, 'min_gain_to_split': 0.06257392278989554}. Best is trial 18 with value: 0.6479554299598419.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8495399231101592, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8495399231101592\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6025565385596322, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6025565385596322\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.05260748579896332, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.05260748579896332\n",
      "[I 2025-06-03 12:12:40,646] Trial 20 finished with value: 0.6615346306462223 and parameters: {'n_estimators': 600, 'learning_rate': 0.01480125329980439, 'num_leaves': 41, 'max_depth': 10, 'min_child_samples': 38, 'feature_fraction': 0.6025565385596322, 'bagging_fraction': 0.8495399231101592, 'bagging_freq': 1, 'reg_alpha': 0.23873254376306483, 'reg_lambda': 0.0017204224802308075, 'min_gain_to_split': 0.05260748579896332}. Best is trial 18 with value: 0.6479554299598419.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6705541479611306, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6705541479611306\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8054110802831455, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8054110802831455\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.05732612889222273, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.05732612889222273\n",
      "[I 2025-06-03 12:12:40,784] Trial 21 finished with value: 0.6549017683700693 and parameters: {'n_estimators': 850, 'learning_rate': 0.034545253281573984, 'num_leaves': 32, 'max_depth': 7, 'min_child_samples': 44, 'feature_fraction': 0.8054110802831455, 'bagging_fraction': 0.6705541479611306, 'bagging_freq': 0, 'reg_alpha': 0.35470071759181476, 'reg_lambda': 0.0034112310987082155, 'min_gain_to_split': 0.05732612889222273}. Best is trial 18 with value: 0.6479554299598419.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6830344970747535, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6830344970747535\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7193832757199132, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7193832757199132\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.055827971848118385, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.055827971848118385\n",
      "[I 2025-06-03 12:12:41,000] Trial 22 finished with value: 0.6608882650830966 and parameters: {'n_estimators': 850, 'learning_rate': 0.039528315385825456, 'num_leaves': 30, 'max_depth': 8, 'min_child_samples': 35, 'feature_fraction': 0.7193832757199132, 'bagging_fraction': 0.6830344970747535, 'bagging_freq': 0, 'reg_alpha': 0.17420762779063623, 'reg_lambda': 0.004397821238221396, 'min_gain_to_split': 0.055827971848118385}. Best is trial 18 with value: 0.6479554299598419.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9905131845336743, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9905131845336743\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8258611122516746, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8258611122516746\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.06692163466337997, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.06692163466337997\n",
      "[I 2025-06-03 12:12:41,120] Trial 23 finished with value: 0.6472178643779551 and parameters: {'n_estimators': 700, 'learning_rate': 0.050162313119512754, 'num_leaves': 33, 'max_depth': 7, 'min_child_samples': 42, 'feature_fraction': 0.8258611122516746, 'bagging_fraction': 0.9905131845336743, 'bagging_freq': 0, 'reg_alpha': 0.6536165070423263, 'reg_lambda': 0.001086426825123727, 'min_gain_to_split': 0.06692163466337997}. Best is trial 23 with value: 0.6472178643779551.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9986166433495135, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9986166433495135\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9224304362302653, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9224304362302653\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.07079312603886044, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.07079312603886044\n",
      "[I 2025-06-03 12:12:41,227] Trial 24 finished with value: 0.654423978282075 and parameters: {'n_estimators': 700, 'learning_rate': 0.07377319348492194, 'num_leaves': 35, 'max_depth': 8, 'min_child_samples': 31, 'feature_fraction': 0.9224304362302653, 'bagging_fraction': 0.9986166433495135, 'bagging_freq': 1, 'reg_alpha': 0.6929442450817288, 'reg_lambda': 0.0012327078011426405, 'min_gain_to_split': 0.07079312603886044}. Best is trial 23 with value: 0.6472178643779551.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9483417961054528, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9483417961054528\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8485877854254439, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8485877854254439\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.06896304200798922, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.06896304200798922\n",
      "[I 2025-06-03 12:12:41,326] Trial 25 finished with value: 0.6667620517046952 and parameters: {'n_estimators': 600, 'learning_rate': 0.06880945628426448, 'num_leaves': 25, 'max_depth': 7, 'min_child_samples': 40, 'feature_fraction': 0.8485877854254439, 'bagging_fraction': 0.9483417961054528, 'bagging_freq': 0, 'reg_alpha': 0.047660859063572196, 'reg_lambda': 0.0025278401818138105, 'min_gain_to_split': 0.06896304200798922}. Best is trial 23 with value: 0.6472178643779551.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9105582960114811, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9105582960114811\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9010128539354152, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9010128539354152\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04205287030713489, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04205287030713489\n",
      "[I 2025-06-03 12:12:41,434] Trial 26 finished with value: 0.6566322317633655 and parameters: {'n_estimators': 700, 'learning_rate': 0.04802696225288701, 'num_leaves': 40, 'max_depth': 10, 'min_child_samples': 29, 'feature_fraction': 0.9010128539354152, 'bagging_fraction': 0.9105582960114811, 'bagging_freq': 1, 'reg_alpha': 0.8035555812726977, 'reg_lambda': 0.0010266027041803004, 'min_gain_to_split': 0.04205287030713489}. Best is trial 23 with value: 0.6472178643779551.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.830663534861929, subsample=1.0 will be ignored. Current value: bagging_fraction=0.830663534861929\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8236191700444206, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8236191700444206\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.05673415537215503, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.05673415537215503\n",
      "[I 2025-06-03 12:12:41,566] Trial 27 finished with value: 0.656003093675586 and parameters: {'n_estimators': 750, 'learning_rate': 0.04887948298881714, 'num_leaves': 32, 'max_depth': 5, 'min_child_samples': 36, 'feature_fraction': 0.8236191700444206, 'bagging_fraction': 0.830663534861929, 'bagging_freq': 2, 'reg_alpha': 0.003193930900590354, 'reg_lambda': 0.005949804337539621, 'min_gain_to_split': 0.05673415537215503}. Best is trial 23 with value: 0.6472178643779551.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9630525561766238, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9630525561766238\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7550688545633955, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7550688545633955\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.03250147943413116, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.03250147943413116\n",
      "[I 2025-06-03 12:12:41,756] Trial 28 finished with value: 0.6452714637070879 and parameters: {'n_estimators': 550, 'learning_rate': 0.024183847115206485, 'num_leaves': 34, 'max_depth': 8, 'min_child_samples': 50, 'feature_fraction': 0.7550688545633955, 'bagging_fraction': 0.9630525561766238, 'bagging_freq': 0, 'reg_alpha': 0.00955572641324907, 'reg_lambda': 0.001643792646805036, 'min_gain_to_split': 0.03250147943413116}. Best is trial 28 with value: 0.6452714637070879.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9986477815899895, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9986477815899895\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7648769831761683, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7648769831761683\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.030285527247142716, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.030285527247142716\n",
      "[I 2025-06-03 12:12:41,887] Trial 29 finished with value: 0.6506985916623702 and parameters: {'n_estimators': 500, 'learning_rate': 0.023860688759554764, 'num_leaves': 43, 'max_depth': 9, 'min_child_samples': 50, 'feature_fraction': 0.7648769831761683, 'bagging_fraction': 0.9986477815899895, 'bagging_freq': 1, 'reg_alpha': 0.011686340349447207, 'reg_lambda': 0.0014789679931870728, 'min_gain_to_split': 0.030285527247142716}. Best is trial 28 with value: 0.6452714637070879.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9624115882351703, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9624115882351703\n",
      "[LightGBM] [Warning] feature_fraction is set=0.894843680505444, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.894843680505444\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.014900621848436369, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.014900621848436369\n",
      "[I 2025-06-03 12:12:42,040] Trial 30 finished with value: 0.6441826602430679 and parameters: {'n_estimators': 400, 'learning_rate': 0.02027777177970779, 'num_leaves': 24, 'max_depth': 8, 'min_child_samples': 48, 'feature_fraction': 0.894843680505444, 'bagging_fraction': 0.9624115882351703, 'bagging_freq': 3, 'reg_alpha': 0.04728345421808565, 'reg_lambda': 0.002799219726654241, 'min_gain_to_split': 0.014900621848436369}. Best is trial 30 with value: 0.6441826602430679.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9544637518876657, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9544637518876657\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8962281596959947, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8962281596959947\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.016651196634736153, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.016651196634736153\n",
      "[I 2025-06-03 12:12:42,176] Trial 31 finished with value: 0.6505732655821442 and parameters: {'n_estimators': 400, 'learning_rate': 0.017941925451655703, 'num_leaves': 27, 'max_depth': 8, 'min_child_samples': 48, 'feature_fraction': 0.8962281596959947, 'bagging_fraction': 0.9544637518876657, 'bagging_freq': 4, 'reg_alpha': 0.04587945734339305, 'reg_lambda': 0.0023694596263554102, 'min_gain_to_split': 0.016651196634736153}. Best is trial 30 with value: 0.6441826602430679.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.918364195610942, subsample=1.0 will be ignored. Current value: bagging_fraction=0.918364195610942\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9593511237357383, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9593511237357383\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.01211939256102865, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.01211939256102865\n",
      "[I 2025-06-03 12:12:42,316] Trial 32 finished with value: 0.6540888488784224 and parameters: {'n_estimators': 350, 'learning_rate': 0.025291572314954727, 'num_leaves': 22, 'max_depth': 10, 'min_child_samples': 43, 'feature_fraction': 0.9593511237357383, 'bagging_fraction': 0.918364195610942, 'bagging_freq': 3, 'reg_alpha': 0.01496110913702408, 'reg_lambda': 0.0029874251282987732, 'min_gain_to_split': 0.01211939256102865}. Best is trial 30 with value: 0.6441826602430679.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9595934975459947, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9595934975459947\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8314644039131134, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8314644039131134\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.021309621067944817, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.021309621067944817\n",
      "[I 2025-06-03 12:12:42,450] Trial 33 finished with value: 0.6514587896542456 and parameters: {'n_estimators': 550, 'learning_rate': 0.021211026441267995, 'num_leaves': 33, 'max_depth': 9, 'min_child_samples': 42, 'feature_fraction': 0.8314644039131134, 'bagging_fraction': 0.9595934975459947, 'bagging_freq': 0, 'reg_alpha': 0.16473125129322974, 'reg_lambda': 0.001614024777547583, 'min_gain_to_split': 0.021309621067944817}. Best is trial 30 with value: 0.6441826602430679.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8954541809430488, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8954541809430488\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8899504342504323, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8899504342504323\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.032853237626569734, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.032853237626569734\n",
      "[I 2025-06-03 12:12:42,639] Trial 34 finished with value: 0.6573670569756964 and parameters: {'n_estimators': 550, 'learning_rate': 0.015000480513575882, 'num_leaves': 28, 'max_depth': 8, 'min_child_samples': 37, 'feature_fraction': 0.8899504342504323, 'bagging_fraction': 0.8954541809430488, 'bagging_freq': 2, 'reg_alpha': 0.30245721695072586, 'reg_lambda': 0.006967119707151433, 'min_gain_to_split': 0.032853237626569734}. Best is trial 30 with value: 0.6441826602430679.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9825052192955376, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9825052192955376\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9270538033421147, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9270538033421147\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04917191560219372, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04917191560219372\n",
      "[I 2025-06-03 12:12:42,784] Trial 35 finished with value: 0.6410830521004738 and parameters: {'n_estimators': 650, 'learning_rate': 0.03292748464765017, 'num_leaves': 22, 'max_depth': 6, 'min_child_samples': 50, 'feature_fraction': 0.9270538033421147, 'bagging_fraction': 0.9825052192955376, 'bagging_freq': 1, 'reg_alpha': 0.9847069875010993, 'reg_lambda': 0.012542656978349132, 'min_gain_to_split': 0.04917191560219372}. Best is trial 35 with value: 0.6410830521004738.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9266616462449784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9266616462449784\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9364788321751102, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9364788321751102\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04752217659585742, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04752217659585742\n",
      "[I 2025-06-03 12:12:42,887] Trial 36 finished with value: 0.6514976933622731 and parameters: {'n_estimators': 450, 'learning_rate': 0.03235042505625045, 'num_leaves': 23, 'max_depth': 6, 'min_child_samples': 49, 'feature_fraction': 0.9364788321751102, 'bagging_fraction': 0.9266616462449784, 'bagging_freq': 0, 'reg_alpha': 0.0037119634166293044, 'reg_lambda': 0.016244997015089677, 'min_gain_to_split': 0.04752217659585742}. Best is trial 35 with value: 0.6410830521004738.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9437350749058647, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9437350749058647\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9925742213748159, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9925742213748159\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.03606431090788601, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.03606431090788601\n",
      "[I 2025-06-03 12:12:43,137] Trial 37 finished with value: 0.6417071540176397 and parameters: {'n_estimators': 600, 'learning_rate': 0.017164254798312528, 'num_leaves': 14, 'max_depth': 7, 'min_child_samples': 45, 'feature_fraction': 0.9925742213748159, 'bagging_fraction': 0.9437350749058647, 'bagging_freq': 1, 'reg_alpha': 0.6249077810397432, 'reg_lambda': 0.0652835698672361, 'min_gain_to_split': 0.03606431090788601}. Best is trial 35 with value: 0.6410830521004738.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8612006133326092, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8612006133326092\n",
      "[LightGBM] [Warning] feature_fraction is set=0.999427508529635, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.999427508529635\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.035524564311502464, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.035524564311502464\n",
      "[I 2025-06-03 12:12:43,298] Trial 38 finished with value: 0.6704366207972509 and parameters: {'n_estimators': 600, 'learning_rate': 0.0168414371798098, 'num_leaves': 13, 'max_depth': 5, 'min_child_samples': 5, 'feature_fraction': 0.999427508529635, 'bagging_fraction': 0.8612006133326092, 'bagging_freq': 2, 'reg_alpha': 0.9777697672082901, 'reg_lambda': 0.1889791286542655, 'min_gain_to_split': 0.035524564311502464}. Best is trial 35 with value: 0.6410830521004738.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9412242994269233, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9412242994269233\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9662957052218965, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9662957052218965\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.02395068207039097, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.02395068207039097\n",
      "[I 2025-06-03 12:12:43,457] Trial 39 finished with value: 0.6487622850402561 and parameters: {'n_estimators': 400, 'learning_rate': 0.012158348385960414, 'num_leaves': 14, 'max_depth': 3, 'min_child_samples': 45, 'feature_fraction': 0.9662957052218965, 'bagging_fraction': 0.9412242994269233, 'bagging_freq': 3, 'reg_alpha': 0.02182895292675254, 'reg_lambda': 0.12242464871178069, 'min_gain_to_split': 0.02395068207039097}. Best is trial 35 with value: 0.6410830521004738.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8194605581751677, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8194605581751677\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9743972274236626, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9743972274236626\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.006852012844736244, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.006852012844736244\n",
      "[I 2025-06-03 12:12:43,638] Trial 40 finished with value: 0.6546028577550904 and parameters: {'n_estimators': 200, 'learning_rate': 0.012744137189272016, 'num_leaves': 16, 'max_depth': 7, 'min_child_samples': 50, 'feature_fraction': 0.9743972274236626, 'bagging_fraction': 0.8194605581751677, 'bagging_freq': 1, 'reg_alpha': 0.008433458607626206, 'reg_lambda': 0.06653696336731091, 'min_gain_to_split': 0.006852012844736244}. Best is trial 35 with value: 0.6410830521004738.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9727310298208878, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9727310298208878\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9121022995084509, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9121022995084509\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04305694696558561, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04305694696558561\n",
      "[I 2025-06-03 12:12:43,758] Trial 41 finished with value: 0.6526755432190635 and parameters: {'n_estimators': 650, 'learning_rate': 0.02700376222524875, 'num_leaves': 22, 'max_depth': 6, 'min_child_samples': 47, 'feature_fraction': 0.9121022995084509, 'bagging_fraction': 0.9727310298208878, 'bagging_freq': 0, 'reg_alpha': 0.5441919028677394, 'reg_lambda': 0.011504742960119024, 'min_gain_to_split': 0.04305694696558561}. Best is trial 35 with value: 0.6410830521004738.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9105399555136301, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9105399555136301\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9333334390600997, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9333334390600997\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04937447627010796, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04937447627010796\n",
      "[I 2025-06-03 12:12:43,893] Trial 42 finished with value: 0.6505843900448126 and parameters: {'n_estimators': 650, 'learning_rate': 0.018566592691105346, 'num_leaves': 10, 'max_depth': 7, 'min_child_samples': 45, 'feature_fraction': 0.9333334390600997, 'bagging_fraction': 0.9105399555136301, 'bagging_freq': 0, 'reg_alpha': 0.5992107741014571, 'reg_lambda': 0.05016825114458719, 'min_gain_to_split': 0.04937447627010796}. Best is trial 35 with value: 0.6410830521004738.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9734817801502541, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9734817801502541\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8792552293577941, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8792552293577941\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.03980695633136062, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.03980695633136062\n",
      "[I 2025-06-03 12:12:44,060] Trial 43 finished with value: 0.6511746497863371 and parameters: {'n_estimators': 450, 'learning_rate': 0.013471845379083792, 'num_leaves': 28, 'max_depth': 8, 'min_child_samples': 48, 'feature_fraction': 0.8792552293577941, 'bagging_fraction': 0.9734817801502541, 'bagging_freq': 2, 'reg_alpha': 0.410826270216858, 'reg_lambda': 0.11087961017318901, 'min_gain_to_split': 0.03980695633136062}. Best is trial 35 with value: 0.6410830521004738.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8888496503606174, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8888496503606174\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7657785215484164, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7657785215484164\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.07793421299318776, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.07793421299318776\n",
      "[I 2025-06-03 12:12:44,273] Trial 44 finished with value: 0.6578345291461855 and parameters: {'n_estimators': 550, 'learning_rate': 0.010723613785208856, 'num_leaves': 35, 'max_depth': 7, 'min_child_samples': 41, 'feature_fraction': 0.7657785215484164, 'bagging_fraction': 0.8888496503606174, 'bagging_freq': 1, 'reg_alpha': 0.005226089931296805, 'reg_lambda': 0.020851649438274528, 'min_gain_to_split': 0.07793421299318776}. Best is trial 35 with value: 0.6410830521004738.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9365643663047964, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9365643663047964\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6921592559442173, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6921592559442173\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.026191447567690887, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.026191447567690887\n",
      "[I 2025-06-03 12:12:44,459] Trial 45 finished with value: 0.6812118471297891 and parameters: {'n_estimators': 500, 'learning_rate': 0.02259222043752724, 'num_leaves': 20, 'max_depth': 6, 'min_child_samples': 11, 'feature_fraction': 0.6921592559442173, 'bagging_fraction': 0.9365643663047964, 'bagging_freq': 0, 'reg_alpha': 0.9810133584988838, 'reg_lambda': 0.011039989716821899, 'min_gain_to_split': 0.026191447567690887}. Best is trial 35 with value: 0.6410830521004738.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9779758663352496, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9779758663352496\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9344585891790013, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9344585891790013\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.018337110153949527, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.018337110153949527\n",
      "[I 2025-06-03 12:12:44,577] Trial 46 finished with value: 0.6549937424781551 and parameters: {'n_estimators': 350, 'learning_rate': 0.031038557545002007, 'num_leaves': 15, 'max_depth': 9, 'min_child_samples': 44, 'feature_fraction': 0.9344585891790013, 'bagging_fraction': 0.9779758663352496, 'bagging_freq': 2, 'reg_alpha': 0.4873348576757426, 'reg_lambda': 0.0017911717835534648, 'min_gain_to_split': 0.018337110153949527}. Best is trial 35 with value: 0.6410830521004738.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9558120993942458, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9558120993942458\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8205814011834524, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8205814011834524\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04565990140685325, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04565990140685325\n",
      "[I 2025-06-03 12:12:44,723] Trial 47 finished with value: 0.6334877025838145 and parameters: {'n_estimators': 600, 'learning_rate': 0.037677130751088576, 'num_leaves': 24, 'max_depth': 5, 'min_child_samples': 48, 'feature_fraction': 0.8205814011834524, 'bagging_fraction': 0.9558120993942458, 'bagging_freq': 5, 'reg_alpha': 0.036165492913113516, 'reg_lambda': 0.0033852492069886535, 'min_gain_to_split': 0.04565990140685325}. Best is trial 47 with value: 0.6334877025838145.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.899576183725351, subsample=1.0 will be ignored. Current value: bagging_fraction=0.899576183725351\n",
      "[LightGBM] [Warning] feature_fraction is set=0.947375584319092, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.947375584319092\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.03599927181144037, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.03599927181144037\n",
      "[I 2025-06-03 12:12:44,809] Trial 48 finished with value: 0.6534879927287268 and parameters: {'n_estimators': 600, 'learning_rate': 0.038586859547369824, 'num_leaves': 24, 'max_depth': 3, 'min_child_samples': 48, 'feature_fraction': 0.947375584319092, 'bagging_fraction': 0.899576183725351, 'bagging_freq': 5, 'reg_alpha': 0.035989003219871224, 'reg_lambda': 0.007805113657634083, 'min_gain_to_split': 0.03599927181144037}. Best is trial 47 with value: 0.6334877025838145.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9323661546175879, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9323661546175879\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7253024966850661, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7253024966850661\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04392022430524379, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04392022430524379\n",
      "[I 2025-06-03 12:12:44,956] Trial 49 finished with value: 0.6448968027382078 and parameters: {'n_estimators': 650, 'learning_rate': 0.026577652044987323, 'num_leaves': 21, 'max_depth': 4, 'min_child_samples': 50, 'feature_fraction': 0.7253024966850661, 'bagging_fraction': 0.9323661546175879, 'bagging_freq': 6, 'reg_alpha': 0.014744439685880269, 'reg_lambda': 0.0029570759491638126, 'min_gain_to_split': 0.04392022430524379}. Best is trial 47 with value: 0.6334877025838145.\n",
      "Best Optuna trial for LightGBM: Value=0.6335, Params={'n_estimators': 600, 'learning_rate': 0.037677130751088576, 'num_leaves': 24, 'max_depth': 5, 'min_child_samples': 48, 'feature_fraction': 0.8205814011834524, 'bagging_fraction': 0.9558120993942458, 'bagging_freq': 5, 'reg_alpha': 0.036165492913113516, 'reg_lambda': 0.0033852492069886535, 'min_gain_to_split': 0.04565990140685325}\n",
      "\n",
      "--- LightGBM Model Training: GOOGL ---\n",
      "Training LightGBM model...\n",
      "Using Optuna-optimized parameters.\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9558120993942458, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9558120993942458\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8205814011834524, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8205814011834524\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.04565990140685325, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.04565990140685325\n",
      "\n",
      "Top 10 features:\n",
      "                        Feature  Importance\n",
      "17                 kurtosis_20          61\n",
      "31  close_quarterly_volatility          42\n",
      "25                 AROONOSC_14          21\n",
      "18               volatility_50          19\n",
      "0                MACDs_12_26_9          18\n",
      "26               STCD_23_50_05          16\n",
      "11                   BBL_20_20          15\n",
      "19                     skew_50          14\n",
      "21              close_cwt_mean          13\n",
      "32           sma_10_regime_adj          12\n",
      "\n",
      "🎯 Accuracy on mapped test data: 0.5893\n",
      "📊 AUC: 0.6648\n",
      "\n",
      "Classification Report (on mapped and valid test labels):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.90      0.73        68\n",
      "           1       0.42      0.11      0.18        44\n",
      "\n",
      "    accuracy                           0.59       112\n",
      "   macro avg       0.51      0.51      0.45       112\n",
      "weighted avg       0.53      0.59      0.51       112\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35088/1678251532.py:938: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  ax = sns.barplot(x='Plot_Importance', y='Feature', data=plot_data, palette=\"viridis\", orient='h')\n",
      "The maximum opset needed by this model is only 9.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance plot saved to feature_importance_GOOGL.png\n",
      "\n",
      "Exporting LGBM model to ONNX: lgbm_model_GOOGL.onnx (opset=12)\n",
      "Model exported to ONNX: lgbm_model_GOOGL.onnx\n",
      "ONNX model check OK.\n",
      "Autoformer forecast for GOOGL: [172.77906926 171.45177371 172.60277009 171.99249383 172.54494512]\n",
      "\n",
      "--- Foundation Model Forecasting (PatchTST): GOOGL ---\n",
      "\n",
      "--- Forecasting for GOOGL using PatchTST (ibm-research/patchtst-etth1-pretrain) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight transfer warning/error: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
      "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434. Model may use more random init for some layers.\n",
      "Epoch 1/10 | Loss: 0.000429 | LR: 0.000098\n",
      "Epoch 2/10 | Loss: 0.000427 | LR: 0.000090\n",
      "Epoch 3/10 | Loss: 0.000426 | LR: 0.000079\n",
      "Epoch 4/10 | Loss: 0.000425 | LR: 0.000065\n",
      "Epoch 5/10 | Loss: 0.000416 | LR: 0.000050\n",
      "Epoch 6/10 | Loss: 0.000404 | LR: 0.000035\n",
      "Epoch 7/10 | Loss: 0.000415 | LR: 0.000021\n",
      "Epoch 8/10 | Loss: 0.000420 | LR: 0.000010\n",
      "Epoch 9/10 | Loss: 0.000410 | LR: 0.000002\n",
      "Early stopping at epoch 9\n",
      "Fine-tuning completed. Best loss: 0.000404\n",
      "PatchTST Forecast for GOOGL: [168.36952209472656, 168.96121215820312, 170.31472778320312, 171.6829376220703, 172.35374450683594]\n",
      "  Direction: 📈 UP, Magnitude: 1.97%\n",
      "\n",
      "🏁 Workflow completed for GOOGL. Final Status: LightGBM Model Trained\n",
      "\n",
      "--- Results Summary for GOOGL ---\n",
      "  Overall Status: LightGBM Model Trained\n",
      "  Raw Data Shape: (750, 5)\n",
      "  Featured Data Shape: (750, 105)\n",
      "  Selected Features Count: 35\n",
      "  Top LGBM Features: ['kurtosis_20', 'close_quarterly_volatility', 'AROONOSC_14']\n",
      "  PatchTST Forecast (PatchTST Fine-tuned (10 epochs attempted)):\n",
      "    Values: [168.36952209472656, 168.96121215820312, 170.31472778320312, 171.6829376220703, 172.35374450683594]\n",
      "    Direction: 📈 UP, Magnitude: 1.97%\n",
      "  Autoformer Forecast: [172.77906926066208, 171.45177371452243, 172.60277008869812, 171.99249382641545, 172.54494512136827]\n",
      "  ONNX Model Path: lgbm_model_GOOGL.onnx\n",
      "----------------------------------------\n",
      "\n",
      "Total execution time for 2 symbol(s): 15.32 seconds.\n",
      "DEBUG: Script __main__ block finished.\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_ta\n",
    "# import yfinance as yf # Not strictly used if Twelve Data is primary\n",
    "from datetime import datetime, timedelta\n",
    "import pywt\n",
    "import antropy as ant\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import TimeSeriesSplit # Not directly used in main flow, Optuna handles its split.\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss, roc_auc_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, VarianceThreshold\n",
    "# import gc # Unused\n",
    "\n",
    "# Foundational Model Imports\n",
    "from transformers import PatchTSTConfig, PatchTSTForPrediction # Removed Trainer, TrainingArguments, EarlyStoppingCallback, set_seed\n",
    "\n",
    "# --- Global Flags and Initializations for Optional Libraries ---\n",
    "optuna_available = False\n",
    "torch_available = False\n",
    "# sktime_available = False # Becomes irrelevant if add_clasp_regimes is removed\n",
    "# ClaSPSegmentation = None # Becomes irrelevant\n",
    "dowhy_available = False\n",
    "CausalModel = None\n",
    "nx = None\n",
    "# ONNX related flags and types\n",
    "onnx_available = False\n",
    "skl2onnx_available = False\n",
    "onnxmltools_available = False\n",
    "FloatTensorType = None # Crucial to initialize globally for the check later\n",
    "SKIP_CAUSAL_ANALYSIS_FOR_DEBUGGING = True # Set to False to enable Causal Analysis\n",
    "\n",
    "if hasattr(pd.DataFrame, 'ta') is False and pandas_ta is not None:\n",
    "    try:\n",
    "        pd.DataFrame.ta = pandas_ta.Core(df=None)\n",
    "        print(\"pandas_ta DataFrame accessor registered globally.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not globally register pandas_ta accessor: {e}\")\n",
    "\n",
    "try:\n",
    "    import optuna\n",
    "    optuna_available = True\n",
    "    print(\"Optuna imported successfully.\")\n",
    "except ImportError:\n",
    "    print(\"Optuna not found. LightGBM hyperparameter optimization with Optuna will be skipped.\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    torch_available = True\n",
    "    print(\"PyTorch imported successfully.\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"PyTorch CUDA available: True, Version: {torch.version.cuda}\")\n",
    "        print(f\"Using PyTorch on GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"PyTorch CUDA available: False.\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch not found. Custom Transformer and Foundation Model features will be SKIPPED.\")\n",
    "\n",
    "try:\n",
    "    import dowhy\n",
    "    from dowhy import CausalModel\n",
    "    import networkx as nx # networkx is often used with dowhy\n",
    "    dowhy_available = True\n",
    "    print(f\"DoWhy {dowhy.__version__} and NetworkX {nx.__version__} imported successfully.\")\n",
    "except ImportError:\n",
    "    print(\"DoWhy or NetworkX not found. Causal Discovery will be skipped.\")\n",
    "\n",
    "try:\n",
    "    import onnx\n",
    "    onnx_available = True\n",
    "    # import onnxruntime as ort\n",
    "    import skl2onnx\n",
    "    skl2onnx_available = True\n",
    "    from skl2onnx.common.data_types import FloatTensorType # Assigns to global FloatTensorType\n",
    "    import onnxmltools\n",
    "    onnxmltools_available = True\n",
    "    print(\"ONNX, ONNXRuntime, skl2onnx, and onnxmltools imported successfully.\")\n",
    "    if hasattr(onnxmltools, '__version__'):\n",
    "          print(f\"Onnxmltools version: {onnxmltools.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"One or more ONNX components not found: {e}. ONNX features will be skipped.\")\n",
    "\n",
    "print(\"\\nAll libraries and modules conditional imports attempted.\")\n",
    "\n",
    "# --- Constants ---\n",
    "TWELVE_DATA_API_KEY = \"b6dbb92e551a46f2b20de27540aeef0a\" # Replace with your actual key\n",
    "API_KEY = TWELVE_DATA_API_KEY # Ensure this is set\n",
    "DEFAULT_SYMBOL = \"MSFT\"\n",
    "START_DATE = (datetime.now() - timedelta(days=3*365)).strftime('%Y-%m-%d')\n",
    "END_DATE = datetime.now().strftime('%Y-%m-%d')\n",
    "AUTOFORMER_MODEL_PATH = \"autoformer_predictor_trained.pth\" # For optional Autoformer\n",
    "\n",
    "# --- Function Definitions (AutoformerPredictor and its helpers - Conditionally Used) ---\n",
    "class AutoformerPredictor:\n",
    "    def __init__(self, input_len=60, pred_len=5, d_model=64, n_heads=8, num_encoder_layers=2, num_decoder_layers=1, dim_feedforward=256):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() and torch_available else 'cpu'\n",
    "        self.model = None\n",
    "        self.input_len = input_len\n",
    "        self.pred_len = pred_len\n",
    "        self.d_model = d_model\n",
    "\n",
    "        if torch_available:\n",
    "            # print(f\"🔧 Initializing Custom AutoformerPredictor on device: {self.device}\") # Less verbose\n",
    "            self.model = nn.Transformer(\n",
    "                d_model=d_model, nhead=n_heads, num_encoder_layers=num_encoder_layers,\n",
    "                num_decoder_layers=num_decoder_layers, dim_feedforward=dim_feedforward,\n",
    "                activation='gelu', batch_first=False #Transformer expects seq_len first by default\n",
    "            ).to(self.device)\n",
    "            self.enc_embedding = nn.Linear(1, d_model).to(self.device)\n",
    "            self.dec_embedding = nn.Linear(1, d_model).to(self.device)\n",
    "            self.projection = nn.Linear(d_model, 1).to(self.device)\n",
    "        else:\n",
    "            print(\"PyTorch not available. AutoformerPredictor cannot be initialized.\")\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        if not torch_available: return None\n",
    "        mask = (torch.triu(torch.ones(sz, sz, device=self.device)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt): # src: (seq_len, batch, features), tgt: (tgt_len, batch, features)\n",
    "        if not torch_available or self.model is None: return None\n",
    "        src_embedded = self.enc_embedding(src)\n",
    "        tgt_embedded = self.dec_embedding(tgt)\n",
    "        tgt_mask = self._generate_square_subsequent_mask(tgt.size(0))\n",
    "        output = self.model(src_embedded, tgt_embedded, tgt_mask=tgt_mask)\n",
    "        output = self.projection(output)\n",
    "        return output\n",
    "\n",
    "    def train_model(self, train_loader, epochs=10, learning_rate=0.001):\n",
    "        if not torch_available or self.model is None: print(\"Cannot train: PyTorch or model not available.\"); return\n",
    "        if not train_loader: print(\"Cannot train: No data loader provided.\"); return\n",
    "        print(f\"Starting AutoformerPredictor training for {epochs} epochs...\")\n",
    "        self.model.train()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.MSELoss()\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for i, (batch_src, batch_tgt_input, batch_tgt_output) in enumerate(train_loader):\n",
    "                batch_src = batch_src.float().to(self.device).permute(1,0,2)\n",
    "                batch_tgt_input = batch_tgt_input.float().to(self.device).permute(1,0,2)\n",
    "                batch_tgt_output = batch_tgt_output.float().to(self.device).permute(1,0,2)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                predictions = self.forward(batch_src, batch_tgt_input)\n",
    "                if predictions is None: continue\n",
    "                loss = criterion(predictions, batch_tgt_output)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            avg_epoch_loss = epoch_loss / len(train_loader) if len(train_loader) > 0 else float('nan')\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_epoch_loss:.4f}\")\n",
    "        print(\"Training finished.\"); self.save_model(AUTOFORMER_MODEL_PATH)\n",
    "\n",
    "    def predict(self, series_data): # Expects 1D series_data\n",
    "        if not torch_available or self.model is None: print(\"Cannot predict: PyTorch or model not available.\"); return np.array([np.nan] * self.pred_len)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            if isinstance(series_data, pd.Series): series_data = series_data.values\n",
    "            if isinstance(series_data, list): series_data = np.array(series_data)\n",
    "            if not isinstance(series_data, np.ndarray): print(f\"Unsupported series data type for predict: {type(series_data)}\"); return np.array([np.nan] * self.pred_len)\n",
    "            if series_data.ndim > 1: series_data = series_data.squeeze()\n",
    "            if series_data.ndim == 0: print(f\"Series data is scalar, cannot process.\"); return np.array([np.nan]*self.pred_len)\n",
    "\n",
    "            mean_val = series_data[-self.input_len:].mean() if len(series_data) >= self.input_len else series_data.mean()\n",
    "            std_val = series_data[-self.input_len:].std() if len(series_data) >= self.input_len else series_data.std()\n",
    "            std_val = std_val if std_val > 1e-8 else 1e-8\n",
    "\n",
    "            if len(series_data) < self.input_len:\n",
    "                padding_val = series_data[0] if len(series_data) > 0 else 0\n",
    "                norm_series_np = (np.concatenate([np.full(self.input_len - len(series_data), padding_val), series_data]) - mean_val) / std_val\n",
    "            else:\n",
    "                norm_series_np = (series_data[-self.input_len:] - mean_val) / std_val\n",
    "\n",
    "            src_tensor = torch.tensor(norm_series_np, dtype=torch.float32).unsqueeze(0).unsqueeze(-1).permute(1,0,2).to(self.device)\n",
    "            current_tgt_sequence_normalized = torch.zeros((1, 1, 1), dtype=torch.float32, device=self.device)\n",
    "            predictions_normalized_list = []\n",
    "\n",
    "            for _ in range(self.pred_len):\n",
    "                prediction_step_output = self.forward(src_tensor, current_tgt_sequence_normalized)\n",
    "                if prediction_step_output is None: return np.array([np.nan] * self.pred_len)\n",
    "                last_predicted_value_normalized = prediction_step_output[-1:, :, :]\n",
    "                predictions_normalized_list.append(last_predicted_value_normalized.squeeze().cpu().item())\n",
    "                current_tgt_sequence_normalized = torch.cat([current_tgt_sequence_normalized, last_predicted_value_normalized], dim=0)\n",
    "\n",
    "            denorm_preds = (np.array(predictions_normalized_list) * std_val) + mean_val\n",
    "            return denorm_preds\n",
    "\n",
    "    def save_model(self, path):\n",
    "        if not torch_available or self.model is None: return\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'enc_embedding_state_dict': self.enc_embedding.state_dict(),\n",
    "            'dec_embedding_state_dict': self.dec_embedding.state_dict(),\n",
    "            'projection_state_dict': self.projection.state_dict(),\n",
    "            'input_len': self.input_len,\n",
    "            'pred_len': self.pred_len,\n",
    "            'd_model': self.d_model\n",
    "        }, path)\n",
    "        print(f\"AutoformerPredictor model saved to {path}\")\n",
    "\n",
    "    def load_model(self, path):\n",
    "        if not torch_available: print(\"Cannot load model: PyTorch not available.\"); return False\n",
    "        if not os.path.exists(path): print(f\"Model file not found: {path}. New model needed.\"); return False\n",
    "        try:\n",
    "            checkpoint = torch.load(path, map_location=self.device)\n",
    "            self.input_len = checkpoint.get('input_len', self.input_len)\n",
    "            self.pred_len = checkpoint.get('pred_len', self.pred_len)\n",
    "            saved_d_model = checkpoint.get('d_model', self.d_model)\n",
    "            self.__init__(input_len=self.input_len, pred_len=self.pred_len, d_model=saved_d_model)\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.enc_embedding.load_state_dict(checkpoint['enc_embedding_state_dict'])\n",
    "            self.dec_embedding.load_state_dict(checkpoint['dec_embedding_state_dict'])\n",
    "            self.projection.load_state_dict(checkpoint['projection_state_dict'])\n",
    "            self.model.eval()\n",
    "            print(f\"AutoformerPredictor model loaded from {path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Autoformer model: {e}. A new model might be needed.\")\n",
    "            return False\n",
    "\n",
    "def create_sequences(data, input_len, pred_len): # For Autoformer training\n",
    "    X, y_input, y_output = [], [], []\n",
    "    for i in range(len(data) - input_len - pred_len + 1):\n",
    "        X.append(data[i:(i + input_len)])\n",
    "        y_dec_inp_seq = np.concatenate(([data[i + input_len -1]], data[(i + input_len):(i + input_len + pred_len -1)]))\n",
    "        y_input.append(y_dec_inp_seq)\n",
    "        y_output.append(data[(i + input_len):(i + input_len + pred_len)])\n",
    "    return np.array(X), np.array(y_input), np.array(y_output)\n",
    "\n",
    "def train_or_load_autoformer(df_series, input_len=60, pred_len=5, force_train=False): # For Autoformer\n",
    "    if not torch_available: print(\"PyTorch not available for Autoformer.\"); return None\n",
    "    autoformer_forecaster = AutoformerPredictor(input_len=input_len, pred_len=pred_len)\n",
    "    if not force_train and os.path.exists(AUTOFORMER_MODEL_PATH):\n",
    "        # print(f\"Loading Autoformer model from {AUTOFORMER_MODEL_PATH}\") # Less verbose\n",
    "        if autoformer_forecaster.load_model(AUTOFORMER_MODEL_PATH):\n",
    "            return autoformer_forecaster\n",
    "        else:\n",
    "            print(f\"Failed to load Autoformer. Training new one.\")\n",
    "            force_train = True\n",
    "\n",
    "    if force_train:\n",
    "        print(\"Training new AutoformerPredictor model...\")\n",
    "        if df_series.isnull().any():\n",
    "            print(\"Warning: df_series contains NaNs. Autoformer training might be affected. Consider imputation.\")\n",
    "            df_series = df_series.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        series_scaled = scaler.fit_transform(df_series.values.reshape(-1, 1))\n",
    "        X_seq, y_seq_dec_in, y_seq_out = create_sequences(series_scaled.flatten(), input_len, pred_len)\n",
    "\n",
    "        if len(X_seq) == 0:\n",
    "            print(\"Not enough data for Autoformer training sequences. Model will be un-trained.\");\n",
    "            return autoformer_forecaster\n",
    "\n",
    "        X_seq = X_seq.reshape(X_seq.shape[0], X_seq.shape[1], 1)\n",
    "        y_seq_dec_in = y_seq_dec_in.reshape(y_seq_dec_in.shape[0], y_seq_dec_in.shape[1], 1)\n",
    "        y_seq_out = y_seq_out.reshape(y_seq_out.shape[0], y_seq_out.shape[1], 1)\n",
    "\n",
    "        dataset = torch.utils.data.TensorDataset(\n",
    "            torch.from_numpy(X_seq).float(),\n",
    "            torch.from_numpy(y_seq_dec_in).float(),\n",
    "            torch.from_numpy(y_seq_out).float()\n",
    "        )\n",
    "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "        autoformer_forecaster.train_model(train_loader, epochs=20) # Consider more epochs for real training\n",
    "    else:\n",
    "        # print(\"Autoformer training skipped. Model remains un-trained if not loaded.\") # Less verbose\n",
    "        pass\n",
    "    return autoformer_forecaster\n",
    "\n",
    "# --- Data Fetching and Feature Engineering Functions ---\n",
    "def fetch_twelve_data(symbol: str, api_key: str, start_date_str: str = None, end_date_str: str = None) -> pd.DataFrame | None:\n",
    "    base_url = \"https://api.twelvedata.com/time_series\"\n",
    "    params = {\"symbol\": symbol, \"interval\": \"1day\", \"apikey\": api_key, \"format\": \"JSON\", \"outputsize\": 5000}\n",
    "    if start_date_str: params[\"start_date\"] = start_date_str\n",
    "    if end_date_str: params[\"end_date\"] = end_date_str\n",
    "    print(f\"Fetching data for {symbol} from Twelve Data (interval=1day, from {start_date_str} to {end_date_str})...\")\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "    except requests.exceptions.RequestException as e: print(f\"Request failed for {symbol}: {e}\"); return None\n",
    "    except ValueError as e: print(f\"Failed to parse JSON for {symbol}: {e}. Response: {response.text[:200]}...\"); return None\n",
    "\n",
    "    if isinstance(data, dict) and (data.get(\"status\") == \"error\" or \"values\" not in data):\n",
    "        print(f\"API Error for {symbol}: {data.get('message', 'Unknown error')}\"); return None\n",
    "    if not isinstance(data, dict) or \"values\" not in data or not data[\"values\"]:\n",
    "        print(f\"No data values for {symbol}, or unexpected format.\"); return None\n",
    "\n",
    "    df = pd.DataFrame(data[\"values\"]).rename(columns={'datetime': 'date'})\n",
    "    for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').astype('float64')\n",
    "        else:\n",
    "            if col in ['open', 'high', 'low', 'close']: print(f\"Critical column '{col}' missing.\"); return None\n",
    "            df[col] = 0.0\n",
    "    if 'date' not in df.columns: print(\"Critical 'date' column missing.\"); return None\n",
    "\n",
    "    df.index = pd.to_datetime(df['date'])\n",
    "    df.drop(columns=['date'], inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    df.dropna(subset=[col for col in ['open', 'high', 'low', 'close'] if col in df.columns], inplace=True)\n",
    "    if df.empty: print(f\"No data remaining for {symbol} after initial processing.\"); return None\n",
    "    print(f\"Successfully fetched/processed {len(df)} data points for {symbol}.\")\n",
    "    return df\n",
    "\n",
    "def add_technical_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_feat = df.copy()\n",
    "    if not hasattr(df_feat.ta, 'rsi'): print(\"pandas_ta not registered. Skipping TIs.\"); return df_feat\n",
    "    print(\"Adding technical indicators...\")\n",
    "    try:\n",
    "        for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "            if col in df_feat.columns: df_feat[col] = df_feat[col].astype('float64')\n",
    "            else:\n",
    "                if col in ['high', 'low', 'open', 'close'] and 'close' in df_feat: df_feat[col] = df_feat['close']\n",
    "                elif col == 'volume': df_feat[col] = 0.0\n",
    "\n",
    "        c, h, l, v = 'close', 'high', 'low', 'volume'\n",
    "        required_cols_for_all_ta = [c,h,l,v]\n",
    "        if not all(col in df_feat.columns for col in required_cols_for_all_ta):\n",
    "            print(\"Warning: Not all OHLCV columns present, some TIs might fail or be inaccurate.\")\n",
    "\n",
    "        if c in df_feat:\n",
    "            df_feat.ta.rsi(close=df_feat[c], length=14, append=True)\n",
    "            df_feat.ta.rsi(close=df_feat[c], length=9, append=True, col_names='RSI_9')\n",
    "            df_feat.ta.rsi(close=df_feat[c], length=25, append=True, col_names='RSI_25')\n",
    "            df_feat.ta.macd(close=df_feat[c], fast=12, slow=26, signal=9, append=True)\n",
    "            df_feat.ta.macd(close=df_feat[c], fast=5, slow=15, signal=9, append=True, col_names=('MACD_5_15_9', 'MACDh_5_15_9', 'MACDs_5_15_9'))\n",
    "            for p in [10, 20, 50, 100, 200]:\n",
    "                df_feat.ta.sma(close=df_feat[c], length=p, append=True)\n",
    "                df_feat.ta.ema(close=df_feat[c], length=p, append=True)\n",
    "            df_feat.ta.bbands(close=df_feat[c], length=20, std=2, append=True)\n",
    "        else:\n",
    "            print(f\"Column '{c}' not found, skipping some TIs.\")\n",
    "\n",
    "        if all(x in df_feat.columns for x in [h,l,c]):\n",
    "            df_feat.ta.atr(high=df_feat[h], low=df_feat[l], close=df_feat[c], length=14, append=True)\n",
    "            df_feat.ta.adx(high=df_feat[h], low=df_feat[l], close=df_feat[c], length=14, append=True)\n",
    "            df_feat.ta.stoch(high=df_feat[h], low=df_feat[l], close=df_feat[c], append=True)\n",
    "            df_feat.ta.willr(high=df_feat[h], low=df_feat[l], close=df_feat[c], append=True)\n",
    "            df_feat.ta.cci(high=df_feat[h], low=df_feat[l], close=df_feat[c], append=True)\n",
    "        else:\n",
    "            print(f\"One or more of '{h}', '{l}', '{c}' not found, skipping some TIs.\")\n",
    "\n",
    "        if all(x in df_feat.columns for x in [h,l,c,v]):\n",
    "            df_feat.ta.mfi(high=df_feat[h], low=df_feat[l], close=df_feat[c], volume=df_feat[v], append=True)\n",
    "        else:\n",
    "            print(f\"One or more of '{h}', '{l}', '{c}', '{v}' not found, skipping MFI.\")\n",
    "\n",
    "        df_feat.columns = df_feat.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
    "        # print(\"Technical indicators added.\") # Less verbose\n",
    "    except Exception as e: print(f\"Error adding TIs: {e}\\n{traceback.format_exc()}\"); return df\n",
    "    return df_feat\n",
    "\n",
    "def add_optimized_features(df: pd.DataFrame, price_col='close', volume_col='volume') -> pd.DataFrame:\n",
    "    # print(\"Adding optimized statistical and ratio features...\") # Less verbose\n",
    "    df_new = df.copy()\n",
    "    if price_col not in df_new.columns:\n",
    "        print(f\"Price column '{price_col}' not in DataFrame. Skipping optimized features.\"); return df_new\n",
    "\n",
    "    df_new['returns'] = df_new[price_col].pct_change()\n",
    "    safe_price = df_new[price_col].replace(0, np.nan)\n",
    "    safe_price_shifted = df_new[price_col].shift(1).replace(0, np.nan)\n",
    "    df_new['log_returns'] = np.log(safe_price / safe_price_shifted)\n",
    "\n",
    "    for window in [5, 10, 20, 50]:\n",
    "        df_new[f'volatility_{window}'] = df_new['log_returns'].rolling(window).std()\n",
    "        df_new[f'skew_{window}'] = df_new['log_returns'].rolling(window).skew()\n",
    "        df_new[f'kurtosis_{window}'] = df_new['log_returns'].rolling(window).kurt()\n",
    "\n",
    "    if volume_col in df_new.columns and df_new[volume_col].isnull().sum() < len(df_new):\n",
    "        rolling_mean_volume = df_new[volume_col].rolling(20).mean().replace(0, np.nan)\n",
    "        df_new['volume_ratio'] = df_new[volume_col] / rolling_mean_volume\n",
    "        df_new['price_volume'] = df_new[price_col] * df_new[volume_col]\n",
    "        df_new['volume_change'] = df_new[volume_col].pct_change()\n",
    "\n",
    "    if all(col in df_new.columns for col in ['high', 'low', 'close']):\n",
    "        safe_low = df_new['low'].replace(0, np.nan)\n",
    "        safe_high = df_new['high'].replace(0, np.nan)\n",
    "        df_new['high_low_ratio'] = df_new['high'] / safe_low\n",
    "        df_new['close_to_high_ratio'] = safe_price / safe_high\n",
    "        df_new['close_to_low_ratio'] = safe_price / safe_low\n",
    "        df_new['intraday_range_norm'] = (df_new['high'] - df_new['low']) / safe_price\n",
    "    else:\n",
    "        print(\"High, Low, or Close columns missing for some ratio calculations.\")\n",
    "\n",
    "    if 'RSI_14' in df_new.columns: # Ensure RSI_14 was added by add_technical_indicators\n",
    "        df_new['RSI_signal'] = 0\n",
    "        df_new.loc[df_new['RSI_14'] < 30, 'RSI_signal'] = 1\n",
    "        df_new.loc[df_new['RSI_14'] > 70, 'RSI_signal'] = -1\n",
    "\n",
    "    macd_col_name = 'MACD_12_26_9'\n",
    "    macds_col_name = 'MACDs_12_26_9'\n",
    "    if macd_col_name in df_new.columns and macds_col_name in df_new.columns:\n",
    "        df_new['MACD_signal_line_cross'] = (df_new[macd_col_name] > df_new[macds_col_name]).astype(int)\n",
    "    return df_new\n",
    "\n",
    "def add_wavelet_features(df: pd.DataFrame, column='close', wavelet='mexh', scales_range=(1, 32), num_scales_to_extract=5) -> pd.DataFrame:\n",
    "    df_feat = df.copy()\n",
    "    # print(f\"Adding CWT for '{column}' using wavelet '{wavelet}'...\") # Less verbose\n",
    "    if pywt is None: print(\"PyWavelets not available.\"); return df_feat\n",
    "    if column not in df_feat.columns: print(f\"Column '{column}' not found for wavelet. Skipping.\"); return df_feat\n",
    "\n",
    "    signal = df_feat[column].values\n",
    "    if len(signal) < scales_range[1] + 5: # Ensure enough data for max scale\n",
    "        print(f\"Signal length {len(signal)} too short for CWT with max scale {scales_range[1]}. Skipping.\"); return df_feat\n",
    "\n",
    "    actual_max_scale = min(scales_range[1], len(signal) // 2 - 1) # Max scale cannot be > N/2\n",
    "    if actual_max_scale < scales_range[0]:\n",
    "        print(f\"Max scale {actual_max_scale} too small after constraint (min_scale {scales_range[0]}). Skipping CWT.\"); return df_feat\n",
    "\n",
    "    scales = np.arange(scales_range[0], actual_max_scale + 1)\n",
    "    if len(scales) == 0: print(\"No valid scales for CWT. Skipping.\"); return df_feat\n",
    "\n",
    "    try:\n",
    "        coefficients, _ = pywt.cwt(signal, scales, wavelet) # coefficients shape: (num_scales, len_signal)\n",
    "        coeffs_df = pd.DataFrame(coefficients.T, index=df_feat.index, columns=[f\"cwt_scale_{s}\" for s in scales])\n",
    "\n",
    "        df_feat[f'{column}_cwt_mean'] = coeffs_df.mean(axis=1)\n",
    "        df_feat[f'{column}_cwt_std'] = coeffs_df.std(axis=1)\n",
    "\n",
    "        s_indices_to_extract = np.linspace(0, len(scales)-1, min(num_scales_to_extract, len(scales)), dtype=int)\n",
    "        for s_idx in s_indices_to_extract:\n",
    "            actual_scale_val = scales[s_idx]\n",
    "            col_name_for_scale = f\"cwt_scale_{actual_scale_val}\" # This should match column in coeffs_df\n",
    "            if col_name_for_scale in coeffs_df.columns:\n",
    "                df_feat[f'{column}_cwt_energy_s{actual_scale_val}'] = coeffs_df[col_name_for_scale]**2\n",
    "        # print(\"Wavelet features added.\") # Less verbose\n",
    "    except Exception as e: print(f\"Error adding wavelet features: {e}\\n{traceback.format_exc()}\"); return df\n",
    "    return df_feat\n",
    "\n",
    "def add_entropy_features(df: pd.DataFrame, column='close', window=40) -> pd.DataFrame:\n",
    "    df_feat = df.copy()\n",
    "    # print(f\"Adding Entropy for '{column}' (window={window})...\") # Less verbose\n",
    "    if ant is None: print(\"Antropy not available.\"); return df_feat\n",
    "    if column not in df_feat.columns: print(f\"Column '{column}' not found for entropy. Skipping.\"); return df_feat\n",
    "\n",
    "    if len(df_feat) < window + 15: # Ensure enough data for rolling window calculations\n",
    "        print(f\"Data length {len(df_feat)} too short for entropy features with window {window}. Skipping.\"); return df_feat\n",
    "    try:\n",
    "        sig = df_feat[column].astype(float) # Ensure float for antropy functions\n",
    "        df_feat[f'{column}_entropy_sample'] = sig.rolling(window=window, min_periods=window).apply(\n",
    "            lambda x: ant.sample_entropy(x.dropna()) if x.dropna().shape[0] >= window//2 and x.dropna().std() > 1e-6 else np.nan, raw=False\n",
    "        )\n",
    "        df_feat[f'{column}_entropy_spectral'] = sig.rolling(window=window, min_periods=window).apply(\n",
    "            lambda x: ant.spectral_entropy(x.dropna(), sf=1.0, method='welch',\n",
    "                                           nperseg=min(x.dropna().shape[0], window // 2 if window // 2 > 0 else 1) if x.dropna().shape[0] > 1 else None\n",
    "                                          ) if x.dropna().shape[0] == window and x.dropna().std() > 1e-6 else np.nan, raw=False\n",
    "        )\n",
    "        # print(\"Entropy features added.\") # Less verbose\n",
    "    except Exception as e: print(f\"Error adding entropy features: {e}\\n{traceback.format_exc()}\"); return df\n",
    "    return df_feat\n",
    "\n",
    "def add_advanced_technical_features(df: pd.DataFrame, price_col='close', high_col='high', low_col='low', volume_col='volume') -> pd.DataFrame:\n",
    "    # print(\"Adding advanced TIs...\") # Less verbose\n",
    "    df_new = df.copy()\n",
    "    if not hasattr(df_new.ta, 'mom'): print(\"pandas_ta not registered. Skipping advanced TIs.\"); return df_new\n",
    "    try:\n",
    "        if not all(c in df_new.columns for c in [price_col, high_col, low_col]):\n",
    "            print(f\"Missing one or more of {price_col}, {high_col}, {low_col} for adv TIs. Skipping.\"); return df_new\n",
    "        for col in [price_col, high_col, low_col]: df_new[col] = df_new[col].astype(float)\n",
    "        if volume_col in df_new.columns: df_new[volume_col] = df_new[volume_col].astype(float)\n",
    "\n",
    "        df_new.ta.mom(close=df_new[price_col], append=True)\n",
    "        df_new.ta.roc(close=df_new[price_col], append=True)\n",
    "        df_new.ta.natr(high=df_new[high_col], low=df_new[low_col], close=df_new[price_col], append=True)\n",
    "        df_new.ta.aroon(high=df_new[high_col], low=df_new[low_col], append=True)\n",
    "        df_new.ta.stc(close=df_new[price_col], tclength=23, fast=50, slow=100, factor=0.5, append=True, col_names=('STC_23_50_05', 'STCD_23_50_05', 'STCK_23_50_05'))\n",
    "\n",
    "        if volume_col in df_new.columns and df_new[volume_col].isnull().sum() < len(df_new):\n",
    "            df_new.ta.pvol(close=df_new[price_col], volume=df_new[volume_col], append=True)\n",
    "            df_new.ta.cmf(high=df_new[high_col], low=df_new[low_col], close=df_new[price_col], volume=df_new[volume_col], append=True)\n",
    "\n",
    "        df_new.columns = df_new.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True) # Final sanitize\n",
    "        # print(\"Advanced TIs added.\") # Less verbose\n",
    "    except Exception as e: print(f\"Error adding advanced TIs: {e}\\n{traceback.format_exc()}\"); return df\n",
    "    return df_new\n",
    "\n",
    "def add_transformer_features_conceptual(df: pd.DataFrame, column='close', sequence_length=20) -> pd.DataFrame:\n",
    "    df_feat = df.copy()\n",
    "    # print(f\"Adding Transformer-inspired conceptual features for '{column}'...\") # Less verbose\n",
    "    if column not in df_feat.columns: print(f\"Column '{column}' not found for conceptual Transformer features. Skipping.\"); return df_feat\n",
    "    if len(df_feat) < sequence_length + 5: print(f\"Data too short for conceptual Transformer features. Skipping.\"); return df_feat\n",
    "\n",
    "    feature_col_base = f\"{column}_trans_seq\"\n",
    "    for col_suffix in ['mean', 'std', 'trend', 'volatility', 'autocorr1']:\n",
    "        df_feat[f'{feature_col_base}_{col_suffix}'] = np.nan\n",
    "    try:\n",
    "        data_series = df_feat[column].values\n",
    "        windows = np.lib.stride_tricks.sliding_window_view(data_series, sequence_length)\n",
    "        results = {key: [np.nan] * (sequence_length -1) for key in ['mean', 'std', 'trend', 'volatility', 'autocorr1']}\n",
    "\n",
    "        for seq in windows:\n",
    "            if np.isnan(seq).any(): # Skip windows with NaNs\n",
    "                for key in results: results[key].append(np.nan)\n",
    "                continue\n",
    "            mean_val, std_val = np.mean(seq), np.std(seq)\n",
    "            norm_seq = (seq - mean_val) / std_val if std_val > 1e-8 else np.zeros_like(seq)\n",
    "\n",
    "            results['mean'].append(np.mean(norm_seq))\n",
    "            results['std'].append(np.std(norm_seq))\n",
    "            current_trend, current_vol, current_ac = 0.0, 0.0, 0.0 # Defaults for safety\n",
    "\n",
    "            if len(norm_seq) > 1:\n",
    "                try:\n",
    "                    fit_params = np.polyfit(np.arange(len(norm_seq)), norm_seq, 1)\n",
    "                    current_trend = fit_params[0] if not np.isnan(fit_params[0]) else 0.0\n",
    "                except (np.linalg.LinAlgError, ValueError): pass # Keep 0.0\n",
    "\n",
    "                diff_norm_seq = np.diff(norm_seq)\n",
    "                current_vol = np.std(diff_norm_seq) if len(diff_norm_seq) > 0 else 0.0\n",
    "\n",
    "                if len(norm_seq) >= 2: # Autocorrelation needs at least 2 points\n",
    "                    s1, s2 = norm_seq[:-1], norm_seq[1:]\n",
    "                    if len(s1) >= 1 and np.std(s1) > 1e-8 and np.std(s2) > 1e-8:\n",
    "                        try:\n",
    "                            corr_matrix = np.corrcoef(s1, s2)\n",
    "                            current_ac = corr_matrix[0, 1] if not np.isnan(corr_matrix[0, 1]) else 0.0\n",
    "                        except (ValueError, IndexError): pass # Keep 0.0\n",
    "            results['trend'].append(current_trend)\n",
    "            results['volatility'].append(current_vol)\n",
    "            results['autocorr1'].append(current_ac)\n",
    "\n",
    "        for key, values in results.items():\n",
    "            if len(values) == len(df_feat):\n",
    "                df_feat[f'{feature_col_base}_{key}'] = values\n",
    "            else:\n",
    "                # print(f\"Warning: Length mismatch for conceptual feature {key}. Expected {len(df_feat)}, got {len(values)}. Padding with NaN.\") # Less verbose\n",
    "                padded_values = np.full(len(df_feat), np.nan)\n",
    "                if len(values) > 0 : padded_values[-len(values):] = values # Align at the end\n",
    "                df_feat[f'{feature_col_base}_{key}'] = padded_values\n",
    "        # print(\"Conceptual Transformer-inspired features added.\") # Less verbose\n",
    "    except Exception as e: print(f\"Error in conceptual Transformer features: {e}\\n{traceback.format_exc()}\"); return df\n",
    "    return df_feat\n",
    "\n",
    "def detect_regimes_simple(df: pd.DataFrame, column='close', window=20) -> pd.DataFrame:\n",
    "    df_reg = df.copy()\n",
    "    # print(f\"Detecting regimes (simplified volatility-based) for {column}...\") # Less verbose\n",
    "    if column not in df_reg.columns:\n",
    "        print(f\"'{column}' not found. Skipping simple regimes.\")\n",
    "        df_reg['regime_simple'] = 0\n",
    "        return df_reg\n",
    "\n",
    "    if 'log_returns' not in df_reg.columns:\n",
    "        safe_price = df_reg[column].replace(0, np.nan)\n",
    "        safe_price_shifted = df_reg[column].shift(1).replace(0, np.nan)\n",
    "        df_reg['log_returns_temp_for_regime'] = np.log(safe_price / safe_price_shifted)\n",
    "        returns_col_for_regime = 'log_returns_temp_for_regime'\n",
    "    else:\n",
    "        returns_col_for_regime = 'log_returns'\n",
    "\n",
    "    returns = df_reg[returns_col_for_regime].dropna()\n",
    "    if returns.empty:\n",
    "        print(\"No valid returns for simple regime detection. Skipping.\")\n",
    "        df_reg['regime_simple'] = 0\n",
    "        if 'log_returns_temp_for_regime' in df_reg.columns: df_reg.drop(columns=['log_returns_temp_for_regime'], inplace=True)\n",
    "        return df_reg\n",
    "\n",
    "    rolling_vol = returns.rolling(window=window, min_periods=window//2 if window//2 > 0 else 1).std()\n",
    "    df_reg['regime_simple'] = 0 # Default: medium volatility (class 0)\n",
    "\n",
    "    if not rolling_vol.dropna().empty:\n",
    "        vol_low_thresh = rolling_vol.quantile(0.33)\n",
    "        vol_high_thresh = rolling_vol.quantile(0.67)\n",
    "        df_reg.loc[rolling_vol.index[rolling_vol <= vol_low_thresh], 'regime_simple'] = 1  # Low vol (class 1)\n",
    "        df_reg.loc[rolling_vol.index[rolling_vol > vol_high_thresh], 'regime_simple'] = 2   # High vol (class 2)\n",
    "    else:\n",
    "        print(\"Not enough data for rolling volatility percentile calculation. Defaulting simple regimes to 0.\")\n",
    "\n",
    "    df_reg['regime_simple'] = df_reg['regime_simple'].fillna(method='bfill').fillna(method='ffill').fillna(0)\n",
    "    if 'log_returns_temp_for_regime' in df_reg.columns: df_reg.drop(columns=['log_returns_temp_for_regime'], inplace=True)\n",
    "\n",
    "    print(f\"Simple Regimes (0:Med,1:Low,2:High):\\n{df_reg['regime_simple'].value_counts(normalize=True, dropna=False).sort_index()*100} %\")\n",
    "    return df_reg\n",
    "\n",
    "def balanced_target_definition(df: pd.DataFrame, column='close', periods=5, lower_q_thresh=0.45, upper_q_thresh=0.55) -> pd.DataFrame:\n",
    "    df_t = df.copy()\n",
    "    # print(f\"Balanced target definition for '{column}' over {periods} periods...\") # Less verbose\n",
    "    if column not in df_t.columns:\n",
    "        print(f\"'{column}' not found for target. Defaulting target.\")\n",
    "        df_t['target'] = 0\n",
    "        return df_t\n",
    "\n",
    "    df_t[column] = pd.to_numeric(df_t[column], errors='coerce').replace(0, np.nan)\n",
    "    df_t['future_log_return_target'] = np.log(df_t[column].shift(-periods) / df_t[column])\n",
    "    valid_returns = df_t['future_log_return_target'].dropna()\n",
    "    df_t['target'] = 0 # Default class (e.g., Hold/Neutral)\n",
    "\n",
    "    if len(valid_returns) > 20: # Need enough data for meaningful quantiles\n",
    "        lower_q_val = valid_returns.quantile(lower_q_thresh)\n",
    "        upper_q_val = valid_returns.quantile(upper_q_thresh)\n",
    "        if lower_q_val >= upper_q_val and upper_q_val > 0 : lower_q_val = upper_q_val * 0.99 # small adjustment\n",
    "        elif lower_q_val >= upper_q_val and upper_q_val < 0 : upper_q_val = lower_q_val * 0.99\n",
    "        df_t.loc[df_t['future_log_return_target'] < lower_q_val, 'target'] = 0\n",
    "        df_t.loc[df_t['future_log_return_target'] > upper_q_val, 'target'] = 1\n",
    "    else:\n",
    "        print(\"Not enough valid returns for quantile-based target balancing. Default target (all 0s) used or target may be skewed.\")\n",
    "\n",
    "    df_t.drop(columns=['future_log_return_target'], inplace=True, errors='ignore')\n",
    "    print(f\"Target distribution:\\n{df_t['target'].value_counts(normalize=True, dropna=False)*100}\")\n",
    "    return df_t\n",
    "\n",
    "def discover_causal_structure(df_features: pd.DataFrame, target_col='target', price_c='close', max_feats=10, symbol=\"\") -> CausalModel | None:\n",
    "    print(f\"\\nDiscovering causal structure for {symbol} using DoWhy...\")\n",
    "    if not dowhy_available or CausalModel is None:\n",
    "        print(\"DoWhy not available.\")\n",
    "        return None\n",
    "\n",
    "    df_c = df_features.copy()\n",
    "    if target_col not in df_c.columns or df_c[target_col].isnull().all():\n",
    "        print(f\"Target '{target_col}' missing for causal discovery.\")\n",
    "        return None\n",
    "    df_c[target_col] = pd.to_numeric(df_c[target_col], errors='coerce')\n",
    "    cand_cols = [c for c in df_c.columns if pd.api.types.is_numeric_dtype(df_c[c]) and c != target_col and df_c[c].notnull().any() and df_c[c].var() > 1e-6]\n",
    "    if not cand_cols:\n",
    "        print(\"No numeric candidate columns with variance for causal discovery.\")\n",
    "        return None\n",
    "\n",
    "    df_subset_for_causal = df_c[cand_cols + [target_col]].copy()\n",
    "    df_subset_for_causal.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    if cand_cols:\n",
    "        scaler_causal = StandardScaler()\n",
    "        df_subset_for_causal[cand_cols] = scaler_causal.fit_transform(df_subset_for_causal[cand_cols])\n",
    "    df_subset_for_causal.dropna(inplace=True)\n",
    "    if df_subset_for_causal.empty or target_col not in df_subset_for_causal.columns or df_subset_for_causal[target_col].nunique() < 1:\n",
    "        print(\"Not enough data post-cleaning/scaling for causal discovery.\")\n",
    "        return None\n",
    "\n",
    "    cwt_mean_col = f\"{price_c}_cwt_mean\" if f\"{price_c}_cwt_mean\" in df_subset_for_causal.columns else 'close_cwt_mean'\n",
    "    cwt_std_col = f\"{price_c}_cwt_std\" if f\"{price_c}_cwt_std\" in df_subset_for_causal.columns else 'close_cwt_std'\n",
    "    entropy_sample_col = f\"{price_c}_entropy_sample\" if f\"{price_c}_entropy_sample\" in df_subset_for_causal.columns else 'close_entropy_sample'\n",
    "    potential_causes = ['RSI_14', 'MACDh_12_26_9', 'ADX_14', 'ATR_14', cwt_mean_col, cwt_std_col, entropy_sample_col, 'regime_simple', 'volatility_20', 'log_returns', 'BBP_2020', 'BBB_2020']\n",
    "    graph_feats = [c for c in potential_causes if c in df_subset_for_causal.columns and c != target_col and df_subset_for_causal[c].nunique() > 1]\n",
    "    if not graph_feats:\n",
    "        print(\"Predefined causal graph_feats not suitable or not found, selecting top varying features (after scaling).\")\n",
    "        num_to_select = min(max_feats, len(cand_cols))\n",
    "        if num_to_select > 0:\n",
    "            graph_feats = df_subset_for_causal[cand_cols].var().nlargest(num_to_select).index.tolist()\n",
    "        else:\n",
    "            print(\"No candidate columns for graph_feats fallback.\")\n",
    "            return None\n",
    "    if not graph_feats:\n",
    "        print(\"No suitable graph features for causal discovery.\")\n",
    "        return None\n",
    "\n",
    "    final_df_for_causal_model = df_subset_for_causal[graph_feats + [target_col]].copy()\n",
    "    if final_df_for_causal_model.empty or final_df_for_causal_model.shape[0] < 20 or final_df_for_causal_model[target_col].nunique() < 1:\n",
    "        print(\"Final DF for causal model too small or target has no variation.\")\n",
    "        return None\n",
    "    print(f\"DoWhy using graph features: {graph_feats} for Outcome: {target_col}\")\n",
    "    treatment_var = graph_feats[0]\n",
    "    graph_str = \"digraph { \" + \"; \".join([f'\"{f}\" -> \"{target_col}\"' for f in graph_feats]) + \" }\"\n",
    "    print(f\"Generated Causal Graph:\\n{graph_str}\")\n",
    "    try:\n",
    "        model = CausalModel(data=final_df_for_causal_model, treatment=treatment_var, outcome=target_col, graph=graph_str)\n",
    "        print(\"DoWhy CausalModel created.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"DoWhy CausalModel error: {e}\\n{traceback.format_exc()}\")\n",
    "        return None\n",
    "\n",
    "def prepare_ml_data(df: pd.DataFrame, target_col='target', test_split_size=0.15, min_test_samples=50):\n",
    "    # print(\"Preparing ML data...\") # Less verbose\n",
    "    if target_col not in df.columns:\n",
    "        print(f\"Target '{target_col}' missing.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    cols_to_drop_base = ['open', 'high', 'low', 'close', 'volume', 'returns']\n",
    "    cols_to_drop_dynamic = [c for c in df.columns if 'target_' in c and c != target_col] + \\\n",
    "                           [c for c in df.columns if 'future_return' in c]\n",
    "\n",
    "    all_cols_to_drop = list(set(cols_to_drop_base + cols_to_drop_dynamic))\n",
    "    if target_col in all_cols_to_drop:\n",
    "        all_cols_to_drop.remove(target_col)\n",
    "\n",
    "    X = df.drop(columns=[c for c in all_cols_to_drop if c in df.columns] + [target_col], errors='ignore')\n",
    "    y = df[target_col].copy()\n",
    "\n",
    "    if y.isnull().all():\n",
    "        print(\"Target is all NaN.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    valid_target_mask = y.notna()\n",
    "    X = X.loc[valid_target_mask]\n",
    "    y = y.loc[valid_target_mask]\n",
    "\n",
    "    if X.empty or y.empty:\n",
    "        print(\"X or y empty after target NaN filter.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    if X.isnull().any().any():\n",
    "        # print(f\"NaNs in X before median imputation: {X.isnull().sum().sum()}\") # Less verbose\n",
    "        for col in X.columns:\n",
    "            if X[col].isnull().any():\n",
    "                if pd.api.types.is_numeric_dtype(X[col]):\n",
    "                    X[col] = X[col].fillna(X[col].median())\n",
    "                else:\n",
    "                    X[col] = X[col].fillna(X[col].mode()[0] if not X[col].mode().empty else \"Unknown\")\n",
    "\n",
    "    if X.isnull().any().any():\n",
    "        print(f\"Warning: NaNs still present after imputation. Dropping rows with NaNs in X. Nulls per col:\\n{X.isnull().sum()[X.isnull().sum()>0]}\")\n",
    "        X.dropna(axis=0, how='any', inplace=True)\n",
    "        y = y.loc[X.index]\n",
    "\n",
    "    if X.empty or y.empty:\n",
    "        print(\"X or y empty after internal NaN handling.\")\n",
    "        return None, None, None, None, None\n",
    "    # print(f\"Data shape post-NaN handling in prepare_ml_data: X={X.shape}, y={y.shape}\") # Less verbose\n",
    "\n",
    "    if len(X) < min_test_samples * 2:\n",
    "        print(f\"Not enough data ({len(X)} rows) for robust train/test split. Min required for split: {min_test_samples*2}.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    n_samples = len(X)\n",
    "    test_size_abs = max(min_test_samples, int(n_samples * test_split_size))\n",
    "\n",
    "    if n_samples - test_size_abs < min_test_samples:\n",
    "        test_size_abs = n_samples - min_test_samples\n",
    "\n",
    "    if test_size_abs < 1 and n_samples > 0:\n",
    "        test_size_abs = 1\n",
    "    elif test_size_abs < 1:\n",
    "        print(f\"Cannot make meaningful split (test_size_abs < 1).\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    train_size = n_samples - test_size_abs\n",
    "    if train_size < 1:\n",
    "        print(f\"Train size too small ({train_size}). Cannot split.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "    y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "\n",
    "    if X_train.empty or y_train.empty or X_test.empty or y_test.empty:\n",
    "        print(\"Train/Test set empty post-split.\")\n",
    "        return None, None, None, None, None\n",
    "    print(f\"Train shapes: X_train={X_train.shape}, y_train={y_train.shape}; Test shapes: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
    "\n",
    "    numeric_cols_xtrain = X_train.select_dtypes(include=np.number).columns\n",
    "    scaler = None\n",
    "\n",
    "    if not numeric_cols_xtrain.empty:\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled_np = scaler.fit_transform(X_train[numeric_cols_xtrain])\n",
    "        X_train_scaled_df = pd.DataFrame(X_train_scaled_np, columns=numeric_cols_xtrain, index=X_train.index)\n",
    "\n",
    "        X_train_final = X_train.copy()\n",
    "        X_train_final[numeric_cols_xtrain] = X_train_scaled_df\n",
    "\n",
    "        numeric_cols_xtest = X_test.select_dtypes(include=np.number).columns\n",
    "        common_numeric_cols = [col for col in numeric_cols_xtrain if col in numeric_cols_xtest]\n",
    "\n",
    "        X_test_final = X_test.copy()\n",
    "        if common_numeric_cols:\n",
    "            X_test_scaled_np = scaler.transform(X_test[common_numeric_cols])\n",
    "            X_test_scaled_df = pd.DataFrame(X_test_scaled_np, columns=common_numeric_cols, index=X_test[common_numeric_cols].index)\n",
    "            X_test_final[common_numeric_cols] = X_test_scaled_df\n",
    "        else:\n",
    "            print(\"No common numeric columns to scale in X_test, or X_test has no numeric columns that were scaled in train.\")\n",
    "\n",
    "        return X_train_final, X_test_final, y_train, y_test, scaler\n",
    "    else:\n",
    "        print(\"No numeric columns in X_train for scaling.\")\n",
    "        return X_train, X_test, y_train, y_test, None\n",
    "\n",
    "def lgbm_objective(trial, X_train, y_train, X_val, y_val, base_params):\n",
    "    params = {\n",
    "        **base_params,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=50),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 50),\n",
    "        'max_depth': trial.suggest_int('max_depth', -1, 10),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 0, 7),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 1.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 1.0, log=True),\n",
    "        'min_gain_to_split': trial.suggest_float('min_gain_to_split', 0.0, 0.1) # Added for Optuna\n",
    "    }\n",
    "    if params.get('num_class') is None and 'num_class' in params: # Should not be needed if base_params is set right\n",
    "        del params['num_class']\n",
    "\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X_train, y_train,\n",
    "              eval_set=[(X_val, y_val)],\n",
    "              eval_metric=base_params.get('metric', 'logloss'), # Ensure metric is passed from base\n",
    "              callbacks=[lgb.early_stopping(30, verbose=False)])\n",
    "    y_proba_val = model.predict_proba(X_val)\n",
    "    return log_loss(y_val, y_proba_val)\n",
    "\n",
    "def optimize_lgbm_hyperparameters(X_train: pd.DataFrame, y_train: pd.Series, base_params: dict, n_trials=30, validation_ratio=0.2) -> dict:\n",
    "    if not optuna_available:\n",
    "        print(\"Optuna not available. Using default HPs.\")\n",
    "        return optimized_lightgbm_params() # Return default set\n",
    "\n",
    "    print(f\"Optimizing LightGBM HPs with Optuna ({n_trials} trials)...\")\n",
    "    if len(X_train) * validation_ratio < 1 or len(X_train) * (1-validation_ratio) < 1:\n",
    "        print(\"Too few samples for Optuna validation split. Using default HPs.\")\n",
    "        return optimized_lightgbm_params()\n",
    "\n",
    "    n_val_samples = int(len(X_train) * validation_ratio)\n",
    "    if n_val_samples == 0 and len(X_train) > 1: n_val_samples = 1\n",
    "    elif n_val_samples == 0 :\n",
    "        print(\"Cannot create validation set for Optuna (0 samples). Using default HPs.\")\n",
    "        return optimized_lightgbm_params()\n",
    "\n",
    "    n_train_opt_samples = len(X_train) - n_val_samples\n",
    "    if n_train_opt_samples == 0:\n",
    "        print(\"Train set for Optuna is empty after split. Using default HPs.\")\n",
    "        return optimized_lightgbm_params()\n",
    "\n",
    "    X_train_opt, X_val_opt = X_train.iloc[:n_train_opt_samples], X_train.iloc[n_train_opt_samples:]\n",
    "    y_train_opt, y_val_opt = y_train.iloc[:n_train_opt_samples], y_train.iloc[n_train_opt_samples:]\n",
    "\n",
    "    # Ensure base_params for objective has 'num_class' if multiclass\n",
    "    # This should already be handled when base_params is created before calling this function\n",
    "    study = optuna.create_study(direction='minimize') # Minimize log_loss\n",
    "    study.optimize(lambda trial: lgbm_objective(trial, X_train_opt, y_train_opt, X_val_opt, y_val_opt, base_params),\n",
    "                   n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "    print(f\"Best Optuna trial for LightGBM: Value={study.best_value:.4f}, Params={study.best_params}\")\n",
    "    # Return only the tuned parameters, base_params will be merged by the caller\n",
    "    return study.best_params\n",
    "\n",
    "def train_lightgbm_model(X_train, y_train, X_test, y_test, optimized_params=None):\n",
    "    print(\"Training LightGBM model...\")\n",
    "    if X_train is None or X_train.empty or y_train is None or y_train.empty:\n",
    "        print(\"X_train or y_train is empty. Skipping LightGBM training.\")\n",
    "        return None, None\n",
    "\n",
    "    y_train_squeezed = y_train.squeeze()\n",
    "    y_test_squeezed = y_test.squeeze() if y_test is not None else pd.Series()\n",
    "    unique_labels_train = sorted(y_train_squeezed.unique())\n",
    "    num_classes = len(unique_labels_train)\n",
    "\n",
    "    if num_classes <= 1:\n",
    "        print(f\"Only {num_classes} class(es) in y_train. Skipping LightGBM training.\")\n",
    "        return None, None\n",
    "\n",
    "    # Define base parameters for LightGBM\n",
    "    current_params = optimized_lightgbm_params() # Start with defaults\n",
    "    current_params['objective'] = 'multiclass' if num_classes > 2 else 'binary'\n",
    "    current_params['metric'] = 'multi_logloss' if num_classes > 2 else 'binary_logloss'\n",
    "    if num_classes > 2:\n",
    "        current_params['num_class'] = num_classes\n",
    "    else: # Ensure num_class is not present for binary\n",
    "        if 'num_class' in current_params:\n",
    "            del current_params['num_class']\n",
    "\n",
    "\n",
    "    if optimized_params and isinstance(optimized_params, dict):\n",
    "        print(\"Using Optuna-optimized parameters.\")\n",
    "        current_params.update(optimized_params) # Merge Optuna's best params\n",
    "    else:\n",
    "        print(\"Using default (or non-Optuna optimized) LightGBM parameters.\")\n",
    "\n",
    "\n",
    "    label_map = {label: i for i, label in enumerate(unique_labels_train)}\n",
    "    y_train_mapped = y_train_squeezed.map(label_map)\n",
    "    model = lgb.LGBMClassifier(**current_params)\n",
    "    eval_set_data = None\n",
    "    valid_eval_indices = None\n",
    "    y_test_mapped_for_eval = None\n",
    "\n",
    "    if X_test is not None and not X_test.empty and not y_test_squeezed.empty:\n",
    "        y_test_mapped = y_test_squeezed.map(label_map).fillna(-1).astype(int)\n",
    "        valid_eval_indices = (y_test_mapped != -1)\n",
    "        if valid_eval_indices.any():\n",
    "            y_test_mapped_for_eval = y_test_mapped[valid_eval_indices]\n",
    "            # Ensure X_test for eval_set has same columns as X_train\n",
    "            X_test_eval = X_test[valid_eval_indices][X_train.columns] if all(c in X_test.columns for c in X_train.columns) else X_test[valid_eval_indices]\n",
    "\n",
    "            eval_set_data = (X_test_eval, y_test_mapped_for_eval)\n",
    "\n",
    "    if eval_set_data:\n",
    "        model.fit(X_train, y_train_mapped, eval_set=[eval_set_data],\n",
    "                  eval_metric=current_params.get('metric'), # Use metric from current_params\n",
    "                  callbacks=[lgb.early_stopping(30, verbose=False)])\n",
    "    else:\n",
    "        print(\"Warning: No valid eval set. Fitting on full training data without early stopping based on eval set.\")\n",
    "        model.fit(X_train, y_train_mapped)\n",
    "\n",
    "    feat_imp_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': model.feature_importances_}).sort_values(by='Importance', ascending=False)\n",
    "    print(\"\\nTop 10 features:\\n\", feat_imp_df.head(10))\n",
    "\n",
    "    if eval_set_data and y_test_mapped_for_eval is not None and not y_test_mapped_for_eval.empty:\n",
    "        # Ensure X_test for predict has same columns as X_train\n",
    "        X_test_predict = X_test[valid_eval_indices][X_train.columns] if all(c in X_test.columns for c in X_train.columns) else X_test[valid_eval_indices]\n",
    "        y_pred_mapped_on_valid = model.predict(X_test_predict)\n",
    "        y_proba_on_valid = model.predict_proba(X_test_predict)\n",
    "        acc = accuracy_score(y_test_mapped_for_eval, y_pred_mapped_on_valid)\n",
    "        print(f\"\\n🎯 Accuracy on mapped test data: {acc:.4f}\")\n",
    "\n",
    "        if current_params['objective'] == 'binary' and y_proba_on_valid.shape[1] == 2:\n",
    "            try:\n",
    "                auc = roc_auc_score(y_test_mapped_for_eval, y_proba_on_valid[:, 1])\n",
    "                print(f\"📊 AUC: {auc:.4f}\")\n",
    "            except ValueError as e_auc:\n",
    "                print(f\"AUC Calculation Error: {e_auc}\")\n",
    "        print(\"\\nClassification Report (on mapped and valid test labels):\")\n",
    "        try:\n",
    "            report_labels = sorted(np.unique(np.concatenate((y_test_mapped_for_eval.unique(), pd.Series(y_pred_mapped_on_valid).unique()))))\n",
    "            print(classification_report(y_test_mapped_for_eval, y_pred_mapped_on_valid, labels=report_labels, zero_division=0))\n",
    "        except Exception as e_cr:\n",
    "            print(f\"Classification Report Error: {e_cr}\")\n",
    "    else:\n",
    "        print(\"No valid test samples for evaluation after mapping, or X_test/y_test was not provided.\")\n",
    "    return model, feat_imp_df\n",
    "\n",
    "def plot_feature_importance(feature_importance_df, top_n=20, symbol_for_plot=\"\", min_bar_height=0.05):\n",
    "    if feature_importance_df is None or feature_importance_df.empty:\n",
    "        print(\"No feature importance to plot.\")\n",
    "        return\n",
    "\n",
    "    plot_data = feature_importance_df.head(top_n).copy()\n",
    "    if plot_data.empty:\n",
    "        print(\"No features in plot_data after head(top_n).\")\n",
    "        return\n",
    "\n",
    "    max_importance = plot_data['Importance'].max()\n",
    "    min_threshold = max(max_importance * 0.02, 1e-6) # Use a small absolute minimum if relative is too small\n",
    "    plot_data['Plot_Importance'] = np.maximum(plot_data['Importance'], min_threshold)\n",
    "\n",
    "    plt.figure(figsize=(14, max(8, min(top_n, len(plot_data)) * 0.5)))\n",
    "    ax = sns.barplot(x='Plot_Importance', y='Feature', data=plot_data, palette=\"viridis\", orient='h')\n",
    "    for i, row_data in enumerate(plot_data.itertuples()): # Use itertuples for easier access\n",
    "        original_val = row_data.Importance # Get original importance for label\n",
    "        plot_val = row_data.Plot_Importance # Get the value used for plotting the bar\n",
    "        ax.text(plot_val + max_importance * 0.01, i, f'{original_val:.0f}', va='center', fontsize=9, fontweight='bold') # Changed to .0f for integer display\n",
    "\n",
    "    plt.title(f'Top {top_n} Feature Importances for {symbol_for_plot} (LightGBM)', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Importance', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Feature', fontsize=12, fontweight='bold')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    # plt.show() # Comment out for non-interactive environments if needed\n",
    "    # Instead of plt.show(), save the figure\n",
    "    plot_filename = f\"feature_importance_{symbol_for_plot}.png\"\n",
    "    plt.savefig(plot_filename)\n",
    "    print(f\"Feature importance plot saved to {plot_filename}\")\n",
    "    plt.close() # Close the plot to free up memory\n",
    "\n",
    "\n",
    "def export_lgbm_to_onnx(lgbm_model, X_sample_df, file_path=\"lgbm_model.onnx\", target_opset=12):\n",
    "    print(f\"\\nExporting LGBM model to ONNX: {file_path} (opset={target_opset})\")\n",
    "    if not all([onnx_available, skl2onnx_available, onnxmltools_available, (FloatTensorType is not None)]):\n",
    "        print(\"One or more ONNX libraries missing or FloatTensorType not imported. Skipping ONNX export.\")\n",
    "        return None\n",
    "    if lgbm_model is None or X_sample_df is None or X_sample_df.empty:\n",
    "        print(\"Model or sample data empty for ONNX. Skipping.\")\n",
    "        return None\n",
    "    try:\n",
    "        initial_type = [('float_input', FloatTensorType([None, X_sample_df.shape[1]]))]\n",
    "        converted_model = onnxmltools.convert_lightgbm(lgbm_model, initial_types=initial_type, target_opset=target_opset)\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(converted_model.SerializeToString())\n",
    "        print(f\"Model exported to ONNX: {file_path}\")\n",
    "        onnx.checker.check_model(file_path)\n",
    "        print(\"ONNX model check OK.\")\n",
    "        return file_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting LGBM to ONNX: {e}. Fallback to pickle.\")\n",
    "        try:\n",
    "            import pickle\n",
    "            pkl_path = file_path.replace('.onnx', '.pkl')\n",
    "            with open(pkl_path, 'wb') as pf:\n",
    "                pickle.dump(lgbm_model, pf)\n",
    "            print(f\"Model saved as pickle: {pkl_path}\")\n",
    "            return pkl_path\n",
    "        except Exception as ep:\n",
    "            print(f\"Pickle save error: {ep}\")\n",
    "            return None\n",
    "\n",
    "def simple_feature_selection_fallback(X_train, y_train, max_features=20):\n",
    "    print(\"Using simple variance-based feature selection fallback...\")\n",
    "    if X_train.empty: return pd.DataFrame()\n",
    "    X_train_numeric = X_train.select_dtypes(include=np.number)\n",
    "    if X_train_numeric.empty:\n",
    "        print(\"No numeric features for variance selection. Returning first few columns if available.\")\n",
    "        return pd.DataFrame({'Feature': X_train.columns[:max_features].tolist()})\n",
    "    variance_scores = X_train_numeric.var().sort_values(ascending=False)\n",
    "    num_features_to_select = min(max_features, len(variance_scores))\n",
    "    selected_features = variance_scores.head(num_features_to_select).index.tolist()\n",
    "    return pd.DataFrame({'Feature': selected_features, 'Score': variance_scores.head(num_features_to_select).values})\n",
    "\n",
    "def prioritized_feature_selection(X_train, y_train, causal_ranking, max_features=25):\n",
    "    print(\"Prioritized feature selection: Causal Ranking + Mutual Information...\")\n",
    "    if X_train.empty or y_train.empty:\n",
    "        print(\"X_train or y_train is empty in prioritized_feature_selection. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    top_causal_features = []\n",
    "    num_causal_to_select = 0\n",
    "    if causal_ranking and isinstance(causal_ranking, list) and all(isinstance(item, tuple) and len(item)==2 for item in causal_ranking):\n",
    "        num_causal_to_select = min(len(causal_ranking), max_features // 2)\n",
    "        if num_causal_to_select > 0:\n",
    "            print(f\"Selecting up to {num_causal_to_select} features from causal ranking.\")\n",
    "            for feat, score in causal_ranking[:num_causal_to_select]:\n",
    "                if feat in X_train.columns:\n",
    "                    top_causal_features.append(feat)\n",
    "                else:\n",
    "                    print(f\"Causal feature '{feat}' not in X_train.columns. Skipping.\")\n",
    "    else:\n",
    "        print(\"No valid causal ranking provided or num_causal_to_select is 0.\")\n",
    "\n",
    "    # print(f\"Features selected from causal ranking: {top_causal_features}\") # Less verbose\n",
    "    remaining_slots = max_features - len(top_causal_features)\n",
    "    stat_selected_features = []\n",
    "\n",
    "    if remaining_slots > 0:\n",
    "        features_for_stat_selection = [f for f in X_train.columns if f not in top_causal_features]\n",
    "        if features_for_stat_selection:\n",
    "            X_remaining_for_stat = X_train[features_for_stat_selection]\n",
    "            y_train_squeezed = y_train.squeeze()\n",
    "\n",
    "            if y_train_squeezed.nunique() > 1 and not X_remaining_for_stat.empty:\n",
    "                X_remaining_numeric = X_remaining_for_stat.select_dtypes(include=np.number)\n",
    "                if not X_remaining_numeric.empty:\n",
    "                    num_stat_to_select = min(remaining_slots, X_remaining_numeric.shape[1])\n",
    "                    if num_stat_to_select > 0:\n",
    "                        try:\n",
    "                            # print(f\"Selecting up to {num_stat_to_select} features using Mutual Information from {X_remaining_numeric.shape[1]} numeric features.\") # Less verbose\n",
    "                            selector_mi = SelectKBest(mutual_info_classif, k=num_stat_to_select)\n",
    "                            selector_mi.fit(X_remaining_numeric, y_train_squeezed)\n",
    "                            stat_selected_features = X_remaining_numeric.columns[selector_mi.get_support()].tolist()\n",
    "                            # print(f\"Features selected from Mutual Information: {stat_selected_features}\") # Less verbose\n",
    "                        except Exception as e_mi:\n",
    "                            print(f\"Error in MI based feature selection: {e_mi}. Proceeding without these stat features.\")\n",
    "                    # else: print(\"No statistical features to select (num_stat_to_select is 0).\") # Less verbose\n",
    "                # else: print(\"No numeric features remaining for MI based statistical selection.\") # Less verbose\n",
    "            # else: print(\"Not enough target variance or no remaining features for MI based selection.\") # Less verbose\n",
    "        # else: print(\"No features remaining for MI based statistical selection (features_for_stat_selection is empty).\") # Less verbose\n",
    "    # else: print(\"No remaining slots for statistical feature selection.\") # Less verbose\n",
    "\n",
    "    final_selected_features = list(dict.fromkeys(top_causal_features + stat_selected_features))\n",
    "    if not final_selected_features and not X_train.empty:\n",
    "        print(\"No features from prioritized selection, falling back to simple variance-based selection.\")\n",
    "        simple_fallback_df = simple_feature_selection_fallback(X_train, y_train, max_features)\n",
    "        if simple_fallback_df is not None and 'Feature' in simple_fallback_df.columns:\n",
    "            final_selected_features = simple_fallback_df['Feature'].tolist()\n",
    "        else:\n",
    "            final_selected_features = X_train.columns[:max_features].tolist()\n",
    "\n",
    "    # print(f\"Total prioritized features selected: {len(final_selected_features)}. Top 5: {final_selected_features[:5] if final_selected_features else 'None'}\") # Less verbose\n",
    "    return pd.DataFrame({'Feature': final_selected_features})\n",
    "\n",
    "def configure_extended_context(base_context=512, data_length=750):\n",
    "    max_possible_context = int(data_length * 0.7)\n",
    "    extended_contexts = {\n",
    "        'short_term': min(256, max_possible_context, data_length - 60),\n",
    "        'medium_term': min(512, max_possible_context, data_length - 60),\n",
    "        'long_term': min(1024, max_possible_context, data_length - 60),\n",
    "        'adaptive': min(base_context * 2, max_possible_context, data_length - 60)\n",
    "    }\n",
    "    extended_contexts['adaptive'] = max(extended_contexts['adaptive'], 64)\n",
    "    # print(f\"Context window options for {data_length} data points (ensuring >60 points remain):\") # Less verbose\n",
    "    # for name, length in extended_contexts.items(): # Less verbose\n",
    "        # print(f\"  {name}: {max(1, length)} days\") # Less verbose\n",
    "    return extended_contexts\n",
    "\n",
    "def add_multitimeframe_features(df, price_col='close'):\n",
    "    df_mtf = df.copy()\n",
    "    if price_col not in df_mtf.columns:\n",
    "        print(f\"Price column '{price_col}' not in DataFrame. Skipping multi-timeframe features.\")\n",
    "        return df_mtf\n",
    "\n",
    "    df_mtf[f'{price_col}_weekly_mean'] = df_mtf[price_col].rolling(5, min_periods=1).mean()\n",
    "    df_mtf[f'{price_col}_weekly_std'] = df_mtf[price_col].rolling(5, min_periods=1).std()\n",
    "    df_mtf[f'{price_col}_weekly_max'] = df_mtf[price_col].rolling(5, min_periods=1).max()\n",
    "    df_mtf[f'{price_col}_weekly_min'] = df_mtf[price_col].rolling(5, min_periods=1).min()\n",
    "\n",
    "    df_mtf[f'{price_col}_monthly_mean'] = df_mtf[price_col].rolling(21, min_periods=1).mean()\n",
    "    df_mtf[f'{price_col}_monthly_std'] = df_mtf[price_col].rolling(21, min_periods=1).std()\n",
    "    df_mtf[f'{price_col}_monthly_trend'] = df_mtf[price_col].rolling(21, min_periods=2).apply(\n",
    "        lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) >= 2 else np.nan, raw=False\n",
    "    )\n",
    "    df_mtf[f'{price_col}_quarterly_mean'] = df_mtf[price_col].rolling(63, min_periods=1).mean()\n",
    "    df_mtf[f'{price_col}_quarterly_volatility'] = df_mtf[price_col].rolling(63, min_periods=1).std() / df_mtf[f'{price_col}_quarterly_mean'].replace(0, np.nan)\n",
    "\n",
    "    df_mtf[f'{price_col}_weekly_monthly_ratio'] = df_mtf[f'{price_col}_weekly_mean'] / df_mtf[f'{price_col}_monthly_mean'].replace(0, np.nan)\n",
    "    df_mtf[f'{price_col}_monthly_quarterly_ratio'] = df_mtf[f'{price_col}_monthly_mean'] / df_mtf[f'{price_col}_quarterly_mean'].replace(0, np.nan)\n",
    "\n",
    "    # print(f\"Added {11} multi-timeframe features\") # Less verbose\n",
    "    return df_mtf\n",
    "\n",
    "def detect_volatility_regimes(returns, window=21, threshold_multiplier=1.5):\n",
    "    if returns.empty or len(returns) < window:\n",
    "        # print(f\"Not enough return data ({len(returns)}) for volatility regime detection with window {window}. Defaulting.\") # Less verbose\n",
    "        return pd.Series(1, index=returns.index)\n",
    "\n",
    "    rolling_vol = returns.rolling(window, min_periods=window // 2 if window // 2 > 0 else 1).std()\n",
    "    if rolling_vol.dropna().empty:\n",
    "        # print(\"Rolling volatility is all NaN. Defaulting regimes.\") # Less verbose\n",
    "        return pd.Series(1, index=returns.index)\n",
    "\n",
    "    vol_median = rolling_vol.median()\n",
    "    if pd.isna(vol_median) or vol_median == 0:\n",
    "        vol_median = rolling_vol.mean()\n",
    "        if pd.isna(vol_median) or vol_median == 0:\n",
    "            # print(\"Median and Mean of rolling volatility are NaN/zero. Cannot determine thresholds. Defaulting regimes.\") # Less verbose\n",
    "            return pd.Series(1, index=returns.index)\n",
    "\n",
    "    high_vol_threshold = vol_median * threshold_multiplier\n",
    "    low_vol_threshold = vol_median / threshold_multiplier\n",
    "    regimes = pd.Series(1, index=returns.index, dtype=int)\n",
    "    regimes[rolling_vol >= high_vol_threshold] = 2\n",
    "    regimes[rolling_vol <= low_vol_threshold] = 0\n",
    "    regimes = regimes.fillna(method='ffill').fillna(1)\n",
    "    # regime_counts = regimes.value_counts().sort_index() # Less verbose\n",
    "    # print(f\"Volatility regime distribution:\") # Less verbose\n",
    "    # print(f\"  Low volatility (0): {regime_counts.get(0, 0)} periods\") # Less verbose\n",
    "    # print(f\"  Normal volatility (1): {regime_counts.get(1, 0)} periods\") # Less verbose\n",
    "    # print(f\"  High volatility (2): {regime_counts.get(2, 0)} periods\") # Less verbose\n",
    "    return regimes\n",
    "\n",
    "def add_regime_features(df, returns_col='log_returns', price_col='close'):\n",
    "    df_rf = df.copy()\n",
    "    if returns_col not in df_rf.columns:\n",
    "        print(f\"Returns column '{returns_col}' not found. Skipping regime features.\")\n",
    "        return df_rf\n",
    "    if price_col not in df_rf.columns:\n",
    "        print(f\"Price column '{price_col}' not found for regime-adjusted MAs. Skipping those.\")\n",
    "\n",
    "    regimes = detect_volatility_regimes(df_rf[returns_col])\n",
    "    df_rf['volatility_regime'] = regimes\n",
    "    df_rf['regime_0'] = (regimes == 0).astype(int)\n",
    "    df_rf['regime_1'] = (regimes == 1).astype(int)\n",
    "    df_rf['regime_2'] = (regimes == 2).astype(int)\n",
    "    num_regime_features_added = 4\n",
    "    if price_col in df_rf.columns:\n",
    "        for window in [10, 20, 50]:\n",
    "            df_rf[f'sma_{window}_regime_adj'] = df_rf[price_col].rolling(window, min_periods=1).mean() * (1 + 0.1 * regimes)\n",
    "            num_regime_features_added +=1\n",
    "    # print(f\"Added {num_regime_features_added} regime-based features\") # Less verbose\n",
    "    return df_rf\n",
    "\n",
    "def optimized_lightgbm_params():\n",
    "    # These are reasonable defaults if Optuna is not run\n",
    "    return {\n",
    "        'boosting_type': 'gbdt', 'num_leaves': 31, 'learning_rate': 0.05,\n",
    "        'n_estimators': 200, 'feature_fraction': 0.8, 'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5, 'min_child_samples': 20, 'reg_alpha': 0.01,\n",
    "        'reg_lambda': 0.01, 'random_state': 42, 'verbose': -1, 'n_jobs': -1,\n",
    "        'class_weight': 'balanced',\n",
    "        'min_gain_to_split': 0.0 # Default LightGBM value\n",
    "    }\n",
    "\n",
    "def enhanced_patchtst_finetune(log_returns, context_length=512, prediction_length=5, min_sequences=20):\n",
    "    if not isinstance(log_returns, np.ndarray): log_returns = np.array(log_returns)\n",
    "    if log_returns.ndim > 1 : log_returns = log_returns.squeeze()\n",
    "\n",
    "    sequence_length_needed = context_length + prediction_length\n",
    "    padded_returns = log_returns.copy()\n",
    "\n",
    "    if len(log_returns) < sequence_length_needed:\n",
    "        # print(f\"Insufficient raw data ({len(log_returns)} < {sequence_length_needed}). Applying intelligent padding...\") # Less verbose\n",
    "        mean_return = np.mean(log_returns) if len(log_returns) > 0 else 0\n",
    "        std_return = np.std(log_returns) if len(log_returns) > 1 else 0.01\n",
    "        std_return = max(std_return, 1e-6)\n",
    "        padding_length = sequence_length_needed - len(log_returns)\n",
    "        synthetic_padding = np.random.normal(mean_return, std_return, padding_length)\n",
    "        padded_returns = np.concatenate([synthetic_padding, log_returns])\n",
    "        # print(f\"Applied padding: {padding_length} synthetic points added. New length: {len(padded_returns)}\") # Less verbose\n",
    "\n",
    "    X_sequences, y_sequences = [], []\n",
    "    if len(padded_returns) >= sequence_length_needed:\n",
    "        for i in range(len(padded_returns) - sequence_length_needed + 1):\n",
    "            seq = padded_returns[i : i + context_length]\n",
    "            target = padded_returns[i + context_length : i + sequence_length_needed]\n",
    "            X_sequences.append(seq)\n",
    "            y_sequences.append(target)\n",
    "\n",
    "    if len(X_sequences) >= min_sequences:\n",
    "        # print(f\"Created {len(X_sequences)} training sequences for fine-tuning.\") # Less verbose\n",
    "        return np.array(X_sequences), np.array(y_sequences), True\n",
    "    else:\n",
    "        # print(f\"Insufficient sequences ({len(X_sequences)} < {min_sequences}) after padding. Using pre-trained weights only.\") # Less verbose\n",
    "        return None, None, False\n",
    "\n",
    "def run_patchtst_foundation_forecast(\n",
    "    symbol_name: str,\n",
    "    historical_data_df: pd.DataFrame,\n",
    "    prediction_length: int = 5,\n",
    "    model_checkpoint: str = \"ibm-research/patchtst-etth1-pretrain\",\n",
    "    fine_tune_epochs: int = 10,\n",
    "    enable_fine_tuning: bool = True,\n",
    "    configured_context_length: int | None = None\n",
    "):\n",
    "    print(f\"\\n--- Forecasting for {symbol_name} using PatchTST ({model_checkpoint}) ---\")\n",
    "    if not torch_available or 'PatchTSTForPrediction' not in globals():\n",
    "        print(\"Torch or PatchTST not available. Skipping.\")\n",
    "        return {\"status\": \"failed\", \"reason\": \"Dependencies missing\", \"forecast\": None}\n",
    "    if 'close' not in historical_data_df.columns or historical_data_df['close'].isnull().all():\n",
    "        print(f\"'close' column missing or all NaN for {symbol_name}. Skipping.\")\n",
    "        return {\"status\": \"failed\", \"reason\": \"'close' missing or all NaN\", \"forecast\": None}\n",
    "\n",
    "    df_clean = historical_data_df.dropna(subset=['close'])\n",
    "    if len(df_clean) < 2:\n",
    "        print(f\"Not enough non-NaN close prices ({len(df_clean)}) for {symbol_name}. Skipping.\")\n",
    "        return {\"status\": \"failed\", \"reason\": \"Insufficient non-NaN close prices\", \"forecast\": None}\n",
    "\n",
    "    try:\n",
    "        # print(\"Loading PatchTST configuration...\") # Less verbose\n",
    "        base_config = PatchTSTConfig.from_pretrained(model_checkpoint)\n",
    "        effective_context_length = configured_context_length if configured_context_length is not None else base_config.context_length\n",
    "        min_data_for_one_sequence = effective_context_length + prediction_length + 1\n",
    "        if len(df_clean) < min_data_for_one_sequence:\n",
    "            print(f\"Data length {len(df_clean)} too short for context {effective_context_length} + pred {prediction_length}. Adjusting context or skipping.\")\n",
    "            effective_context_length = max(10, len(df_clean) - prediction_length - 5)\n",
    "            if effective_context_length < 10 :\n",
    "                return {\"status\": \"failed\", \"reason\": f\"Cannot determine valid context with data {len(df_clean)}\", \"forecast\": None}\n",
    "\n",
    "        financial_config = PatchTSTConfig(\n",
    "            context_length=effective_context_length, prediction_length=prediction_length,\n",
    "            patch_length=min(16, effective_context_length // 2 if effective_context_length > 32 else 8),\n",
    "            patch_stride=min(8, effective_context_length // 4 if effective_context_length > 32 else 4),\n",
    "            num_input_channels=1, d_model=base_config.d_model,\n",
    "            num_attention_heads=base_config.num_attention_heads, num_hidden_layers=base_config.num_hidden_layers,\n",
    "            ffn_dim=base_config.ffn_dim, dropout=0.1, head_dropout=0.1, scaling=\"std\", loss=\"mse\"\n",
    "        )\n",
    "        # print(f\"Financial config - Context: {financial_config.context_length}, Pred: {financial_config.prediction_length}, Patch: {financial_config.patch_length}\") # Less verbose\n",
    "        model = PatchTSTForPrediction(financial_config)\n",
    "\n",
    "        # print(\"Loading and transferring compatible weights...\") # Less verbose\n",
    "        try:\n",
    "            pretrained_model = PatchTSTForPrediction.from_pretrained(model_checkpoint, local_files_only=False, trust_remote_code=True)\n",
    "            pretrained_dict = pretrained_model.state_dict()\n",
    "            model_dict = model.state_dict()\n",
    "            compatible_weights = {}\n",
    "            for k, v in pretrained_dict.items():\n",
    "                if k in model_dict and v.size() == model_dict[k].size():\n",
    "                    if not any(skip_layer in k for skip_layer in ['input_embedding', 'projection', 'head', 'value_embedding', 'patch_embedding.weight', 'patch_embedding.bias']):\n",
    "                        compatible_weights[k] = v\n",
    "            model.load_state_dict(compatible_weights, strict=False)\n",
    "            # print(f\"Transferred {len(compatible_weights)}/{len(model_dict)} compatible weights\") # Less verbose\n",
    "        except Exception as e_load:\n",
    "            print(f\"Weight transfer warning/error: {e_load}. Model may use more random init for some layers.\")\n",
    "\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"; model.to(device)\n",
    "        close_prices = df_clean['close'].values.astype(np.float32)\n",
    "        log_prices = np.log(np.maximum(close_prices, 1e-6))\n",
    "        log_returns = np.diff(log_prices)\n",
    "        if len(log_returns) < financial_config.context_length + financial_config.prediction_length:\n",
    "            print(f\"Insufficient log_returns ({len(log_returns)}) after diff for context/pred. Skipping fine-tune/forecast.\")\n",
    "            return {\"status\": \"failed\", \"reason\": \"insufficient log_returns data\", \"forecast\": None}\n",
    "\n",
    "        finetuned_this_run = False\n",
    "        if enable_fine_tuning and fine_tune_epochs > 0:\n",
    "            # print(f\"Attempting fine-tuning on {symbol_name} for {fine_tune_epochs} epochs...\") # Less verbose\n",
    "            X_seqs, y_seqs, finetune_data_ok = enhanced_patchtst_finetune(\n",
    "                log_returns, financial_config.context_length, financial_config.prediction_length)\n",
    "            if finetune_data_ok and X_seqs is not None and len(X_seqs) > 0:\n",
    "                finetuned_this_run = True\n",
    "                train_inputs = torch.tensor(X_seqs, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "                train_targets = torch.tensor(y_seqs, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "                dataset = torch.utils.data.TensorDataset(train_inputs, train_targets)\n",
    "                batch_size = min(16, len(X_seqs) // 2 if len(X_seqs) >= 4 else 1)\n",
    "                if batch_size == 0 and len(X_seqs) > 0: batch_size = 1\n",
    "\n",
    "                if batch_size > 0:\n",
    "                    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True,\n",
    "                                                               drop_last=True if len(X_seqs) > batch_size else False)\n",
    "                    model.train(); optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "                    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=fine_tune_epochs)\n",
    "                    best_loss = float('inf'); patience, patience_counter = 3, 0\n",
    "\n",
    "                    for epoch in range(fine_tune_epochs):\n",
    "                        epoch_loss, num_batches = 0, 0\n",
    "                        if not train_loader: break\n",
    "                        for batch_inputs_data, batch_targets_data in train_loader:\n",
    "                            optimizer.zero_grad()\n",
    "                            outputs = model(past_values=batch_inputs_data, future_values=batch_targets_data)\n",
    "                            loss = outputs.loss\n",
    "                            loss.backward(); torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0); optimizer.step()\n",
    "                            epoch_loss += loss.item(); num_batches += 1\n",
    "                        if num_batches > 0:\n",
    "                            avg_loss = epoch_loss / num_batches; scheduler.step()\n",
    "                            print(f\"Epoch {epoch+1}/{fine_tune_epochs} | Loss: {avg_loss:.6f} | LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "                            if avg_loss < best_loss: best_loss = avg_loss; patience_counter = 0\n",
    "                            else:\n",
    "                                patience_counter += 1\n",
    "                                if patience_counter >= patience: print(f\"Early stopping at epoch {epoch+1}\"); break\n",
    "                        else: print(f\"Epoch {epoch+1}/{fine_tune_epochs} | No batches. Stopping fine-tuning.\"); break\n",
    "                    loss_display = f\"{best_loss:.6f}\" if best_loss != float('inf') else \"N/A\"\n",
    "                    if num_batches > 0: print(f\"Fine-tuning completed. Best loss: {loss_display}\")\n",
    "                else: print(f\"Fine-tuning skipped for {symbol_name}: Not enough sequences or batch_size issue.\"); finetuned_this_run = False\n",
    "            # else: print(f\"Fine-tuning skipped for {symbol_name} due to insufficient sequences from enhanced_patchtst_finetune.\") # Less verbose\n",
    "        # else: print(f\"Fine-tuning disabled or fine_tune_epochs is 0 for {symbol_name}.\") # Less verbose\n",
    "\n",
    "        model.eval()\n",
    "        current_model_context_length = model.config.context_length\n",
    "        if len(log_returns) < current_model_context_length:\n",
    "            # print(f\"Warning: log_returns ({len(log_returns)}) shorter than model context ({current_model_context_length}) for forecast. Padding...\") # Less verbose\n",
    "            mean_lr = np.mean(log_returns) if len(log_returns) > 0 else 0\n",
    "            padding_needed = current_model_context_length - len(log_returns)\n",
    "            past_returns_for_forecast = np.concatenate([np.full(padding_needed, mean_lr), log_returns])\n",
    "        else:\n",
    "            past_returns_for_forecast = log_returns[-current_model_context_length:]\n",
    "\n",
    "        returns_mean = np.mean(past_returns_for_forecast); returns_std = max(np.std(past_returns_for_forecast), 1e-8)\n",
    "        norm_returns_forecast_input = (past_returns_for_forecast - returns_mean) / returns_std\n",
    "        past_tensor = torch.tensor(norm_returns_forecast_input, dtype=torch.float32).view(1, current_model_context_length, 1).to(device)\n",
    "\n",
    "        with torch.no_grad(): outputs = model(past_values=past_tensor)\n",
    "        fc_returns_norm = outputs.prediction_outputs.cpu().numpy().squeeze()\n",
    "        if fc_returns_norm.ndim == 0: fc_returns_norm = np.array([fc_returns_norm])\n",
    "        elif fc_returns_norm.ndim > 1: fc_returns_norm = fc_returns_norm.flatten()\n",
    "\n",
    "        target_pred_len = model.config.prediction_length\n",
    "        if len(fc_returns_norm) < target_pred_len:\n",
    "            last_val = fc_returns_norm[-1] if len(fc_returns_norm) > 0 else 0\n",
    "            fc_returns_norm = np.concatenate([fc_returns_norm, np.full(target_pred_len - len(fc_returns_norm), last_val)])\n",
    "        forecast_log_returns = (fc_returns_norm[:target_pred_len] * returns_std) + returns_mean\n",
    "\n",
    "        last_log_price = log_prices[-1]\n",
    "        forecast_log_prices = last_log_price + np.cumsum(forecast_log_returns)\n",
    "        forecast_prices = np.exp(forecast_log_prices)\n",
    "        forecast_prices = np.maximum(forecast_prices, 0.01).tolist()\n",
    "\n",
    "        last_actual_price = close_prices[-1]\n",
    "        price_change = forecast_prices[-1] - last_actual_price\n",
    "        magnitude_pct = (price_change / last_actual_price) * 100 if last_actual_price != 0 else 0\n",
    "        direction = \"📈 UP\" if price_change > 0.001 * last_actual_price else \"📉 DOWN\" if price_change < -0.001 * last_actual_price else \"횡보 HOLD\"\n",
    "        atr_val = np.nan\n",
    "        if 'ATR_14' in df_clean.columns and not df_clean['ATR_14'].empty: atr_val = df_clean['ATR_14'].iloc[-1]\n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\", \"forecast\": forecast_prices, \"last_price\": float(last_actual_price),\n",
    "            \"direction\": direction, \"magnitude\": float(magnitude_pct), \"confidence\": \"🟡 Medium\",\n",
    "            \"atr_threshold\": float(atr_val) if pd.notna(atr_val) else None,\n",
    "            \"method\": f\"PatchTST {'Fine-tuned' if finetuned_this_run else 'Pre-trained'} ({fine_tune_epochs} epochs attempted)\",\n",
    "            \"model_info\": {\"context_length\": model.config.context_length, \"prediction_length\": model.config.prediction_length, \"fine_tuned_actually\": finetuned_this_run}\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in PatchTST forecasting for {symbol_name}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return {\"status\": \"failed\", \"reason\": f\"PatchTST error: {str(e)}\", \"forecast\": None}\n",
    "\n",
    "def simple_ma_fallback(prices, length=5):\n",
    "    if not isinstance(prices, np.ndarray): prices = np.array(prices)\n",
    "    if len(prices) == 0: return np.full(length, np.nan)\n",
    "    if len(prices) < 5: return np.full(length, prices[-1])\n",
    "    ma_val = np.mean(prices[-5:])\n",
    "    return np.full(length, ma_val)\n",
    "\n",
    "# --- MAIN WORKFLOW FUNCTION ---\n",
    "def run_full_workflow(symbol=DEFAULT_SYMBOL, start_date_str=START_DATE, end_date_str=END_DATE,\n",
    "                      api_key_val=API_KEY, force_train_autoformer=False,\n",
    "                      run_optuna_lgbm=False, use_foundation_model=True):\n",
    "    print(f\"\\n{'='*40}\\n🚀 ENHANCED WORKFLOW FOR: {symbol}\\n{'='*40}\")\n",
    "    default_return = {\n",
    "        \"symbol\": symbol, \"status\": \"Workflow Started\", \"raw_data_shape\": (0,0),\n",
    "        \"featured_data_shape\": (0,0), \"X_train_shape\": (0,0), \"X_test_shape\": (0,0),\n",
    "        \"selected_features_count\": 0, \"selected_feature_names\": [], \"scaler_object\": None,\n",
    "        \"ml_model_object\": None, \"lgbm_feature_importance\": None,\n",
    "        \"causal_model_object\": None, \"causal_feature_ranking\": [],\n",
    "        \"onnx_model_path\": None,\n",
    "        \"forecasting_results\": {\n",
    "            \"configured_context_length\": 512, # Default, will be updated\n",
    "            \"patchtst_forecast\": None,\n",
    "            \"autoformer_forecast\": None\n",
    "        },\n",
    "        \"lgbm_optimized_params\": None\n",
    "    }\n",
    "    price_c, target_col = 'close', 'target'\n",
    "\n",
    "    # Data Fetching\n",
    "    # print(f\"DEBUG: About to fetch data for {symbol} using API key ending with '...{api_key_val[-4:] if len(api_key_val)>4 else api_key_val}'\") # Less verbose\n",
    "    df_raw = fetch_twelve_data(symbol, api_key_val, start_date_str=start_date_str, end_date_str=end_date_str)\n",
    "    if df_raw is None or df_raw.empty:\n",
    "        default_return[\"status\"] = \"Data Fetching Failed\"\n",
    "        print(f\"Workflow aborted for {symbol}: Data Fetching Failed.\")\n",
    "        return default_return\n",
    "    default_return[\"raw_data_shape\"] = df_raw.shape\n",
    "    # print(f\"DEBUG: Data fetched for {symbol}, shape: {df_raw.shape}\") # Less verbose\n",
    "\n",
    "    # Configure Context Windows\n",
    "    # print(f\"\\n--- ⚙️ Configuring Context Windows for {symbol} ---\") # Less verbose\n",
    "    dynamic_context_length = 512\n",
    "    if not df_raw.empty:\n",
    "        context_configs = configure_extended_context(data_length=len(df_raw))\n",
    "        dynamic_context_length = context_configs.get('adaptive', 512)\n",
    "        # print(f\"Dynamically selected context length for {symbol}: {dynamic_context_length} days\") # Less verbose\n",
    "    default_return[\"forecasting_results\"][\"configured_context_length\"] = dynamic_context_length\n",
    "\n",
    "    # Feature Engineering\n",
    "    print(f\"\\n--- 🔧 Feature Engineering: {symbol} ---\")\n",
    "    df_f = df_raw.copy()\n",
    "    df_f = add_technical_indicators(df_f)\n",
    "    df_f = add_optimized_features(df_f, price_col=price_c, volume_col='volume')\n",
    "    df_f = add_wavelet_features(df_f, column=price_c)\n",
    "    df_f = add_entropy_features(df_f, column=price_c, window=40)\n",
    "    df_f = add_advanced_technical_features(df_f, price_col=price_c, high_col='high', low_col='low', volume_col='volume')\n",
    "    df_f = add_transformer_features_conceptual(df_f, column=price_c, sequence_length=20)\n",
    "    df_f = add_multitimeframe_features(df_f, price_col=price_c)\n",
    "\n",
    "    if 'log_returns' in df_f.columns:\n",
    "        df_f = add_regime_features(df_f, returns_col='log_returns', price_col=price_c)\n",
    "    else:\n",
    "        print(f\"Skipping regime features for {symbol} as 'log_returns' column is missing.\")\n",
    "\n",
    "    if 'RSI_14' in df_f.columns and 'ADX_14' in df_f.columns:\n",
    "        df_f['RSI_ADX_interaction'] = df_f['RSI_14'] * df_f['ADX_14'] / 100.0\n",
    "    if 'ATR_14' in df_f.columns and 'volatility_20' in df_f.columns:\n",
    "        volatility_safe = df_f['volatility_20'].replace(0, np.nan)\n",
    "        df_f['ATR_vol_ratio'] = df_f['ATR_14'] / volatility_safe\n",
    "    default_return[\"featured_data_shape\"] = df_f.shape\n",
    "    # print(f\"DEBUG: Feature engineering complete for {symbol}, shape: {df_f.shape}\") # Less verbose\n",
    "\n",
    "    # Data Cleaning Post-Feature Engineering\n",
    "    print(f\"\\n--- Data Cleaning (Inf/NaN Handling & Imputation): {symbol} ---\")\n",
    "    df_f.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    numeric_cols_to_impute = df_f.select_dtypes(include=np.number).columns\n",
    "    if not numeric_cols_to_impute.empty:\n",
    "        nan_counts_before = df_f[numeric_cols_to_impute].isnull().sum().sum()\n",
    "        if nan_counts_before > 0:\n",
    "            print(f\"NaNs before imputation: {nan_counts_before}\")\n",
    "            df_f[numeric_cols_to_impute] = df_f[numeric_cols_to_impute].interpolate(method='linear', limit_direction='both', axis=0)\n",
    "            df_f[numeric_cols_to_impute] = df_f[numeric_cols_to_impute].fillna(method='bfill').fillna(method='ffill').fillna(0)\n",
    "            print(f\"NaNs after imputation: {df_f[numeric_cols_to_impute].isnull().sum().sum()}\")\n",
    "        # else: print(\"No NaNs in numeric features to impute.\") # Less verbose\n",
    "    # else: print(\"No numeric columns for imputation.\") # Less verbose\n",
    "    # print(f\"DEBUG: Data cleaning complete for {symbol}.\") # Less verbose\n",
    "\n",
    "    # Regime Detection and Target Definition\n",
    "    print(f\"\\n--- 📊 Regime Detection (Simplified): {symbol} ---\")\n",
    "    df_f = detect_regimes_simple(df_f, column=price_c)\n",
    "    print(f\"\\n--- 🎯 Target Definition: {symbol} ---\")\n",
    "    df_f = balanced_target_definition(df_f, column=price_c, periods=5)\n",
    "    # print(f\"DEBUG: Target definition complete for {symbol}.\") # Less verbose\n",
    "\n",
    "    # Causal Discovery (Conditional)\n",
    "    causal_model_obj = None\n",
    "    causal_feat_ranking = []\n",
    "    if not SKIP_CAUSAL_ANALYSIS_FOR_DEBUGGING:\n",
    "        print(f\"\\n--- Causal Discovery & Ranking (Actual Process): {symbol} ---\")\n",
    "        if df_f is not None and not df_f.empty and target_col in df_f.columns and df_f[target_col].nunique(dropna=True) > 1:\n",
    "            causal_model_obj = discover_causal_structure(df_f.copy(), target_col=target_col, price_c=price_c, symbol=symbol, max_feats=10)\n",
    "            if causal_model_obj:\n",
    "                print(f\"Causal model object created for {symbol}.\")\n",
    "                # causal_feat_ranking = causal_feature_selection(causal_model_obj, df_f.copy(), target_col=target_col, max_causal_feats=10) # Placeholder\n",
    "            else:\n",
    "                print(f\"Causal model object was not created for {symbol}, skipping feature ranking.\")\n",
    "        else:\n",
    "            print(f\"Skipping Causal Discovery for {symbol} due to data/target issues.\")\n",
    "    else:\n",
    "        print(f\"\\n--- SKIPPING Causal Discovery & Ranking for {symbol} (DEBUG MODE) ---\")\n",
    "    default_return[\"causal_model_object\"] = causal_model_obj\n",
    "    default_return[\"causal_feature_ranking\"] = causal_feat_ranking\n",
    "    # print(f\"DEBUG: Causal discovery section complete for {symbol}.\") # Less verbose\n",
    "\n",
    "    # ML Preparation and Feature Selection\n",
    "    print(f\"\\n--- ML Preparation & Feature Selection: {symbol} ---\")\n",
    "    X_tr, X_te, y_tr, y_te, scaler_obj = None, None, None, None, None\n",
    "    sel_feat_names = []\n",
    "    ml_model = None\n",
    "    lgbm_feat_imp_df = None\n",
    "    onnx_file_path = None\n",
    "    current_status_ml = \"ML Prep Incomplete\"\n",
    "\n",
    "    if df_f is None or df_f.empty or target_col not in df_f.columns or df_f[target_col].isnull().all():\n",
    "        current_status_ml = \"ML Prep Failed - DataFrame empty, target missing, or target all NaN\"\n",
    "        print(f\"{current_status_ml} for {symbol}.\")\n",
    "    elif df_f[target_col].nunique(dropna=True) <= 1:\n",
    "        unique_vals_count = df_f[target_col].nunique(dropna=True)\n",
    "        current_status_ml = f\"ML Prep Skipped - Target has {unique_vals_count} unique non-NaN value(s). Training not meaningful.\"\n",
    "        print(f\"{current_status_ml} for {symbol}.\")\n",
    "    else:\n",
    "        ml_data_prep_output = prepare_ml_data(df_f.copy(), target_col=target_col, test_split_size=0.15, min_test_samples=30)\n",
    "\n",
    "        if ml_data_prep_output is None or not all(item is not None for item in ml_data_prep_output[:4]):\n",
    "            current_status_ml = \"ML Data Preparation Failed or returned insufficient data.\"\n",
    "            print(f\"{current_status_ml} for {symbol}. Skipping subsequent ML steps.\")\n",
    "        else:\n",
    "            X_tr, X_te, y_tr, y_te, scaler_obj = ml_data_prep_output\n",
    "            default_return[\"scaler_object\"] = scaler_obj\n",
    "            default_return[\"X_train_shape\"] = X_tr.shape if X_tr is not None else (0,0)\n",
    "            default_return[\"X_test_shape\"] = X_te.shape if X_te is not None else (0,0)\n",
    "\n",
    "            if X_tr is None or X_tr.empty or y_tr is None or y_tr.empty:\n",
    "                current_status_ml = \"ML Training Data (X_tr or y_tr) is empty after preparation. Skipping training.\"\n",
    "                print(f\"{current_status_ml} for {symbol}.\")\n",
    "            else:\n",
    "                # print(\"Starting feature selection...\") # Less verbose\n",
    "                selected_features_df = prioritized_feature_selection(X_tr.copy(), y_tr.copy(), causal_feat_ranking, max_features=35)\n",
    "\n",
    "                if selected_features_df is not None and 'Feature' in selected_features_df.columns and not selected_features_df.empty:\n",
    "                    sel_feat_names = selected_features_df['Feature'].tolist()\n",
    "                    if not sel_feat_names:\n",
    "                         print(\"Prioritized selection returned no features. Using simple variance fallback on original X_tr.\")\n",
    "                         sel_feat_df_fallback = simple_feature_selection_fallback(X_tr.copy(), y_tr.copy(), max_features=20)\n",
    "                         if sel_feat_df_fallback is not None and 'Feature' in sel_feat_df_fallback.columns:\n",
    "                            sel_feat_names = sel_feat_df_fallback['Feature'].tolist()\n",
    "                         else:\n",
    "                            sel_feat_names = X_tr.columns[:20].tolist() if not X_tr.empty else []\n",
    "                else:\n",
    "                    print(\"Prioritized feature selection failed or returned empty DataFrame. Using simple variance fallback.\")\n",
    "                    sel_feat_df_fallback = simple_feature_selection_fallback(X_tr.copy(), y_tr.copy(), max_features=20)\n",
    "                    if sel_feat_df_fallback is not None and 'Feature' in sel_feat_df_fallback.columns:\n",
    "                        sel_feat_names = sel_feat_df_fallback['Feature'].tolist()\n",
    "                    else:\n",
    "                        sel_feat_names = X_tr.columns[:20].tolist() if not X_tr.empty else []\n",
    "\n",
    "\n",
    "                default_return[\"selected_feature_names\"] = sel_feat_names\n",
    "                default_return[\"selected_features_count\"] = len(sel_feat_names)\n",
    "                print(f\"Selected {len(sel_feat_names)} features: {sel_feat_names[:10]}...\")\n",
    "\n",
    "                if not sel_feat_names:\n",
    "                    current_status_ml = \"No features selected. Skipping LightGBM training.\"\n",
    "                    print(current_status_ml)\n",
    "                else:\n",
    "                    X_tr_selected = X_tr[sel_feat_names].copy()\n",
    "                    X_te_selected = X_te[sel_feat_names].copy() if X_te is not None and not X_te.empty and all(f in X_te.columns for f in sel_feat_names) else pd.DataFrame()\n",
    "\n",
    "\n",
    "                    # Prepare base_params for Optuna, ensuring num_class is set correctly\n",
    "                    num_classes_for_optuna = y_tr.nunique()\n",
    "                    optuna_base_params = optimized_lightgbm_params() # Start with general defaults\n",
    "                    optuna_base_params['objective'] = 'multiclass' if num_classes_for_optuna > 2 else 'binary'\n",
    "                    optuna_base_params['metric'] = 'multi_logloss' if num_classes_for_optuna > 2 else 'binary_logloss'\n",
    "                    if num_classes_for_optuna > 2:\n",
    "                        optuna_base_params['num_class'] = num_classes_for_optuna\n",
    "                    elif 'num_class' in optuna_base_params: # remove if binary\n",
    "                        del optuna_base_params['num_class']\n",
    "\n",
    "                    lgbm_final_params = optuna_base_params.copy() # These will be the params used for training\n",
    "\n",
    "                    if run_optuna_lgbm and optuna_available:\n",
    "                        print(f\"\\n--- Hyperparameter Optimization (Optuna for LightGBM): {symbol} ---\")\n",
    "                        # Pass the correctly configured optuna_base_params to Optuna\n",
    "                        tuned_params_from_optuna = optimize_lgbm_hyperparameters(X_tr_selected.copy(), y_tr.copy(), optuna_base_params, n_trials=50) # Increased trials\n",
    "                        if tuned_params_from_optuna:\n",
    "                           lgbm_final_params.update(tuned_params_from_optuna) # Update with Optuna's best\n",
    "                        default_return[\"lgbm_optimized_params\"] = lgbm_final_params\n",
    "                    else:\n",
    "                         print(\"Optuna HPO for LightGBM skipped or Optuna not available.\")\n",
    "                         default_return[\"lgbm_optimized_params\"] = lgbm_final_params # Store base/default if Optuna not run\n",
    "\n",
    "\n",
    "                    print(f\"\\n--- LightGBM Model Training: {symbol} ---\")\n",
    "                    # Pass the final determined parameters to the training function\n",
    "                    ml_model, lgbm_feat_imp_df = train_lightgbm_model(X_tr_selected, y_tr, X_te_selected, y_te, optimized_params=lgbm_final_params)\n",
    "                    default_return[\"ml_model_object\"] = ml_model\n",
    "                    default_return[\"lgbm_feature_importance\"] = lgbm_feat_imp_df\n",
    "\n",
    "                    if ml_model:\n",
    "                        current_status_ml = \"LightGBM Model Trained\"\n",
    "                        plot_feature_importance(lgbm_feat_imp_df, top_n=20, symbol_for_plot=symbol)\n",
    "                        if not X_tr_selected.empty:\n",
    "                             onnx_file_path = export_lgbm_to_onnx(ml_model, X_tr_selected.head(1), file_path=f\"lgbm_model_{symbol}.onnx\")\n",
    "                             default_return[\"onnx_model_path\"] = onnx_file_path\n",
    "                        else:\n",
    "                            print(\"X_tr_selected is empty. Skipping ONNX export.\")\n",
    "                    else:\n",
    "                        current_status_ml = \"LightGBM Model Training Failed.\"\n",
    "                        print(current_status_ml)\n",
    "    default_return[\"status\"] = current_status_ml\n",
    "    # print(f\"DEBUG: ML section complete for {symbol}. Status: {current_status_ml}\") # Less verbose\n",
    "\n",
    "\n",
    "    # Autoformer Forecasting (Optional)\n",
    "    autoformer_predictions = None\n",
    "    if torch_available and 'AutoformerPredictor' in globals():\n",
    "        # print(f\"\\n--- Autoformer Forecasting (Custom): {symbol} ---\") # Less verbose\n",
    "        if price_c in df_raw.columns and not df_raw[price_c].isnull().all():\n",
    "            series_for_autoformer = df_raw[price_c].copy()\n",
    "            autoformer_input_len = min(90, max(10, len(series_for_autoformer) - 15))\n",
    "            autoformer_pred_len = 5\n",
    "\n",
    "            if autoformer_input_len > 0 and len(series_for_autoformer) > autoformer_input_len + autoformer_pred_len :\n",
    "                autoformer_model = train_or_load_autoformer(series_for_autoformer,\n",
    "                                                            input_len=autoformer_input_len,\n",
    "                                                            pred_len=autoformer_pred_len,\n",
    "                                                            force_train=force_train_autoformer)\n",
    "                if autoformer_model and autoformer_model.model is not None:\n",
    "                    autoformer_predictions = autoformer_model.predict(series_for_autoformer)\n",
    "                    print(f\"Autoformer forecast for {symbol}: {autoformer_predictions}\")\n",
    "                    default_return[\"forecasting_results\"][\"autoformer_forecast\"] = autoformer_predictions.tolist() if autoformer_predictions is not None else None\n",
    "                # else: print(f\"Autoformer model not available/trained for {symbol}.\") # Less verbose\n",
    "            # else: print(f\"Not enough data for Autoformer input/pred length on {symbol} (Data len: {len(series_for_autoformer)}, Needed: >{autoformer_input_len + autoformer_pred_len}).\") # Less verbose\n",
    "        # else: print(f\"Price column '{price_c}' missing or all NaNs in df_raw for Autoformer on {symbol}.\") # Less verbose\n",
    "    # else: print(f\"PyTorch or AutoformerPredictor not available. Skipping Autoformer for {symbol}.\") # Less verbose\n",
    "    # print(f\"DEBUG: Autoformer section complete for {symbol}.\") # Less verbose\n",
    "\n",
    "\n",
    "    # Foundation Model (PatchTST) Forecasting (Optional)\n",
    "    if use_foundation_model and torch_available and 'PatchTSTForPrediction' in globals():\n",
    "        print(f\"\\n--- Foundation Model Forecasting (PatchTST): {symbol} ---\")\n",
    "        patchtst_results = run_patchtst_foundation_forecast(\n",
    "            symbol_name=symbol,\n",
    "            historical_data_df=df_raw.copy(),\n",
    "            prediction_length=5,\n",
    "            enable_fine_tuning=True,\n",
    "            fine_tune_epochs=10, # Increased epochs for PatchTST fine-tuning\n",
    "            configured_context_length=dynamic_context_length\n",
    "        )\n",
    "        default_return[\"forecasting_results\"][\"patchtst_forecast\"] = patchtst_results\n",
    "        if patchtst_results and patchtst_results.get(\"status\") == \"success\":\n",
    "            print(f\"PatchTST Forecast for {symbol}: {patchtst_results.get('forecast')}\")\n",
    "            print(f\"  Direction: {patchtst_results.get('direction')}, Magnitude: {patchtst_results.get('magnitude',0):.2f}%\")\n",
    "    else:\n",
    "        # print(f\"PatchTST forecasting skipped for {symbol} (disabled or dependencies missing).\") # Less verbose\n",
    "        default_return[\"forecasting_results\"][\"patchtst_forecast\"] = {\"status\": \"skipped\", \"reason\": \"Disabled or dependencies missing\"}\n",
    "    # print(f\"DEBUG: PatchTST section complete for {symbol}.\") # Less verbose\n",
    "\n",
    "    print(f\"\\n🏁 Workflow completed for {symbol}. Final Status: {default_return['status']}\")\n",
    "    return default_return\n",
    "\n",
    "# --- Example Usage and Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"DEBUG: Script execution started, entering __main__ block.\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Ensure API_KEY is set correctly.\n",
    "    # For example, you could set it from an environment variable:\n",
    "    # API_KEY = os.environ.get(\"TWELVE_DATA_API_KEY\", \"YOUR_API_KEY_HERE\")\n",
    "    # Or directly in the script for testing (replace \"YOUR_API_KEY_HERE\"):\n",
    "    # API_KEY = \"your_real_api_key_goes_here\" \n",
    "    \n",
    "    # This line should be MODIFIED LOCALLY with your actual key.\n",
    "    # It's inheriting from TWELVE_DATA_API_KEY which is also a placeholder.\n",
    "    # For this run, I will manually set it here if it's the placeholder.\n",
    "    \n",
    "    current_api_key = API_KEY # Uses the global API_KEY\n",
    "\n",
    "    if current_api_key == \"YOUR_API_KEY_HERE\" or not current_api_key:\n",
    "        print(\"🛑 CRITICAL: TWELVE_DATA_API_KEY is not set. Update 'YOUR_API_KEY_HERE' in the script constants or set API_KEY directly.\")\n",
    "        print(\"🛑 CRITICAL: Workflow cannot proceed without a valid API key.\")\n",
    "        # exit() # Uncomment to force exit\n",
    "    else:\n",
    "        print(f\"DEBUG: API_KEY appears to be set. Proceeding. Key ends with '...{current_api_key[-4:] if len(current_api_key)>4 else current_api_key}'\")\n",
    "\n",
    "    symbols_to_run = [\"AAPL\", \"GOOGL\"]\n",
    "    # symbols_to_run = [\"MSFT\"] # For a single quick test\n",
    "    print(f\"DEBUG: Symbols to process: {symbols_to_run}\")\n",
    "    all_results_dict = {}\n",
    "\n",
    "    if current_api_key == \"YOUR_API_KEY_HERE\" or not current_api_key:\n",
    "        print(\"DEBUG: Halting before loop due to missing API Key.\")\n",
    "    else:\n",
    "        for sym_item in symbols_to_run:\n",
    "            print(f\"DEBUG: Starting main loop for symbol: {sym_item}\")\n",
    "            try:\n",
    "                workflow_output = run_full_workflow(\n",
    "                    symbol=sym_item,\n",
    "                    api_key_val=current_api_key, \n",
    "                    run_optuna_lgbm=True, # <<<< MODIFIED TO TRUE\n",
    "                    force_train_autoformer=False, \n",
    "                    use_foundation_model=True \n",
    "                )\n",
    "                all_results_dict[sym_item] = workflow_output\n",
    "                print(f\"\\n--- Results Summary for {sym_item} ---\")\n",
    "                if workflow_output:\n",
    "                    print(f\"  Overall Status: {workflow_output.get('status')}\")\n",
    "                    print(f\"  Raw Data Shape: {workflow_output.get('raw_data_shape')}\")\n",
    "                    print(f\"  Featured Data Shape: {workflow_output.get('featured_data_shape')}\")\n",
    "                    print(f\"  Selected Features Count: {workflow_output.get('selected_features_count')}\")\n",
    "                    \n",
    "                    lgbm_feat_imp = workflow_output.get(\"lgbm_feature_importance\")\n",
    "                    if lgbm_feat_imp is not None and not lgbm_feat_imp.empty:\n",
    "                        print(f\"  Top LGBM Features: {lgbm_feat_imp['Feature'].head(3).tolist()}\")\n",
    "                    \n",
    "                    forecast_summary = workflow_output.get(\"forecasting_results\", {})\n",
    "                    patchtst_info = forecast_summary.get(\"patchtst_forecast\")\n",
    "                    autoformer_info = forecast_summary.get(\"autoformer_forecast\")\n",
    "\n",
    "                    if patchtst_info and isinstance(patchtst_info, dict) and patchtst_info.get(\"status\") == \"success\":\n",
    "                        print(f\"  PatchTST Forecast ({patchtst_info.get('method', 'N/A')}):\")\n",
    "                        print(f\"    Values: {patchtst_info.get('forecast')}\")\n",
    "                        print(f\"    Direction: {patchtst_info.get('direction')}, Magnitude: {patchtst_info.get('magnitude', 0):.2f}%\")\n",
    "                    elif patchtst_info and isinstance(patchtst_info, dict):\n",
    "                        print(f\"  PatchTST Status: {patchtst_info.get('status')}, Reason: {patchtst_info.get('reason')}\")\n",
    "                    \n",
    "                    if autoformer_info:\n",
    "                         print(f\"  Autoformer Forecast: {autoformer_info}\")\n",
    "                    \n",
    "                    print(f\"  ONNX Model Path: {workflow_output.get('onnx_model_path')}\")\n",
    "                print(\"-\" * 40)\n",
    "                # print(f\"DEBUG: Completed processing for symbol: {sym_item}\") # Less verbose\n",
    "\n",
    "            except Exception as e_main_loop:\n",
    "                print(f\"🛑 ERROR: Unhandled exception in main loop for symbol {sym_item}: {e_main_loop}\")\n",
    "                traceback.print_exc()\n",
    "                all_results_dict[sym_item] = {\"status\": f\"Error: {e_main_loop}\", \"symbol\": sym_item}\n",
    "                # print(f\"DEBUG: Error processing symbol: {sym_item}\") # Less verbose\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time for {len(symbols_to_run)} symbol(s): {(end_time - start_time):.2f} seconds.\")\n",
    "    print(\"DEBUG: Script __main__ block finished.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6aabb2-d6ff-4286-9395-cbf1407f3ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Stock Analysis Py310",
   "language": "python",
   "name": "stock_py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
