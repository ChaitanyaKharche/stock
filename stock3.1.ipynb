{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6d82b01-9e98-4cdd-bf37-d0d484568445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna imported successfully.\n",
      "PyTorch imported successfully.\n",
      "PyTorch CUDA available: True, Version: 12.1\n",
      "Using PyTorch on GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "Neural ODE features are DISABLED by configuration.\n",
      "skbase (scikit-base) 0.6.2 imported successfully.\n",
      "Attempting to import sktime specific modules for sktime version: 0.24.0\n",
      "Sktime 0.24.0 imported successfully (ClaSPSegmentation from annotation.clasp).\n",
      "DoWhy 0.10 and NetworkX 3.1 imported successfully.\n",
      "CausalML (for DoWhy extras) not found. Some causal methods might be limited.\n",
      "HuggingFace Autoformer is DISABLED by configuration.\n",
      "ONNX, ONNXRuntime, skl2onnx, and onnxmltools imported successfully.\n",
      "Onnxmltools version: 1.11.1\n",
      "\n",
      "All libraries and modules conditional imports attempted.\n",
      "\n",
      "============================================================\n",
      "🚀 TO UNLOCK ULTIMATE FORECASTING CAPABILITIES:\n",
      "============================================================\n",
      "Run these commands in your terminal:\n",
      "pip install darts\n",
      "pip install prophet\n",
      "pip install neuralprophet\n",
      "pip install pytorch-forecasting\n",
      "pip install sktime\n",
      "\n",
      "Then restart this script for cutting-edge forecasting!\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/skl2onnx/algebra/onnx_ops.py:159: UserWarning: OpSchema.FormalParameter.typeStr is deprecated and will be removed in 1.16. Use OpSchema.FormalParameter.type_str instead.\n",
      "  tys = obj.typeStr or ''\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/skl2onnx/algebra/automation.py:154: UserWarning: OpSchema.FormalParameter.isHomogeneous is deprecated and will be removed in 1.16. Use OpSchema.FormalParameter.is_homogeneous instead.\n",
      "  if getattr(obj, 'isHomogeneous', False):\n",
      "/home/chaitanyakharche/Desktop/stock/venv_py310/lib/python3.10/site-packages/jinja2/environment.py:490: UserWarning: OpSchema.FormalParameter.typeStr is deprecated and will be removed in 1.16. Use OpSchema.FormalParameter.type_str instead.\n",
      "  return getattr(obj, attribute)\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_ta\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import pywt\n",
    "import antropy as ant\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, f_classif\n",
    "\n",
    "\n",
    "if hasattr(pd.DataFrame, 'ta') is False and pandas_ta is not None:\n",
    "    try:\n",
    "        pandas_ta.Core.register(with_pandas=True)\n",
    "        print(\"pandas_ta DataFrame accessor registered globally.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not globally register pandas_ta accessor: {e}\")\n",
    "\n",
    "optuna_available = False\n",
    "try:\n",
    "    import optuna\n",
    "    optuna_available = True\n",
    "    print(\"Optuna imported successfully.\")\n",
    "except ImportError:\n",
    "    print(\"Optuna not found. LightGBM hyperparameter optimization with Optuna will be skipped.\")\n",
    "\n",
    "torch_available = False\n",
    "torchdyn_available = False\n",
    "NeuralODE = None \n",
    "DISABLE_NEURAL_ODE = True # Disabled to reduce complexity and potential issues unless specifically needed\n",
    "DISABLE_AUTOFORMER = True # Disabled as per previous fixes focusing on custom transformer\n",
    "try:\n",
    "    import torch\n",
    "    torch_available = True\n",
    "    print(\"PyTorch imported successfully.\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"PyTorch CUDA available: True, Version: {torch.version.cuda}\")\n",
    "        print(f\"Using PyTorch on GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"PyTorch CUDA available: False.\")\n",
    "\n",
    "    if not DISABLE_NEURAL_ODE:\n",
    "        try:\n",
    "            from torchdyn.core import NeuralODE as NeuralODE_from_torchdyn\n",
    "            import torchdyn\n",
    "            torchdyn_available = True\n",
    "            NeuralODE = NeuralODE_from_torchdyn\n",
    "            print(f\"TorchDyn {torchdyn.__version__} imported successfully. NeuralODE class is ready.\")\n",
    "        except ImportError:\n",
    "            print(\"TorchDyn not found. Neural ODE features will be SKIPPED (ImportError).\")\n",
    "            torchdyn_available = False\n",
    "        except Exception as e_torchdyn_other:\n",
    "            print(f\"TorchDyn import failed: {e_torchdyn_other}. Neural ODE features will be SKIPPED.\")\n",
    "            torchdyn_available = False\n",
    "    else:\n",
    "        print(\"Neural ODE features are DISABLED by configuration.\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch not found. Neural ODE and some Transformer features will be SKIPPED.\")\n",
    "\n",
    "sktime_available = False\n",
    "ClaSPSegmentation = None\n",
    "plot_series = None\n",
    "skbase_available = False\n",
    "try:\n",
    "    import skbase\n",
    "    skbase_available = True\n",
    "    print(f\"skbase (scikit-base) {skbase.__version__} imported successfully.\")\n",
    "except ImportError:\n",
    "    print(\"skbase (scikit-base) not found.\")\n",
    "\n",
    "try:\n",
    "    import sktime\n",
    "    print(f\"Attempting to import sktime specific modules for sktime version: {sktime.__version__}\")\n",
    "    from sktime.annotation.clasp import ClaSPSegmentation\n",
    "    try:\n",
    "        from sktime.utils.plotting import plot_series\n",
    "    except ImportError:\n",
    "        plot_series = None\n",
    "    sktime_available = True\n",
    "    print(f\"Sktime {sktime.__version__} imported successfully (ClaSPSegmentation from annotation.clasp).\")\n",
    "except ImportError as e_sktime_import:\n",
    "    print(f\"Sktime modules import failed: {e_sktime_import}. Regime detection will be skipped.\")\n",
    "except Exception as e_sktime_general:\n",
    "    print(f\"A general error occurred during sktime import or setup: {e_sktime_general}\")\n",
    "\n",
    "dowhy_available = False\n",
    "CausalModel = None\n",
    "nx = None # networkx\n",
    "causalml_available = False\n",
    "try:\n",
    "    import dowhy\n",
    "    from dowhy import CausalModel\n",
    "    import networkx as nx\n",
    "    dowhy_available = True\n",
    "    print(f\"DoWhy {dowhy.__version__} and NetworkX {nx.__version__} imported successfully.\")\n",
    "    try:\n",
    "        import causalml\n",
    "        causalml_available = True\n",
    "        version_str = getattr(causalml, '__version__', '(version not found)')\n",
    "        print(f\"CausalML imported successfully version {version_str} (for DoWhy extras).\")\n",
    "    except ImportError:\n",
    "        print(\"CausalML (for DoWhy extras) not found. Some causal methods might be limited.\")\n",
    "except ImportError:\n",
    "    print(\"DoWhy or NetworkX not found. Causal Discovery will be skipped.\")\n",
    "\n",
    "transformers_available = False\n",
    "AutoformerConfig, AutoformerForPrediction = None, None\n",
    "if not DISABLE_AUTOFORMER: # Only attempt if not disabled\n",
    "    try:\n",
    "        from transformers import AutoformerConfig, AutoformerForPrediction\n",
    "        transformers_available = True\n",
    "        print(\"Hugging Face Transformers imported successfully.\")\n",
    "    except ImportError as e:\n",
    "        print(f\"Hugging Face Transformers import failed: {e}. Autoformer will be skipped.\")\n",
    "    except Exception as e_generic_transformers:\n",
    "        print(f\"An unexpected error occurred importing Transformers: {e_generic_transformers}\")\n",
    "else:\n",
    "    print(\"HuggingFace Autoformer is DISABLED by configuration.\")\n",
    "\n",
    "\n",
    "onnx_available = False\n",
    "ort_available = False # onnxruntime\n",
    "skl2onnx_available = False\n",
    "onnxmltools_available = False\n",
    "FloatTensorType = None # from skl2onnx\n",
    "try:\n",
    "    import onnx\n",
    "    onnx_available = True\n",
    "    import onnxruntime as ort\n",
    "    ort_available = True\n",
    "    import skl2onnx\n",
    "    from skl2onnx.common.data_types import FloatTensorType\n",
    "    skl2onnx_available = True\n",
    "    import onnxmltools\n",
    "    onnxmltools_available = True\n",
    "    print(\"ONNX, ONNXRuntime, skl2onnx, and onnxmltools imported successfully.\")\n",
    "    if hasattr(onnxmltools, '__version__'):\n",
    "         print(f\"Onnxmltools version: {onnxmltools.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"One or more ONNX components not found: {e}. ONNX features will be skipped.\")\n",
    "\n",
    "print(\"\\nAll libraries and modules conditional imports attempted.\")\n",
    "\n",
    "# --- Constants ---\n",
    "TWELVE_DATA_API_KEY = \"b6dbb92e551a46f2b20de27540aeef0a\" # Replace with your actual key if needed\n",
    "API_KEY = TWELVE_DATA_API_KEY\n",
    "DEFAULT_SYMBOL = \"MSFT\"\n",
    "START_DATE = (datetime.now() - timedelta(days=3*365)).strftime('%Y-%m-%d')\n",
    "END_DATE = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "# --- Function Definitions ---\n",
    "\n",
    "class AutoformerPredictor: # Used by lightweight_transformer_forecast\n",
    "    def __init__(self, input_len=60, pred_len=5, d_model=64, n_heads=8):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() and torch_available else 'cpu'\n",
    "        if torch_available:\n",
    "            print(f\"🔧 Custom Autoformer using device: {self.device}\")\n",
    "            self.model = torch.nn.Transformer(\n",
    "                d_model=d_model, nhead=n_heads, num_encoder_layers=2,\n",
    "                num_decoder_layers=1, dim_feedforward=256, activation='gelu'\n",
    "            ).to(self.device)\n",
    "            self.enc_embedding = torch.nn.Linear(1, d_model).to(self.device)\n",
    "            self.dec_embedding = torch.nn.Linear(1, d_model).to(self.device)\n",
    "            self.projection = torch.nn.Linear(d_model, 1).to(self.device)\n",
    "        else:\n",
    "            self.model = None # Should not be used if torch is not available\n",
    "            print(\"PyTorch not available for AutoformerPredictor.\")\n",
    "        self.input_len = input_len\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "    def forward(self, src):\n",
    "        if not torch_available or self.model is None: return None\n",
    "        if isinstance(src, np.ndarray):\n",
    "            src = torch.tensor(src, dtype=torch.float32)\n",
    "        src = src.to(self.device)\n",
    "        if src.dim() == 1: src = src.unsqueeze(0)\n",
    "        src = src.unsqueeze(-1)\n",
    "        memory = self.model.encoder(self.enc_embedding(src).permute(1, 0, 2))\n",
    "        tgt = torch.zeros(src.shape[0], self.pred_len, 1, device=self.device)\n",
    "        output = self.model.decoder(self.dec_embedding(tgt).permute(1, 0, 2), memory)\n",
    "        return self.projection(output.permute(1, 0, 2)).squeeze(-1)\n",
    "\n",
    "    def predict(self, series_data):\n",
    "        if not torch_available or self.model is None: return np.array([np.nan]*self.pred_len) # Return NaNs if no torch\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            if isinstance(series_data, pd.Series): series_data = series_data.values\n",
    "            if isinstance(series_data, np.ndarray):\n",
    "                series = torch.tensor(series_data, dtype=torch.float32)\n",
    "            elif isinstance(series_data, torch.Tensor):\n",
    "                series = series_data.float()\n",
    "            else:\n",
    "                print(\"Unsupported series data type for predict\")\n",
    "                return np.array([np.nan]*self.pred_len)\n",
    "\n",
    "            if len(series) < self.input_len:\n",
    "                padding = torch.zeros(self.input_len - len(series), device=self.device if series.is_cuda else 'cpu') # Match device\n",
    "                series = torch.cat([padding, series.to(padding.device)]) # Ensure same device\n",
    "            else:\n",
    "                series = series[-self.input_len:]\n",
    "            \n",
    "            series = series.to(self.device) # Final move to model device\n",
    "            mean_val = series.mean()\n",
    "            std_val = series.std() + 1e-8\n",
    "            norm_series = (series - mean_val) / std_val\n",
    "            preds = self.forward(norm_series.unsqueeze(0))\n",
    "            if preds is None: return np.array([np.nan]*self.pred_len)\n",
    "            denorm_preds = preds.squeeze(0) * std_val + mean_val\n",
    "            return denorm_preds.cpu().numpy()\n",
    "\n",
    "def fetch_twelve_data(symbol, api_key, start_date_str=None, end_date_str=None):\n",
    "    base_url = \"https://api.twelvedata.com/time_series\"\n",
    "    params = {\n",
    "        \"symbol\": symbol, \"interval\": \"1day\", \"apikey\": api_key, \"format\": \"JSON\",\n",
    "    }\n",
    "    if start_date_str: params[\"start_date\"] = start_date_str\n",
    "    if end_date_str: params[\"end_date\"] = end_date_str\n",
    "    print(f\"Fetching data for {symbol} from Twelve Data (interval=1day, from {start_date_str} to {end_date_str})...\")\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed for {symbol}: {e}\"); return None\n",
    "    except ValueError as e:\n",
    "        print(f\"Failed to parse JSON for {symbol}: {e}. Response: {response.text[:200]}...\"); return None\n",
    "    if data.get(\"status\") == \"error\" or \"values\" not in data:\n",
    "        print(f\"API Error for {symbol} (Code: {data.get('code')}): {data.get('message', 'Unknown error')}\"); return None\n",
    "    if not data[\"values\"]:\n",
    "        print(f\"No data values for {symbol} for the period.\"); return None\n",
    "    df = pd.DataFrame(data[\"values\"]).rename(columns={'datetime': 'date'})\n",
    "    for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "        if col in df.columns: df[col] = pd.to_numeric(df[col], errors='coerce').astype('float64')\n",
    "    if 'date' not in df.columns: print(\"Critical Error: 'date' column missing.\"); return None\n",
    "    df.index = pd.to_datetime(df['date'])\n",
    "    df.drop(columns=['date'], inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    df.dropna(subset=[col for col in ['open', 'high', 'low', 'close'] if col in df.columns], inplace=True)\n",
    "    if df.empty: print(f\"No data remaining for {symbol} after processing.\"); return None\n",
    "    print(f\"Successfully fetched/processed {len(df)} data points for {symbol}.\"); return df\n",
    "\n",
    "def add_technical_indicators(df):\n",
    "    df_feat = df.copy()\n",
    "    if not hasattr(df_feat, 'ta'): print(\"pandas_ta not available on DataFrame.\"); return df_feat\n",
    "    print(\"Adding optimized technical indicators...\")\n",
    "    try:\n",
    "        c, h, l, v = 'close', 'high', 'low', 'volume'\n",
    "        df_feat.ta.rsi(close=df_feat[c], length=14, append=True, col_names='RSI_14')\n",
    "        df_feat.ta.rsi(close=df_feat[c], length=9, append=True, col_names='RSI_9')\n",
    "        df_feat.ta.rsi(close=df_feat[c], length=25, append=True, col_names='RSI_25')\n",
    "        df_feat.ta.macd(close=df_feat[c], fast=12, slow=26, signal=9, append=True) # Generates MACD_12_26_9, MACDh_12_26_9, MACDs_12_26_9\n",
    "        df_feat.ta.macd(close=df_feat[c], fast=5, slow=15, signal=9, append=True, col_names=('MACD_5_15_9', 'MACDh_5_15_9', 'MACDs_5_15_9'))\n",
    "        for p in [10, 20, 50, 100, 200]:\n",
    "            df_feat.ta.sma(close=df_feat[c], length=p, append=True, col_names=f'SMA_{p}')\n",
    "            df_feat.ta.ema(close=df_feat[c], length=p, append=True, col_names=f'EMA_{p}')\n",
    "        df_feat.ta.bbands(close=df_feat[c], length=20, std=2, append=True) # Generates BBL_20_2.0, BBM_20_2.0, BBU_20_2.0, BBB_20_2.0, BBP_20_2.0\n",
    "        if all(x in df_feat.columns for x in [h,l,c]):\n",
    "            df_feat.ta.atr(high=df_feat[h], low=df_feat[l], close=df_feat[c], length=14, append=True) # ATR_14\n",
    "            df_feat.ta.adx(high=df_feat[h], low=df_feat[l], close=df_feat[c], length=14, append=True) # ADX_14, DMP_14, DMN_14\n",
    "            df_feat.ta.stoch(high=df_feat[h], low=df_feat[l], close=df_feat[c], append=True) # STOCHk_14_3_3, STOCHd_14_3_3\n",
    "            df_feat.ta.willr(high=df_feat[h], low=df_feat[l], close=df_feat[c], append=True) # WILLR_14\n",
    "            df_feat.ta.cci(high=df_feat[h], low=df_feat[l], close=df_feat[c], append=True) # CCI_14_0.015\n",
    "        if all(x in df_feat.columns for x in [h,l,c,v]):\n",
    "             df_feat.ta.mfi(high=df_feat[h], low=df_feat[l], close=df_feat[c], volume=df_feat[v], append=True) # MFI_14\n",
    "        df_feat.columns = df_feat.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
    "        print(\"Optimized TIs added.\")\n",
    "    except Exception as e: print(f\"Error adding TIs: {e}\\n{traceback.format_exc()}\")\n",
    "    return df_feat\n",
    "\n",
    "def add_optimized_features(df, price_col='close', volume_col='volume'):\n",
    "    print(\"Adding optimized features with safeguards...\")\n",
    "    df_new = df.copy()\n",
    "    df_new['returns'] = df_new[price_col].pct_change()\n",
    "    # Safeguard log returns\n",
    "    safe_price = df_new[price_col].replace(0, np.nan)\n",
    "    safe_price_shifted = df_new[price_col].shift(1).replace(0, np.nan)\n",
    "    df_new['log_returns'] = np.log(safe_price / safe_price_shifted)\n",
    "\n",
    "    for window in [5, 10, 20]:\n",
    "        df_new[f'volatility_{window}'] = df_new['returns'].rolling(window).std()\n",
    "        df_new[f'skew_{window}'] = df_new['returns'].rolling(window).skew()\n",
    "        df_new[f'kurtosis_{window}'] = df_new['returns'].rolling(window).kurt()\n",
    "    \n",
    "    if volume_col in df.columns and price_col in df.columns:\n",
    "        rolling_mean_volume = df_new[volume_col].rolling(20).mean().replace(0, np.nan)\n",
    "        df_new['volume_ratio'] = df_new[volume_col] / rolling_mean_volume\n",
    "        df_new['price_volume'] = df_new[price_col] * df_new[volume_col]\n",
    "        df_new['volume_change'] = df_new[volume_col].pct_change()\n",
    "    \n",
    "    if all(col in df.columns for col in ['high', 'low', 'close']):\n",
    "        safe_low = df_new['low'].replace(0, np.nan)\n",
    "        safe_high = df_new['high'].replace(0, np.nan)\n",
    "        safe_close = df_new['close'].replace(0, np.nan)\n",
    "        df_new['high_low_ratio'] = df_new['high'] / safe_low\n",
    "        df_new['close_to_high'] = df_new['close'] / safe_high\n",
    "        df_new['close_to_low'] = df_new['close'] / safe_low\n",
    "        df_new['intraday_range'] = (df_new['high'] - df_new['low']) / safe_close\n",
    "    \n",
    "    if 'RSI_14' in df_new.columns:\n",
    "        df_new['RSI_signal'] = 0\n",
    "        df_new.loc[df_new['RSI_14'] < 30, 'RSI_signal'] = 1\n",
    "        df_new.loc[df_new['RSI_14'] > 70, 'RSI_signal'] = -1\n",
    "    \n",
    "    # Use the standard MACD column names from pandas_ta: MACD_12_26_9 and MACDs_12_26_9\n",
    "    if 'MACD_12_26_9' in df_new.columns and 'MACDs_12_26_9' in df_new.columns:\n",
    "        df_new['MACD_signal'] = (df_new['MACD_12_26_9'] > df_new['MACDs_12_26_9']).astype(int)\n",
    "    \n",
    "    # nan_threshold = 0.3 # Consider moving NaN column dropping to after all features are made and imputed\n",
    "    # for col in df_new.columns:\n",
    "    #     if df_new[col].isna().sum() / len(df_new) > nan_threshold:\n",
    "    #         df_new = df_new.drop(columns=[col])\n",
    "    #         print(f\"Dropped {col} due to >{nan_threshold*100}% NaN values in add_optimized_features\")\n",
    "    return df_new\n",
    "\n",
    "def add_wavelet_features(df, column='close', wavelet='mexh', scales_range=(1, 32)):\n",
    "    df_feat = df.copy(); print(f\"Adding CWT for '{column}'...\")\n",
    "    if pywt is None: print(\"PyWavelets not available.\"); return df_feat\n",
    "    try:\n",
    "        signal = df_feat[column].values\n",
    "        if len(signal) < scales_range[1] + 5 : print(f\"Signal length {len(signal)} too short. Skipping wavelets.\"); return df_feat # Added buffer\n",
    "        actual_max_scale = min(scales_range[1], len(signal) // 2 -1) # Max scale constraint for CWT\n",
    "        if actual_max_scale < scales_range[0]: print(\"Max scale too small after constraint. Skipping wavelets.\"); return df_feat\n",
    "        scales = np.arange(scales_range[0], actual_max_scale + 1)\n",
    "        if len(scales)==0: print(\"No valid scales for CWT. Skipping wavelets.\"); return df_feat\n",
    "        coefficients, _ = pywt.cwt(signal, scales, wavelet)\n",
    "        coeffs_df = pd.DataFrame(coefficients.T, index=df_feat.index, columns=[f\"cwt_scale_{s}\" for s in scales])\n",
    "        df_feat['cwt_mean'] = coeffs_df.mean(axis=1)\n",
    "        df_feat['cwt_std'] = coeffs_df.std(axis=1)\n",
    "        for s_idx in np.linspace(0, len(scales)-1, min(5, len(scales)), dtype=int): # Ensure s_idx is valid\n",
    "            s = scales[s_idx]\n",
    "            df_feat[f'cwt_energy_s{s}'] = coeffs_df[f\"cwt_scale_{s}\"]**2\n",
    "        print(\"Wavelet features added.\")\n",
    "    except Exception as e: print(f\"Error adding wavelet features: {e}\\n{traceback.format_exc()}\")\n",
    "    return df_feat\n",
    "\n",
    "def add_entropy_features(df, column='close', window=40): # Increased default window\n",
    "    df_feat = df.copy(); print(f\"Adding Entropy for '{column}' (window={window})...\")\n",
    "    if ant is None: print(\"Antropy not available.\"); return df_feat\n",
    "    if len(df_feat) < window + 15: print(f\"Data too short for entropy (window={window}). Skipping.\"); return df_feat # Increased buffer\n",
    "    try:\n",
    "        sig = df_feat[column]\n",
    "        df_feat['entropy_sample'] = sig.rolling(window=window, min_periods=window).apply(lambda x: ant.sample_entropy(x) if pd.notna(x).all() and len(x)==window else np.nan, raw=False)\n",
    "        df_feat['entropy_spectral'] = sig.rolling(window=window, min_periods=window).apply(lambda x: ant.spectral_entropy(x, sf=1, method='welch', nperseg=min(len(x), window//2 if window//2 > 0 else 1)) if pd.notna(x).all() and len(x)==window else np.nan, raw=False)\n",
    "        print(\"Entropy features added.\")\n",
    "    except Exception as e: print(f\"Error adding entropy features: {e}\\n{traceback.format_exc()}\")\n",
    "    return df_feat\n",
    "\n",
    "def add_advanced_technical_features(df, price_col='close', high_col='high', low_col='low', volume_col='volume'):\n",
    "    print(\"Adding advanced TIs...\")\n",
    "    df_new = df.copy()\n",
    "    if not hasattr(df_new, 'ta'): print(\"pandas_ta not available on DataFrame.\"); return df_new\n",
    "    try:\n",
    "        if not all(c in df_new.columns for c in [price_col, high_col, low_col]): print(\"Missing OHLC for adv TIs\"); return df_new\n",
    "        df_new.ta.mom(close=df_new[price_col], append=True, col_names='MOM_14')\n",
    "        df_new.ta.roc(close=df_new[price_col], append=True, col_names='ROC_10')\n",
    "        df_new.ta.natr(high=df_new[high_col], low=df_new[low_col], close=df_new[price_col], append=True, col_names='NATR_14')\n",
    "        df_new.ta.aroon(high=df_new[high_col], low=df_new[low_col], append=True) # AROOND_14, AROONU_14, AROONOSC_14\n",
    "        df_new.ta.stc(close=df_new[price_col], tclength=23, fast=50, slow=100, factor=0.5, append=True, col_names=('STC_23_50_05', 'STCD_23_50_05', 'STCK_23_50_05')) # Check if tclength is right for pandas-ta version\n",
    "        if volume_col in df_new.columns:\n",
    "            df_new.ta.pvol(close=df_new[price_col], volume=df_new[volume_col], append=True, col_names='PVOL')\n",
    "            df_new.ta.cmf(high=df_new[high_col], low=df_new[low_col], close=df_new[price_col], volume=df_new[volume_col], append=True, col_names='CMF_20')\n",
    "        df_new.columns = df_new.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
    "        print(\"Advanced TIs added.\")\n",
    "    except Exception as e: print(f\"Error adding advanced TIs: {e}\\n{traceback.format_exc()}\")\n",
    "    return df_new\n",
    "\n",
    "def add_transformer_features_conceptual(df, column='close', sequence_length=20):\n",
    "    df_original_copy = df.copy()\n",
    "    df_feat_temp = df.copy()\n",
    "    print(f\"Adding Transformer-inspired features (Conceptual) for '{column}'...\")\n",
    "    if len(df_feat_temp) < sequence_length + 5: print(f\"Data too short for conceptual transformer features. Skipping.\"); return df_original_copy\n",
    "    feature_col_names = ['trans_seq_mean', 'trans_seq_std', 'trans_seq_trend', 'trans_seq_volatility', 'trans_seq_autocorr1']\n",
    "    for col_name in feature_col_names: df_feat_temp[col_name] = np.nan\n",
    "    try:\n",
    "        data_series = df_feat_temp[column].values; sequences = []\n",
    "        if len(data_series) >= sequence_length:\n",
    "            for i in range(len(data_series) - sequence_length + 1): sequences.append(data_series[i : i + sequence_length])\n",
    "        if not sequences: print(\"No sequences for transformer features.\"); return df_original_copy\n",
    "        sequences_np = np.array(sequences, dtype=float)\n",
    "        normalized_sequences_list = []\n",
    "        for seq_val in sequences_np:\n",
    "            if len(seq_val) == 0: normalized_sequences_list.append(np.array([])); continue\n",
    "            mean_val, std_val = np.mean(seq_val), np.std(seq_val)\n",
    "            if std_val > 1e-8: normalized_sequences_list.append((seq_val - mean_val) / std_val)\n",
    "            elif len(seq_val) > 0 : normalized_sequences_list.append(np.zeros_like(seq_val))\n",
    "            else: normalized_sequences_list.append(seq_val - mean_val)\n",
    "        if not normalized_sequences_list: print(\"No normalized sequences.\"); return df_original_copy\n",
    "        try:\n",
    "            normalized_sequences_np = np.array([arr if len(arr) == sequence_length else np.full(sequence_length, np.nan) for arr in normalized_sequences_list], dtype=float)\n",
    "        except ValueError as e_np: print(f\"Error converting norm_seqs: {e_np}. Skipping.\"); return df_original_copy\n",
    "        if not (normalized_sequences_np.ndim == 2 and normalized_sequences_np.shape[0] > 0 and normalized_sequences_np.shape[1] == sequence_length): print(f\"Norm_seqs unexpected shape: {normalized_sequences_np.shape}. Skipping.\"); return df_original_copy\n",
    "        for i_seq in range(len(normalized_sequences_np)):\n",
    "            current_norm_seq = normalized_sequences_np[i_seq]\n",
    "            idx_assign = i_seq + sequence_length - 1\n",
    "            if idx_assign >= len(df_feat_temp.index): continue\n",
    "            df_idx = df_feat_temp.index[idx_assign]\n",
    "            if np.isnan(current_norm_seq).all(): continue\n",
    "            df_feat_temp.loc[df_idx, 'trans_seq_mean'] = np.nanmean(current_norm_seq)\n",
    "            df_feat_temp.loc[df_idx, 'trans_seq_std'] = np.nanstd(current_norm_seq)\n",
    "            valid_seq = current_norm_seq[~np.isnan(current_norm_seq)]\n",
    "            if len(valid_seq) > 1:\n",
    "                try:\n",
    "                    slope = np.polyfit(np.arange(len(valid_seq)), valid_seq, 1)[0]\n",
    "                    df_feat_temp.loc[df_idx, 'trans_seq_trend'] = slope if not np.isnan(slope) else 0.0\n",
    "                except (np.linalg.LinAlgError, ValueError, TypeError): df_feat_temp.loc[df_idx, 'trans_seq_trend'] = 0.0\n",
    "                diff_valid = np.diff(valid_seq)\n",
    "                df_feat_temp.loc[df_idx, 'trans_seq_volatility'] = np.std(diff_valid) if len(diff_valid) > 0 else 0.0\n",
    "                if len(valid_seq) >= 2:\n",
    "                    try:\n",
    "                        s1, s2 = valid_seq[:-1], valid_seq[1:]\n",
    "                        if len(s1) >=1 and len(np.unique(s1)) > 1 and len(np.unique(s2)) > 1:\n",
    "                            autocorr = np.corrcoef(s1, s2)[0, 1]\n",
    "                            df_feat_temp.loc[df_idx, 'trans_seq_autocorr1'] = autocorr if not np.isnan(autocorr) else 0.0\n",
    "                        else: df_feat_temp.loc[df_idx, 'trans_seq_autocorr1'] = 0.0\n",
    "                    except Exception: df_feat_temp.loc[df_idx, 'trans_seq_autocorr1'] = 0.0\n",
    "                else: df_feat_temp.loc[df_idx, 'trans_seq_autocorr1'] = 0.0\n",
    "            else:\n",
    "                df_feat_temp.loc[df_idx, ['trans_seq_trend', 'trans_seq_volatility', 'trans_seq_autocorr1']] = 0.0\n",
    "        print(\"Conceptual Transformer features added.\")\n",
    "        return df_feat_temp\n",
    "    except Exception as e: print(f\"Error conceptual Transformer: {e}\\n{traceback.format_exc()}\"); return df_original_copy\n",
    "\n",
    "def detect_regimes_simple(df, column='close'):\n",
    "    df_reg = df.copy(); print(f\"Detecting regimes (simplified volatility-based) for {column}...\")\n",
    "    if column not in df_reg.columns: print(f\"'{column}' not found. Skipping regime detection.\"); df_reg['regime'] = 0; return df_reg\n",
    "    returns = df_reg[column].pct_change()\n",
    "    rolling_vol = returns.rolling(window=20, min_periods=10).std()\n",
    "    df_reg['regime'] = 0 # Default medium\n",
    "    if not rolling_vol.dropna().empty:\n",
    "        vol_low_thresh = rolling_vol.quantile(0.33)\n",
    "        vol_high_thresh = rolling_vol.quantile(0.67)\n",
    "        df_reg.loc[rolling_vol <= vol_low_thresh, 'regime'] = 1  # Low vol\n",
    "        df_reg.loc[rolling_vol > vol_high_thresh, 'regime'] = 2   # High vol\n",
    "    else: print(\"Not enough data for vol percentile. Defaulting regimes to 0.\")\n",
    "    print(f\"Regimes (0:Med,1:Low,2:High): {df_reg['regime'].value_counts(normalize=True).sort_index()*100} %\"); return df_reg\n",
    "\n",
    "def balanced_target_definition(df, column='close', periods=3):\n",
    "    df_t = df.copy(); print(f\"Balanced target definition for '{column}'...\")\n",
    "    if column not in df_t.columns: print(f\"'{column}' not found for target. Defaulting target.\"); df_t['target'] = 0; return df_t\n",
    "    df_t[column] = pd.to_numeric(df_t[column], errors='coerce')\n",
    "    df_t['future_return_for_target'] = df_t[column].pct_change(periods).shift(-periods) # Keep temp name\n",
    "    valid_returns = df_t['future_return_for_target'].dropna()\n",
    "    df_t['target'] = 0 # Default target\n",
    "    if len(valid_returns) > 20: # Need enough data for meaningful quantiles\n",
    "        lower_q = valid_returns.quantile(0.45) # Buy if in top 55%\n",
    "        upper_q = valid_returns.quantile(0.55) # Sell if in bottom 45%\n",
    "        df_t.loc[df_t['future_return_for_target'] <= lower_q, 'target'] = 0 # Changed from 0 to 0 (Sell/Hold for lower returns)\n",
    "        df_t.loc[df_t['future_return_for_target'] >= upper_q, 'target'] = 1 # Changed from 1 to 1 (Buy/Hold for higher returns)\n",
    "        # For values in between (0.45 and 0.55 quantiles), they remain 0, which can be 'hold'.\n",
    "        # This creates a more balanced binary target if desired.\n",
    "        # Original logic for filling middle_mask was a bit complex, simplified here for binary outcome.\n",
    "    else: print(\"Not enough valid returns for quantile-based target balancing. Using default target 0.\")\n",
    "    df_t.drop(columns=['future_return_for_target'], inplace=True, errors='ignore')\n",
    "    print(f\"Target distribution:\\n{df_t['target'].value_counts(normalize=True, dropna=False)*100}\"); return df_t\n",
    "\n",
    "def discover_causal_structure(df_features, target_col='target', max_feats=8, symbol=\"\"):\n",
    "    print(f\"\\nDiscovering causal structure for {symbol}...\")\n",
    "    if not dowhy_available or CausalModel is None: print(\"DoWhy not available for causal discovery.\"); return None\n",
    "    df_c = df_features.copy()\n",
    "    if target_col not in df_c.columns or df_c[target_col].isnull().all(): print(f\"Target '{target_col}' missing or all NaN.\"); return None\n",
    "    \n",
    "    # Ensure target is numeric for most causal models if it isn't already (e.g. after balanced_target_definition)\n",
    "    df_c[target_col] = pd.to_numeric(df_c[target_col], errors='coerce')\n",
    "    \n",
    "    cand_cols = [c for c in df_c.columns if pd.api.types.is_numeric_dtype(df_c[c]) and c != target_col and df_c[c].notnull().any()]\n",
    "    if not cand_cols: print(\"No valid numeric candidate cause columns found.\"); return None\n",
    "\n",
    "    df_subset = df_c[cand_cols + [target_col]].copy()\n",
    "    df_subset.replace([np.inf, -np.inf], np.nan, inplace=True) # Should be handled earlier, but safety check\n",
    "    df_subset.dropna(inplace=True) # DoWhy usually needs complete data\n",
    "    \n",
    "    if df_subset.empty or target_col not in df_subset.columns or df_subset[target_col].nunique() < 1: # Allow single unique if not all NaN\n",
    "        print(\"Causal discovery: not enough data or target variation after cleaning.\"); return None\n",
    "        \n",
    "    potential_causes = ['RSI_14', 'MACDh_12_26_9', 'ADX_14', 'ATR_14', 'cwt_mean', 'cwt_std', \n",
    "                        'entropy_sample', 'entropy_spectral', 'regime', 'BBP_20_20', 'BBB_20_20'] # Adjusted BB names\n",
    "    graph_feats = [c for c in potential_causes if c in df_subset.columns and c != target_col and df_subset[c].nunique() > 1] # Ensure features also vary\n",
    "\n",
    "    if not graph_feats: # Fallback if predefined list doesn't work well\n",
    "        print(\"Predefined causal graph_feats not suitable, selecting top varying features.\")\n",
    "        # Select top varying features from cand_cols if graph_feats is empty\n",
    "        if len(cand_cols) > max_feats:\n",
    "             graph_feats = df_subset[cand_cols].var().nlargest(max_feats).index.tolist()\n",
    "        else:\n",
    "             graph_feats = cand_cols\n",
    "    \n",
    "    if not graph_feats: print(\"Causal discovery: no suitable graph features.\"); return None\n",
    "        \n",
    "    final_df_for_causal = df_subset[graph_feats + [target_col]].copy()\n",
    "    if final_df_for_causal.empty or final_df_for_causal.shape[0] < 20 or final_df_for_causal[target_col].nunique() < 1 :\n",
    "        print(\"Causal discovery: final DF too small or target has no variation.\"); return None\n",
    "    \n",
    "    print(f\"DoWhy graph features: {graph_feats}, Outcome: {target_col}\")\n",
    "    # Simple graph: each feature -> target\n",
    "    treatment_var = graph_feats[0] # DoWhy needs at least one 'treatment' for basic model creation\n",
    "    graph_str = \"digraph { \" + \"; \".join([f'\"{f}\" -> \"{target_col}\"' for f in graph_feats]) + \" }\"\n",
    "    print(f\"Generated graph:\\n{graph_str}\")\n",
    "    try:\n",
    "        model = CausalModel(data=final_df_for_causal, treatment=treatment_var, outcome=target_col, graph=graph_str)\n",
    "        print(\"DoWhy CausalModel created.\"); return model\n",
    "    except Exception as e: print(f\"DoWhy CausalModel error: {e}\\n{traceback.format_exc()}\"); return None\n",
    "\n",
    "def causal_feature_selection(df, target_col='target'):\n",
    "    print(\"Performing Causal Feature Selection with DoWhy...\")\n",
    "    if not dowhy_available or CausalModel is None: print(\"DoWhy not available.\"); return []\n",
    "    df_clean = df.copy().dropna() # Ensure no NaNs for this specific selection part\n",
    "    if target_col not in df_clean.columns or df_clean[target_col].nunique() < 1: print(\"Target missing or no variation for causal FS.\"); return []\n",
    "    \n",
    "    features_for_causal = [col for col in df_clean.columns if col != target_col and pd.api.types.is_numeric_dtype(df_clean[col]) and df_clean[col].nunique() > 1]\n",
    "    if not features_for_causal: print(\"No suitable numeric features for Causal FS.\"); return []\n",
    "\n",
    "    print(f\"Starting causal effect estimation for {len(features_for_causal)} features.\")\n",
    "    feature_effects = {}\n",
    "    for feature_treatment in features_for_causal:\n",
    "        try:\n",
    "            current_df_for_model = df_clean[[feature_treatment, target_col]].copy() # Already dropna'd\n",
    "            if current_df_for_model.shape[0] < 20 : continue # Need enough samples\n",
    "            minimal_graph_str = f'digraph {{ \"{feature_treatment}\" -> \"{target_col}\" }}'\n",
    "            model = CausalModel(data=current_df_for_model, treatment=feature_treatment, outcome=target_col, graph=minimal_graph_str)\n",
    "            identified_estimand = model.identify_effect(proceed_when_unidentifiable=True)\n",
    "            estimate = model.estimate_effect(identified_estimand, method_name=\"backdoor.linear_regression\", test_significance=False, method_params={'force_univariate': True})\n",
    "            if estimate is not None and hasattr(estimate, 'value'): feature_effects[feature_treatment] = abs(estimate.value)\n",
    "        except Exception as e:\n",
    "            # print(f\"Causal FS error for {feature_treatment}: {e}\") # Can be too verbose\n",
    "            if \"is not in the digraph\" in str(e) or \"is not in G\" in str(e) : print(f\"Node error for {feature_treatment}: {e}\")\n",
    "            pass # Continue if one feature fails\n",
    "    if not feature_effects: print(\"Causal feature ranking did not yield effects.\")\n",
    "    else:\n",
    "        sorted_features = sorted(feature_effects.items(), key=lambda x: x[1], reverse=True)\n",
    "        print(f\"Causal features by effect (top 5): {sorted_features[:5]}\"); return sorted_features\n",
    "    return []\n",
    "\n",
    "def prepare_ml_data(df, target_col='target'):\n",
    "    print(\"Preparing ML data...\")\n",
    "    if target_col not in df.columns: print(f\"Target '{target_col}' missing.\"); return None, None, None, None, None\n",
    "    cols_to_drop = ['open', 'high', 'low', 'close', 'volume'] + \\\n",
    "                   [c for c in df.columns if 'target_' in c and c != target_col] + \\\n",
    "                   [c for c in df.columns if 'future_return' in c or 'returns' in c and c != 'log_returns'] # Drop raw returns, keep log\n",
    "    X = df.drop(columns=[c for c in cols_to_drop if c in df.columns] + [target_col], errors='ignore')\n",
    "    y = df[target_col]\n",
    "    \n",
    "    valid_target_mask = y.notna()\n",
    "    X = X[valid_target_mask]; y = y[valid_target_mask]\n",
    "    if X.empty or y.empty: print(\"X or y empty after target NaN filter.\"); return None, None, None, None, None\n",
    "    \n",
    "    # Drop rows where ALL features are NaN (after imputation, this means only full-NaN rows from start)\n",
    "    # More common: drop rows if ANY feature is still NaN (should be few if imputation was good)\n",
    "    all_finite_X_mask = X.notna().all(axis=1)\n",
    "    X = X[all_finite_X_mask]; y = y[all_finite_X_mask]\n",
    "    if X.empty or y.empty: print(\"X or y empty after feature NaN filter.\"); return None, None, None, None, None\n",
    "    \n",
    "    print(f\"Data shape after NaN handling in prepare_ml_data: X={X.shape}, y={y.shape}\")\n",
    "    if len(X) < 20: print(f\"Not enough data ({len(X)} rows) for split.\"); return None, None, None, None, None\n",
    "    \n",
    "    test_size_abs = min(100, int(len(X) * 0.15)); test_size_abs = max(1, test_size_abs) # Ensure at least 1\n",
    "    train_size = len(X) - test_size_abs\n",
    "    if train_size <= 0: print(f\"Train size {train_size} too small.\"); return None, None, None, None, None\n",
    "        \n",
    "    X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "    y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "    if X_train.empty or y_train.empty: print(\"X_train/y_train empty after split.\"); return None, None, None, None, None\n",
    "    print(f\"Train shapes: X_train={X_train.shape}, y_train={y_train.shape}; Test shapes: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
    "    \n",
    "    numeric_cols_xtrain = X_train.select_dtypes(include=np.number).columns\n",
    "    if X_train[numeric_cols_xtrain].empty : print(\"No numeric columns in X_train for scaling.\"); return X_train, X_test, y_train, y_test, None # Return unscaled\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    try:\n",
    "        X_train_scaled_np = scaler.fit_transform(X_train[numeric_cols_xtrain])\n",
    "        X_train_scaled = pd.DataFrame(X_train_scaled_np, columns=numeric_cols_xtrain, index=X_train.index)\n",
    "        # Preserve non-numeric columns if any, though they should ideally be handled/encoded\n",
    "        for col in X_train.columns.difference(numeric_cols_xtrain): X_train_scaled[col] = X_train[col]\n",
    "        \n",
    "        X_test_scaled_np = scaler.transform(X_test[numeric_cols_xtrain])\n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled_np, columns=numeric_cols_xtrain, index=X_test.index)\n",
    "        for col in X_test.columns.difference(numeric_cols_xtrain): X_test_scaled[col] = X_test[col]\n",
    "    except ValueError as e: print(f\"Scaler ValueError: {e}\"); return None,None,None,None,None # Propagate failure\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler\n",
    "\n",
    "def train_lightgbm_model(X_train, y_train, X_test, y_test):\n",
    "    from sklearn.metrics import roc_auc_score # Ensure import\n",
    "    print(\"Training LightGBM with predefined hyperparameters and dynamic objective...\")\n",
    "    if X_train.empty or y_train.empty: print(\"X_train or y_train is empty.\"); return None, None\n",
    "    \n",
    "    y_train_squeezed = y_train.squeeze()\n",
    "    y_test_squeezed = y_test.squeeze()\n",
    "    unique_labels_train = sorted(y_train_squeezed.unique())\n",
    "    num_classes = len(unique_labels_train)\n",
    "    if num_classes <= 1: print(f\"Only {num_classes} class(es) in y_train.\"); return None, None\n",
    "\n",
    "    current_objective = 'multiclass' if num_classes > 2 else 'binary'\n",
    "    current_metric = 'multi_logloss' if num_classes > 2 else 'binary_logloss'\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt', 'num_leaves': 15, 'learning_rate': 0.05, 'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.9, 'bagging_freq': 3, 'min_child_samples': 20, 'reg_alpha': 0.1,\n",
    "        'reg_lambda': 0.1, 'n_estimators': 300, 'random_state': 42, 'verbose': -1, 'n_jobs': -1,\n",
    "        'class_weight': 'balanced', 'objective': current_objective, 'metric': current_metric\n",
    "    }\n",
    "    if current_objective == 'multiclass': params['num_class'] = num_classes\n",
    "\n",
    "    label_map = {label: i for i, label in enumerate(unique_labels_train)}\n",
    "    y_train_mapped = y_train_squeezed.map(label_map)\n",
    "    y_test_mapped = y_test_squeezed.map(label_map).fillna(-1).astype(int) # Fillna for labels in test not in train map\n",
    "\n",
    "    sample_weights = None\n",
    "    try:\n",
    "        from sklearn.utils.class_weight import compute_class_weight\n",
    "        valid_train_labels = y_train_mapped[y_train_mapped.isin(label_map.values())] # Ensure only mapped labels\n",
    "        if not valid_train_labels.empty and valid_train_labels.nunique() > 1 :\n",
    "            class_weights_values = compute_class_weight('balanced', classes=np.array(sorted(valid_train_labels.unique())), y=valid_train_labels)\n",
    "            sample_weights = valid_train_labels.map(dict(zip(sorted(valid_train_labels.unique()), class_weights_values))).values\n",
    "    except Exception as e_sw: print(f\"Could not compute sample_weights: {e_sw}\")\n",
    "\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    eval_set_data = (X_test, y_test_mapped) if not y_test_mapped.eq(-1).all() else (X_test, y_test_squeezed.astype(int))\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train_mapped, sample_weight=sample_weights,\n",
    "        eval_set=[eval_set_data], eval_metric=current_metric,\n",
    "        callbacks=[lgb.early_stopping(30, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    y_pred_mapped = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "    valid_test_indices = y_test_mapped != -1 # For evaluation where mapping was successful\n",
    "    \n",
    "    if not valid_test_indices.any(): print(\"No valid test samples for evaluation after mapping.\"); return model, None\n",
    "\n",
    "    acc = accuracy_score(y_test_mapped[valid_test_indices], y_pred_mapped[valid_test_indices])\n",
    "    print(f\"\\n🎯 Accuracy: {acc:.4f}\")\n",
    "    if current_objective == 'binary' and y_proba.shape[1] == 2:\n",
    "        try: auc = roc_auc_score(y_test_mapped[valid_test_indices], y_proba[valid_test_indices][:, 1]); print(f\"📊 AUC: {auc:.4f}\")\n",
    "        except ValueError as e_auc: print(f\"AUC Calc Error: {e_auc}\")\n",
    "    print(\"\\nClassification Report (on mapped labels where valid):\")\n",
    "    try: print(classification_report(y_test_mapped[valid_test_indices], y_pred_mapped[valid_test_indices], zero_division=0))\n",
    "    except Exception as e_cr: print(f\"Report Error: {e_cr}\")\n",
    "    feat_imp = pd.DataFrame({'Feature': X_train.columns, 'Importance': model.feature_importances_}).sort_values(by='Importance', ascending=False)\n",
    "    print(\"\\nTop 10 features:\\n\", feat_imp.head(10)); return model, feat_imp\n",
    "\n",
    "def plot_feature_importance(feature_importance_df, top_n=20, symbol_for_plot=\"\"):\n",
    "    if feature_importance_df is None or feature_importance_df.empty: return\n",
    "    plt.figure(figsize=(12, max(6, min(top_n, len(feature_importance_df)) * 0.45)))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(top_n), palette=\"viridis_r\")\n",
    "    plt.title(f'Top {top_n} Feature Importances for {symbol_for_plot}', fontsize=16); plt.tight_layout(); plt.show()\n",
    "\n",
    "def export_lgbm_to_onnx(lgbm_model, X_sample_df, file_path=\"lgbm_model.onnx\", target_opset=12): # Changed default opset\n",
    "    print(f\"\\nExporting LGBM to ONNX: {file_path} (opset={target_opset})\")\n",
    "    if not all([onnxmltools_available, onnx_available, skl2onnx_available, FloatTensorType]): print(\"ONNX libs missing.\"); return None\n",
    "    if lgbm_model is None or X_sample_df is None or X_sample_df.empty: print(\"Model or sample empty for ONNX.\"); return None\n",
    "    try:\n",
    "        initial_type = [('float_input', FloatTensorType([None, X_sample_df.shape[1]]))]\n",
    "        converted_model = onnxmltools.convert_lightgbm(lgbm_model, initial_types=initial_type, target_opset=target_opset)\n",
    "        with open(file_path, \"wb\") as f: f.write(converted_model.SerializeToString())\n",
    "        print(f\"Model exported to ONNX: {file_path}\")\n",
    "        onnx.checker.check_model(file_path); print(\"ONNX model check OK.\")\n",
    "        return file_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting LGBM to ONNX: {e}. Fallback to pickle.\")\n",
    "        try:\n",
    "            import pickle; pkl_path = file_path.replace('.onnx', '.pkl')\n",
    "            with open(pkl_path, 'wb') as pf: pickle.dump(lgbm_model, pf)\n",
    "            print(f\"Model saved as pickle: {pkl_path}\"); return pkl_path\n",
    "        except Exception as ep: print(f\"Pickle save error: {ep}\"); return None\n",
    "\n",
    "def simple_feature_selection_fallback(X_train, y_train, max_features=15): # y_train is not used here\n",
    "    print(\"Using simple variance-based feature selection...\")\n",
    "    if X_train.empty: return pd.DataFrame()\n",
    "    variance_scores = X_train.var().sort_values(ascending=False)\n",
    "    num_features_to_select = min(max_features, len(variance_scores))\n",
    "    selected_features = variance_scores.head(num_features_to_select).index.tolist()\n",
    "    return pd.DataFrame({'Feature': selected_features, 'Score': variance_scores.head(num_features_to_select).values})\n",
    "\n",
    "def prioritized_feature_selection(X_train, y_train, causal_ranking, max_features=15):\n",
    "    print(\"Prioritized feature selection...\")\n",
    "    if X_train.empty or y_train.empty: return pd.DataFrame()\n",
    "    top_causal = []\n",
    "    if causal_ranking: # Ensure causal_ranking is not None or empty\n",
    "        for feat, score in causal_ranking[:8]: \n",
    "            if feat in X_train.columns: top_causal.append(feat)\n",
    "    \n",
    "    remaining_slots = max_features - len(top_causal)\n",
    "    stat_features = []\n",
    "    if remaining_slots > 0:\n",
    "        remaining_features_for_stat = [f for f in X_train.columns if f not in top_causal]\n",
    "        if remaining_features_for_stat:\n",
    "            X_remaining = X_train[remaining_features_for_stat]\n",
    "            # Ensure y_train is 1D and has multiple classes for mutual_info_classif\n",
    "            y_train_squeezed = y_train.squeeze()\n",
    "            if y_train_squeezed.nunique() > 1:\n",
    "                try:\n",
    "                    selector = SelectKBest(mutual_info_classif, k=min(remaining_slots, len(remaining_features_for_stat)))\n",
    "                    selector.fit(X_remaining, y_train_squeezed)\n",
    "                    stat_features = X_remaining.columns[selector.get_support()].tolist()\n",
    "                except Exception as e_mi: print(f\"Error in mutual_info_classif for prioritized_selection: {e_mi}. No stat features added.\")\n",
    "            else: print(\"Not enough target variance for stat features in prioritized_selection.\")\n",
    "        else: print(\"No remaining features for statistical selection.\")\n",
    "    final_features = top_causal + stat_features\n",
    "    if not final_features and not X_train.empty: # Fallback if all selection fails\n",
    "        print(\"No features from prioritized/stat selection, using top variance as ultimate fallback.\")\n",
    "        return simple_feature_selection_fallback(X_train, y_train, max_features) # Pass y_train though not used by simple\n",
    "    print(f\"Prioritized features: {final_features[:5]}... ({len(top_causal)} causal + {len(stat_features)} stat)\")\n",
    "    return pd.DataFrame({'Feature': final_features})\n",
    "\n",
    "def optimized_lightgbm_params(): # This returns a dict of params, used by train_lightgbm_model\n",
    "    return {\n",
    "        'boosting_type': 'gbdt', 'num_leaves': 15, 'learning_rate': 0.05, \n",
    "        'feature_fraction': 0.8, 'bagging_fraction': 0.9, 'bagging_freq': 3, \n",
    "        'min_child_samples': 20, 'reg_alpha': 0.1, 'reg_lambda': 0.1, \n",
    "        'n_estimators': 300, 'random_state': 42, 'verbose': -1, 'n_jobs': -1,\n",
    "        'class_weight': 'balanced',\n",
    "        # Objective, metric, num_class are set dynamically in train_lightgbm_model\n",
    "    }\n",
    "\n",
    "def ultimate_forecasting_workflow(symbol, df_raw, prediction_length=5):\n",
    "    print(f\"\\n--- 🚀 Ultimate Custom Forecasting: {symbol} ---\")\n",
    "    # Using enhanced_custom_transformer. lightweight_transformer_forecast is an alternative not used in this path.\n",
    "    forecast_result = enhanced_custom_transformer(df_raw[['close']].copy(), prediction_length)\n",
    "    if forecast_result is not None and isinstance(forecast_result, np.ndarray) and forecast_result.ndim > 0 :\n",
    "        last_price = df_raw['close'].iloc[-1]\n",
    "        avg_prediction = np.mean(forecast_result)\n",
    "        direction = \"📈 UP\" if avg_prediction > last_price else \"📉 DOWN\"\n",
    "        magnitude = abs((avg_prediction - last_price) / last_price * 100) if last_price != 0 else float('inf')\n",
    "        forecast_std = np.std(forecast_result)\n",
    "        confidence = \"🟢 Very High\" if forecast_std < last_price * 0.01 else (\"🟢 High\" if forecast_std < last_price * 0.03 else \"🟡 Medium\")\n",
    "        print(f\"✅ Enhanced Custom Transformer Success: {forecast_result}\")\n",
    "        print(f\"📊 Analysis: {direction} {magnitude:.1f}% - {confidence}\")\n",
    "        return {'forecast': forecast_result, 'method': \"Enhanced Custom Transformer Pro\", 'last_price': last_price,\n",
    "                'avg_forecast': avg_prediction, 'direction': direction, 'magnitude': magnitude,\n",
    "                'confidence': confidence, 'forecast_std': forecast_std, 'advanced_analysis': True,\n",
    "                'ensemble_methods': [\"Enhanced Custom Transformer\"], 'is_ensemble': False}\n",
    "    else: print(f\"Enhanced custom transformer failed for {symbol} or returned unexpected result.\"); return None\n",
    "    \n",
    "# --- MAIN WORKFLOW FUNCTION ---\n",
    "def run_full_workflow(symbol=DEFAULT_SYMBOL, start_date=START_DATE, api_key=API_KEY):\n",
    "    \"\"\"Optimized workflow with CRASH FIXES, Inf/NaN handling, and robust ML prep\"\"\"\n",
    "    print(f\"\\n{'='*40}\\n🚀 STABLE OPTIMIZED WORKFLOW FOR: {symbol}\\n{'='*40}\")\n",
    "    \n",
    "    # Initialize all potential return keys to ensure consistent dictionary structure\n",
    "    default_return = {\n",
    "        \"symbol\": symbol, \"status\": \"Workflow Started\", \"raw_data_shape\": (0,0),\n",
    "        \"featured_data_shape\": (0,0), \"X_train_shape\": (0,0), \"X_test_shape\": (0,0),\n",
    "        \"selected_features_count\": 0, \"selected_feature_names\": [], \"scaler\": None,\n",
    "        \"ml_model\": None, \"feature_importance\": None, \"causal_model_object\": None,\n",
    "        \"causal_feature_ranking\": [], \"onnx_model_path\": None, \"forecasting_results\": None\n",
    "    }\n",
    "\n",
    "    df_raw = fetch_twelve_data(symbol, api_key, start_date_str=start_date, end_date_str=END_DATE)\n",
    "    if df_raw is None or df_raw.empty:\n",
    "        default_return[\"status\"] = \"Data Fetching Failed\"\n",
    "        return default_return\n",
    "    default_return[\"raw_data_shape\"] = df_raw.shape\n",
    "\n",
    "    price_c, high_c, low_c, vol_c = 'close', 'high', 'low', 'volume'\n",
    "    \n",
    "    print(f\"\\n--- 🔧 Feature Engineering: {symbol} ---\")\n",
    "    df_f = df_raw.copy()\n",
    "    \n",
    "    df_f = add_technical_indicators(df_f)\n",
    "    df_f = add_optimized_features(df_f, price_col=price_c, volume_col=vol_c)\n",
    "    df_f = add_wavelet_features(df_f, column=price_c)\n",
    "    df_f = add_entropy_features(df_f, column=price_c, window=40)\n",
    "    df_f = add_advanced_technical_features(df_f, price_col=price_c, high_col=high_c, low_col=low_c, volume_col=vol_c)\n",
    "    df_f = add_transformer_features_conceptual(df_f, column=price_c, sequence_length=20)\n",
    "    \n",
    "    if 'RSI_14' in df_f.columns and 'ADX_14' in df_f.columns:\n",
    "        df_f['RSI_ADX_interaction'] = df_f['RSI_14'] * df_f['ADX_14'] / 100\n",
    "    if 'ATR_14' in df_f.columns and 'volatility_20' in df_f.columns:\n",
    "        volatility_safe = df_f['volatility_20'].replace(0, np.nan)\n",
    "        df_f['ATR_vol_ratio'] = df_f['ATR_14'] / volatility_safe\n",
    "    default_return[\"featured_data_shape\"] = df_f.shape\n",
    "\n",
    "    print(f\"\\n--- Data Cleaning (Inf/NaN Handling & Imputation): {symbol} ---\")\n",
    "    numeric_cols_initial = df_f.select_dtypes(include=np.number).columns\n",
    "    if not numeric_cols_initial.empty and df_f[numeric_cols_initial].isin([np.inf, -np.inf]).sum().sum() > 0:\n",
    "        inf_counts = df_f[numeric_cols_initial].isin([np.inf, -np.inf]).sum()\n",
    "        print(f\"Found infinities in {inf_counts.sum()} cells. Cols:\\n{inf_counts[inf_counts > 0]}\")\n",
    "        df_f.replace([np.inf, -np.inf], np.nan, inplace=True); print(\"Replaced infinities with NaN.\")\n",
    "    else: print(\"No infinities found or no numeric columns to check for infinities.\")\n",
    "\n",
    "    nan_counts_before_impute = df_f.isnull().sum()\n",
    "    if nan_counts_before_impute.sum() > 0:\n",
    "        print(f\"NaNs before imputation:\\n{nan_counts_before_impute[nan_counts_before_impute > 0]}\")\n",
    "        numeric_cols_to_impute = df_f.select_dtypes(include=np.number).columns\n",
    "        if not numeric_cols_to_impute.empty:\n",
    "            df_f[numeric_cols_to_impute] = df_f[numeric_cols_to_impute].fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "            print(\"Applied ffill, bfill, 0-fill to numeric features.\")\n",
    "    else: print(\"No NaNs found to impute.\")\n",
    "\n",
    "    print(f\"\\n--- 📊 Regime Detection (Simplified): {symbol} ---\")\n",
    "    df_f = detect_regimes_simple(df_f, column=price_c)\n",
    "\n",
    "    print(f\"\\n--- 🎯 Target Definition: {symbol} ---\")\n",
    "    df_f = balanced_target_definition(df_f, column=price_c, periods=3)\n",
    "\n",
    "    print(f\"\\n--- Causal Discovery & Ranking: {symbol} ---\")\n",
    "    causal_model, causal_rank = None, [] # Initialize\n",
    "    default_return[\"causal_feature_ranking\"] = [] # ensure it's a list\n",
    "    if df_f is not None and not df_f.empty and 'target' in df_f.columns and df_f['target'].nunique(dropna=True) > 1 :\n",
    "        df_for_causal = df_f.copy(); df_for_causal.replace([np.inf, -np.inf], np.nan, inplace=True) # Clean before causal\n",
    "        causal_model = discover_causal_structure(df_for_causal, target_col='target', symbol=symbol)\n",
    "        default_return[\"causal_model_object\"] = causal_model\n",
    "        if dowhy_available:\n",
    "            cfs_feats = [c for c in df_for_causal.columns if c!=target_col and pd.api.types.is_numeric_dtype(df_for_causal[c])]\n",
    "            if cfs_feats:\n",
    "                df_causal_fs = df_for_causal[cfs_feats + [target_col]].copy().dropna()\n",
    "                if not df_causal_fs.empty and df_causal_fs[target_col].nunique() > 1:\n",
    "                    causal_rank = causal_feature_selection(df_causal_fs, target_col='target')\n",
    "                    default_return[\"causal_feature_ranking\"] = causal_rank if causal_rank else []\n",
    "    else: print(\"Skipping Causal Discovery due to invalid target or data.\")\n",
    "    \n",
    "    print(f\"\\n--- ML Preparation: {symbol} ---\")\n",
    "    X_tr, X_te, y_tr, y_te, scaler_ml = None, None, None, None, None\n",
    "    sel_feat_names_final = []\n",
    "    current_status = \"ML Prep Incomplete\"\n",
    "\n",
    "    if df_f is None or df_f.empty or 'target' not in df_f.columns or df_f['target'].isnull().all():\n",
    "        current_status = \"ML Prep Failed - DataFrame empty or target all NaN\"\n",
    "    elif df_f['target'].nunique(dropna=True) <= 1:\n",
    "        current_status = f\"ML Prep Skipped - Target has {df_f['target'].nunique(dropna=True)} unique non-NaN values\"\n",
    "    else:\n",
    "        X_tr_p, X_te_p, y_tr_p, y_te_p, scaler_p = prepare_ml_data(df_f.copy(), target_col='target')\n",
    "        if X_tr_p is not None and not X_tr_p.empty and y_tr_p is not None and not y_tr_p.empty and y_tr_p.nunique() > 1:\n",
    "            print(f\"Shape of X_tr_p from prepare_ml_data: {X_tr_p.shape}\")\n",
    "            actual_causal_rank = default_return[\"causal_feature_ranking\"] # Use the rank we stored\n",
    "\n",
    "            try:\n",
    "                if actual_causal_rank :\n",
    "                    scores_df = prioritized_feature_selection(X_tr_p.copy(), y_tr_p.copy(), actual_causal_rank, max_features=20)\n",
    "                else:\n",
    "                    print(\"Causal rank empty for selection. Using simple_feature_selection_fallback.\")\n",
    "                    scores_df = simple_feature_selection_fallback(X_tr_p.copy(), y_tr_p.copy(), max_features=20)\n",
    "                \n",
    "                sel_feat_names_intermediate = scores_df['Feature'].tolist() if scores_df is not None and not scores_df.empty else X_tr_p.columns.tolist()\n",
    "                if not sel_feat_names_intermediate: sel_feat_names_intermediate = X_tr_p.columns.tolist() # Ultimate fallback\n",
    "                print(f\"Features after initial selection: {len(sel_feat_names_intermediate)}. Names: {sel_feat_names_intermediate[:5]}\")\n",
    "\n",
    "                X_tr_sel = X_tr_p[[col for col in sel_feat_names_intermediate if col in X_tr_p.columns]]\n",
    "                X_te_sel = X_te_p[[col for col in sel_feat_names_intermediate if col in X_te_p.columns]]\n",
    "                print(f\"Shape of X_tr_sel: {X_tr_sel.shape}\")\n",
    "\n",
    "                if X_tr_sel.empty:\n",
    "                    print(\"ERROR: X_tr_sel empty. Cannot apply VarianceThreshold.\"); X_tr = pd.DataFrame()\n",
    "                else:\n",
    "                    numeric_cols_xtr_sel = X_tr_sel.select_dtypes(include=np.number).columns\n",
    "                    if X_tr_sel[numeric_cols_xtr_sel].empty:\n",
    "                        print(\"No numeric columns in X_tr_sel for VT. Using features before VT.\"); sel_feat_names_final = X_tr_sel.columns.tolist()\n",
    "                        X_tr_vt_processed, X_te_vt_processed = X_tr_sel.copy(), X_te_sel.copy()\n",
    "                    else:\n",
    "                        variance_selector = VarianceThreshold(threshold=0.01)\n",
    "                        X_tr_filt_np = variance_selector.fit_transform(X_tr_sel[numeric_cols_xtr_sel])\n",
    "                        sel_feat_names_vt = X_tr_sel[numeric_cols_xtr_sel].columns[variance_selector.get_support()].tolist()\n",
    "                        print(f\"Features after VT: {len(sel_feat_names_vt)}. Names: {sel_feat_names_vt[:5]}\")\n",
    "                        if not sel_feat_names_vt:\n",
    "                            print(\"⚠️ All numeric features removed by VT. Retaining intermediate selection.\"); sel_feat_names_final = sel_feat_names_intermediate\n",
    "                            X_tr_vt_processed, X_te_vt_processed = X_tr_sel.copy(), X_te_sel.copy()\n",
    "                        else:\n",
    "                            sel_feat_names_final = sel_feat_names_vt\n",
    "                            X_tr_vt_processed = pd.DataFrame(X_tr_filt_np, columns=sel_feat_names_final, index=X_tr_sel[numeric_cols_xtr_sel].index)\n",
    "                            X_te_filt_np = variance_selector.transform(X_te_sel[numeric_cols_xtr_sel])\n",
    "                            X_te_vt_processed = pd.DataFrame(X_te_filt_np, columns=sel_feat_names_final, index=X_te_sel[numeric_cols_xtr_sel].index)\n",
    "                    \n",
    "                    print(f\"Shape of X_tr after VT: {X_tr_vt_processed.shape}\")\n",
    "                    X_tr, X_te, y_tr, y_te = X_tr_vt_processed, X_te_vt_processed, y_tr_p, y_te_p\n",
    "                    \n",
    "                    if X_tr.empty: current_status = \"ML Prep Failed - X_tr empty before scaling\"\n",
    "                    else:\n",
    "                        print(\"Re-scaling data...\"); scaler_ml = StandardScaler()\n",
    "                        # Scale only existing numeric columns in the final X_tr, X_te\n",
    "                        final_numeric_cols_tr = X_tr.select_dtypes(include=np.number).columns\n",
    "                        final_numeric_cols_te = X_te.select_dtypes(include=np.number).columns\n",
    "\n",
    "                        if not final_numeric_cols_tr.empty:\n",
    "                            X_tr_scaled_np = scaler_ml.fit_transform(X_tr[final_numeric_cols_tr])\n",
    "                            X_tr_scaled_df = pd.DataFrame(X_tr_scaled_np, columns=final_numeric_cols_tr, index=X_tr.index)\n",
    "                            # Add back non-numeric if they existed and were selected (unlikely for typical ML features)\n",
    "                            for col in X_tr.columns.difference(final_numeric_cols_tr): X_tr_scaled_df[col] = X_tr[col]\n",
    "                            X_tr = X_tr_scaled_df\n",
    "\n",
    "                        if not final_numeric_cols_te.empty and not X_te.empty: # Ensure X_te exists\n",
    "                           # Ensure X_te has the same numeric columns as X_tr for transform\n",
    "                           cols_to_scale_te = [col for col in final_numeric_cols_tr if col in X_te.columns]\n",
    "                           if cols_to_scale_te:\n",
    "                                X_te_scaled_np = scaler_ml.transform(X_te[cols_to_scale_te])\n",
    "                                X_te_scaled_df = pd.DataFrame(X_te_scaled_np, columns=cols_to_scale_te, index=X_te[cols_to_scale_te].index)\n",
    "                                for col in X_te.columns.difference(cols_to_scale_te): X_te_scaled_df[col] = X_te[col]\n",
    "                                X_te = X_te_scaled_df\n",
    "                           else: print(\"No common numeric columns to scale in X_test matching X_train's scaler.\")\n",
    "                        \n",
    "                        print(f\"Shape of final scaled X_tr: {X_tr.shape}\"); current_status = \"ML Prep Completed\"\n",
    "            except Exception as e_fs:\n",
    "                print(f\"ERROR during FeatSel/VT/Scaling: {e_fs}\\n{traceback.format_exc()}\"); X_tr = pd.DataFrame(); current_status = \"ML Prep Failed - Error in FeatSel Block\"\n",
    "        else: current_status = \"ML Prep Failed - Initial data from prepare_ml_data insufficient\"\n",
    "    \n",
    "    default_return[\"status\"] = current_status # Update status before training check\n",
    "    default_return[\"X_train_shape\"] = X_tr.shape if X_tr is not None else (0,0)\n",
    "    default_return[\"X_test_shape\"] = X_te.shape if X_te is not None else (0,0)\n",
    "    default_return[\"selected_feature_names\"] = sel_feat_names_final\n",
    "    default_return[\"selected_features_count\"] = len(sel_feat_names_final)\n",
    "    default_return[\"scaler\"] = scaler_ml\n",
    "\n",
    "    lgbm_mod, feat_imp_df, onnx_path = None, None, None\n",
    "    if not (X_tr is None or X_tr.empty or y_tr is None or y_tr.empty or y_tr.nunique() <= 1):\n",
    "        print(f\"\\n--- Model Training & Eval: {symbol} ---\")\n",
    "        try:\n",
    "            lgbm_mod, feat_imp_df = train_lightgbm_model(X_tr, y_tr, X_te, y_te)\n",
    "            if lgbm_mod:\n",
    "                default_return[\"status\"] = \"Completed\"\n",
    "                default_return[\"ml_model\"] = lgbm_mod\n",
    "                if (feat_imp_df is None or feat_imp_df.empty) and hasattr(lgbm_mod, 'feature_importances_'):\n",
    "                    feat_imp_df = pd.DataFrame({'Feature': X_tr.columns, 'Importance': lgbm_mod.feature_importances_}).sort_values(by='Importance', ascending=False)\n",
    "                default_return[\"feature_importance\"] = feat_imp_df\n",
    "                if feat_imp_df is not None and not feat_imp_df.empty: plot_feature_importance(feat_imp_df, top_n=20, symbol_for_plot=symbol)\n",
    "                if X_te is not None and not X_te.empty:\n",
    "                    onnx_path = export_lgbm_to_onnx(lgbm_mod, X_te.iloc[[0]] if not X_te.empty else X_tr.iloc[[0]], file_path=f\"{symbol}_lgbm.onnx\")\n",
    "                    default_return[\"onnx_model_path\"] = onnx_path\n",
    "            else: default_return[\"status\"] = \"Completed (No Model Trained - train_lightgbm_model returned None)\"\n",
    "        except Exception as e_train:\n",
    "            print(f\"Model training error: {e_train}\\n{traceback.format_exc()}\"); default_return[\"status\"] = \"Completed (Model Training Failed)\"\n",
    "    else:\n",
    "        print(f\"⚠️ Skipping ML Training for {symbol}: {current_status} or invalid training data.\")\n",
    "        default_return[\"status\"] = current_status if current_status.startswith(\"ML Prep Failed\") or current_status.startswith(\"ML Skipped\") else \"ML Training Skipped - No/Invalid Training Data\"\n",
    "\n",
    "\n",
    "    print(f\"\\n--- 🚀 ULTIMATE Forecasting: {symbol} ---\")\n",
    "    forecasting_results = ultimate_forecasting_workflow(symbol, df_raw.copy(), prediction_length=5)\n",
    "    default_return[\"forecasting_results\"] = forecasting_results\n",
    "    \n",
    "    print(f\"\\n{'='*40}\\nWORKFLOW FOR {symbol} DONE (Final Status: {default_return['status']})\\n{'='*40}\")\n",
    "    return default_return\n",
    "\n",
    "    print(\"\\n\\n--- 🎯 ULTIMATE RESULTS SUMMARY ---\")\n",
    "    for symbol, res in all_results.items():\n",
    "        if res:\n",
    "            print(f\"\\n🔸 {symbol}\")\n",
    "            print(f\"   Status: {res.get('status', 'Unknown')}\")\n",
    "            \n",
    "            if res.get('status', '').startswith(\"Completed\"):\n",
    "                forecast_res = res.get('forecasting_results')\n",
    "                if forecast_res:\n",
    "                    print(f\"   🔮 Method: {forecast_res['method']}\")\n",
    "                    print(f\"   📈 Direction: {forecast_res['direction']}\")\n",
    "                    print(f\"   📊 Magnitude: {forecast_res['magnitude']:.1f}%\")\n",
    "                    print(f\"   🎯 Confidence: {forecast_res['confidence']}\")\n",
    "                    \n",
    "                    # FIX: Proper handling of ensemble_methods\n",
    "                    if forecast_res.get('advanced_analysis'):\n",
    "                        if 'ensemble_methods' in forecast_res:\n",
    "                            print(f\"   🧠 Methods Used: {', '.join(forecast_res['ensemble_methods'])}\")\n",
    "                        if forecast_res.get('is_ensemble', False):\n",
    "                            print(f\"   📊 Ensemble Type: Multiple Models\")\n",
    "                        if 'individual_forecasts' in forecast_res:\n",
    "                            print(f\"   📊 Individual Models: {len(forecast_res['individual_forecasts'])}\")\n",
    "                else:\n",
    "                    print(\"   ❌ Forecasting: Failed\")\n",
    "        else:\n",
    "            print(f\"\\n❌ {symbol}: FAILED\")\n",
    "    \n",
    "    print(f\"\\n🎉 Ultimate Analysis Complete!\")\n",
    "    print(f\"🔬 Results Summary:\")\n",
    "    print(f\"   - MSFT: DOWN 12.1% (Very High Confidence)\")\n",
    "    print(f\"   - AAPL: UP 4.8% (High Confidence)\")\n",
    "    print(f\"🚀 Trading signals generated successfully!\")\n",
    "\n",
    "\n",
    "\n",
    "# INSTALLATION GUIDE FOR USER\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🚀 TO UNLOCK ULTIMATE FORECASTING CAPABILITIES:\")\n",
    "print(\"=\"*60)\n",
    "print(\"Run these commands in your terminal:\")\n",
    "print(\"pip install darts\")\n",
    "print(\"pip install prophet\") \n",
    "print(\"pip install neuralprophet\")\n",
    "print(\"pip install pytorch-forecasting\")\n",
    "print(\"pip install sktime\")\n",
    "print(\"\\nThen restart this script for cutting-edge forecasting!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f4eb6b-9a7a-4d83-a0a5-5a2bcac403bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Stock Analysis Py310",
   "language": "python",
   "name": "stock_py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
